@inproceedings{020d09e37dea422d8fec7de622333553,
  title = {New Technologies for Spoken Dialogue Systems: {{LLMs}}, {{RAG}} and the {{GenAI}} Stack},
  booktitle = {14th International Workshop on Spoken Dialogue Systems ({{IWSDS}} 2024)},
  author = {Wilcock, Graham},
  year = {2024},
  month = mar,
  abstract = {The paper describes some recent developments in Generative AI that can be applied in spoken dialogue systems, with a central theme of retrieval augmented generation from documents and knowledge graphs using open source large language models. The developments include vector indexing in Neo4j graph databases, the GenAI Stack provided by a collaboration of Docker, Neo4j, LangChain and Ollama, and the use of LLMs to generate Cypher queries that access knowledge graphs in graph databases. The example dialogue applications include Chat with PDFs, Chat with Wikipedia, and Chat with a knowledge graph.},
  langid = {english},
  keywords = {6121 Languages,Read},
  file = {/Users/janwardenga/Zotero/storage/7W2TYVH5/Pre__New Technologies for Spoken Dialogue Systems LLMs, RAG and the GenAI Stack.pdf}
}

@inproceedings{abedjanProfilingMiningRDF20142014IEEE30thInt.Conf.DataEng.,
  title = {Profiling and Mining {{RDF}} Data with {{ProLOD}}++},
  booktitle = {2014 {{IEEE}} 30th {{International Conference}} on {{Data Engineering}}},
  author = {Abedjan, Ziawasch and Gruetze, Toni and Jentzsch, Anja and Naumann, Felix},
  year = {2014},
  month = mar,
  pages = {1198--1201},
  publisher = {IEEE},
  address = {Chicago, IL, USA},
  doi = {10.1109/ICDE.2014.6816740},
  urldate = {2025-01-23},
  abstract = {Before reaping the benefits of open data to add value to an organizations internal data, such new, external datasets must be analyzed and understood already at the basic level of data types, constraints, value patterns etc. Such data profiling, already difficult for large relational data sources, is even more challenging for RDF datasets, the preferred data model for linked open data.},
  isbn = {978-1-4799-2555-1},
  langid = {english},
  keywords = {Jab/ICDE,Unread},
  annotation = {74 citations (Semantic Scholar/DOI) [2025-01-23]\\
TLDR: ProLod++ is a novel tool for various profiling and mining tasks to understand and ultimately improve open RDF data, allowing interactive profiling for users interested in exploring the properties and structure of yet unknown datasets.},
  file = {/Users/janwardenga/Zotero/storage/ZHI6UWM6/ICDE_2014_Profiling and mining RDF data with ProLOD++.pdf}
}

@article{abu-salihDomainspecificKnowledgeGraphs2021JournalofNetworkandComputerApplications,
  title = {Domain-Specific Knowledge Graphs: {{A}} Survey},
  shorttitle = {Domain-Specific Knowledge Graphs},
  author = {{Abu-Salih}, Bilal},
  year = {2021},
  month = jul,
  journal = {Journal of Network and Computer Applications},
  volume = {185},
  pages = {103076},
  issn = {10848045},
  doi = {10.1016/j.jnca.2021.103076},
  urldate = {2024-11-14},
  abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.},
  langid = {english},
  keywords = {Read},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-15]\\
258 citations (Semantic Scholar/DOI) [2025-02-15]\\
236 citations (Semantic Scholar/DOI) [2024-11-14]\\
TLDR: This survey is the first to provide an inclusive definition to the notion of domain KG, and a comprehensive review of the state-of-the-art approaches drawn from academic works relevant to seven dissimilar domains of knowledge is provided.},
  file = {/Users/janwardenga/Zotero/storage/8LAMGQHU/Abu-Salih - 2021 - Domain-specific knowledge graphs A survey.pdf}
}

@misc{AddonItem,
  title = {Addon {{Item}}},
  keywords = {Unread}
}

@article{aggourCompoundKnowledgeGraphEnabled2022IntegrMaterManufInnov,
  title = {Compound {{Knowledge Graph-Enabled AI Assistant}} for {{Accelerated Materials Discovery}}},
  author = {Aggour, Kareem S. and Detor, Andrew and Gabaldon, Alfredo and Mulwad, Varish and Moitra, Abha and Cuddihy, Paul and Kumar, Vijay S.},
  year = {2022},
  month = dec,
  journal = {Integrating Materials and Manufacturing Innovation},
  volume = {11},
  number = {4},
  pages = {467--478},
  issn = {2193-9764, 2193-9772},
  doi = {10.1007/s40192-022-00286-z},
  urldate = {2024-09-29},
  abstract = {Materials scientists are facing increasingly challenging multi-objective performance requirements to meet the needs of modern systems such as lighter-weight and more fuel-efficient aircraft engines, and higher heat and oxidation-resistant steam turbines. While so-called second wave statistical machine learning techniques are beginning to accelerate the materials development cycle, most materials science applications are data-deprived when compared to the vastness and complexity of the search space of possible solutions. In line with DARPA's vision of third wave AI approaches, we believe a combination of data-driven statistical machine learning and domain knowledge will be required to achieve a true revolution in materials discovery. To that end, we envision and have begun reducing to practice a system that fuses three forms of knowledge---factual scientific knowledge, physics-based and/or data-driven analytical models, and domain expert knowledge---into a single `Compound Knowledge Graph' in which contextual reasoning and adaptation can be performed to answer increasingly complex questions. We believe this Compound Knowledge Graph-based system can be the nucleus of a collaborative AI assistant that supports stateful natural language back-and-forth dialogs between materials scientists and the AI to accelerate the development and discovery of new materials. This paper details our vision, summarizes our progress to date on a steam turbine blade coating use case, and outlines our thoughts on the key challenges in making this vision a reality.},
  langid = {english},
  keywords = {Jab/IMMI,Unread},
  annotation = {8 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/ZBU2DG3C/IMMI_2022_Compound Knowledge Graph-Enabled AI Assistant for Accelerated Materials Discovery.pdf}
}

@article{ahnRetrievalAugmentedResponseGeneration2022IEEEAccess,
  title = {Retrieval-{{Augmented Response Generation}} for {{Knowledge-Grounded Conversation}} in the {{Wild}}},
  author = {Ahn, Yeonchan and Lee, Sang-Goo and Shim, Junho and Park, Jaehui},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {131374--131385},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3228964},
  urldate = {2024-09-30},
  abstract = {Users on the internet usually have conversations on interesting facts or topics along with diverse knowledge from the web. However, most existing knowledge-grounded conversation models consider only a single document regarding the topic of a conversation. The recently proposed retrieval-augmented models generate a response based on multiple documents; however, they ignore the given topic and use only the local context of the conversation. To this end, we introduce a novel retrieval-augmented response generation model that retrieves an appropriate range of documents relevant to both the topic and local context of a conversation and uses them for generating a knowledge-grounded response. Our model first accepts both topic words extracted from the whole conversation and the tokens before the response to yield multiple representations. It then chooses representations of the first N token and ones of keywords from the conversation and document encoders and compares the two groups of representation from the conversation with those groups of the document, respectively. For training, we introduce a new data-weighting scheme to encourage the model to produce knowledge-grounded responses without ground truth knowledge. Both automatic and human evaluation results with a large-scale dataset show that our models can generate more knowledgeable, diverse, and relevant responses compared to the state-of-the-art models.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  keywords = {Unread},
  annotation = {10 citations (Semantic Scholar/DOI) [2025-02-15]\\
1 citations (Crossref/DOI) [2024-09-30]\\
TLDR: A novel retrieval-augmented response generation model that retrieves an appropriate range of documents relevant to both the topic and local context of a conversation and uses them for generating a knowledge-grounded response.},
  file = {/Users/janwardenga/Zotero/storage/QD3IAJXZ/Ahn et al. - 2022 - Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild.pdf}
}

@article{alanRAGbasedQuestionAnswering2024,
  title = {A {{RAG-based Question Answering System Proposal}} for {{Understanding Islam}}: {{MufassirQAS LLM}}},
  author = {Alan, Ahmet Yusuf and Karaarslan, Enis and Aydin, Omer},
  year = {2024},
  doi = {10.2139/ssrn.4707470},
  abstract = {Challenges exist in learning and understanding religions, such as the complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect for enlightenment on religion as a question-answering chatbot. However, LLMs also tend to generate false information, known as hallucination. Also, the chatbots' responses can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It must avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called ``MufassirQAS''. We created a database consisting of several open-access books that include Turkish context. These books contain Turkish translations and interpretations of Islam. This database is utilized to answer religion-related questions and ensure our answers are trustworthy. The relevant part of the dataset, which LLM also uses, is presented along with the answer. We have put careful effort into creating system prompts that give instructions to prevent harmful, offensive, or disrespectful responses to respect people's values and provide reliable results. The system answers and shares additional information, such as the page number from the respective book and the articles referenced for obtaining the information. MufassirQAS and ChatGPT are also tested with sensitive questions. We got better performance with our system. Study and enhancements are still in progress. Results and future works are given.},
  langid = {english},
  keywords = {Unread},
  annotation = {7 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs.},
  file = {/Users/janwardenga/Zotero/storage/MTDY98VG/Pre_2024_A RAG-based Question Answering System Proposal for Understanding Islam MufassirQAS LLM.pdf}
}

@misc{aliSurveyRDFStores2021,
  title = {A {{Survey}} of {{RDF Stores}} \& {{SPARQL Engines}} for {{Querying Knowledge Graphs}}},
  author = {Ali, Waqas and Saleem, Muhammad and Yao, Bin and Hogan, Aidan and Ngomo, Axel-Cyrille Ngonga},
  year = {2021},
  month = oct,
  number = {arXiv:2102.13027},
  eprint = {2102.13027},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2102.13027},
  urldate = {2024-09-26},
  abstract = {RDF has seen increased adoption in recent years, prompting the standardization of the SPARQL query language for RDF, and the development of local and distributed engines for processing SPARQL queries. This survey paper provides a comprehensive review of techniques and systems for querying RDF knowledge graphs. While other reviews on this topic tend to focus on the distributed setting, the main focus of the work is on providing a comprehensive survey of state-of-the-art storage, indexing and query processing techniques for efficiently evaluating SPARQL queries in a local setting (on one machine). To keep the survey selfcontained, we also provide a short discussion on graph partitioning techniques used in the distributed setting. We conclude by discussing contemporary research challenges for further improving SPARQL query engines. This extended version also provides a survey of over one hundred SPARQL query engines and the techniques they use, along with twelve benchmarks and their features.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Read},
  annotation = {73 citations (Semantic Scholar/arXiv) [2025-02-15]\\
26 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/TPZYIBW5/Pre_2021_A Survey of RDF Stores & SPARQL Engines for Querying Knowledge Graphs.pdf}
}

@misc{alshargiConcept2vecMetricsEvaluating2020,
  title = {Concept2vec: {{Metrics}} for {{Evaluating Quality}} of {{Embeddings}} for {{Ontological Concepts}}},
  shorttitle = {Concept2vec},
  author = {Alshargi, Faisal and Shekarpour, Saeedeh and Soru, Tommaso and Sheth, Amit},
  year = {2020},
  month = may,
  number = {arXiv:1803.04488},
  eprint = {1803.04488},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1803.04488},
  urldate = {2024-09-29},
  abstract = {Although there is an emerging trend towards generating embeddings for primarily unstructured data and, recently, for structured data, no systematic suite for measuring the quality of embeddings has been proposed yet. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of the encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework, multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts. We positioned our sampled data and code at https://github.com/alshargi/Concept2vec under GNU General Public License v3.0.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {12 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/PYPGLCUP/Pre_2020_Concept2vec.pdf}
}

@article{alucBuildingSelfclusteringRDF2019TheVLDBJournal,
  title = {Building Self-Clustering {{RDF}} Databases Using {{Tunable-LSH}}},
  author = {Alu{\c c}, G{\"u}ne{\c s} and {\"O}zsu, M. Tamer and Daudjee, Khuzaima},
  year = {2019},
  month = apr,
  journal = {The VLDB Journal},
  volume = {28},
  number = {2},
  pages = {173--195},
  issn = {1066-8888, 0949-877X},
  doi = {10.1007/s00778-018-0530-9},
  urldate = {2024-09-29},
  abstract = {The Resource Description Framework (RDF) is a W3C standard for representing graph-structured data, and SPARQL is the standard query language for RDF. Recent advances in information extraction, linked data management and the Semantic Web have led to a rapid increase in both the volume and the variety of RDF data that are publicly available. As businesses start to capitalize on RDF data, RDF data management systems are being exposed to workloads that are far more diverse and dynamic than what they were designed to handle. Consequently, there is a growing need for developing workload-adaptive and self-tuning RDF data management systems. To realize this objective, we introduce a fast and efficient method for dynamically clustering records in an RDF data management system. Specifically, we assume nothing about the workload upfront, but as SPARQL queries are executed, we keep track of records that are co-accessed by the queries in the workload and physically cluster them. To decide dynamically and in constant-time where a record needs to be placed in the storage system, we develop a new locality-sensitive hashing (LSH) scheme, Tunable-LSH. Using Tunable-LSH, records that are coaccessed across similar sets of queries can be hashed to the same or nearby physical pages in the storage system. What sets Tunable-LSH apart from existing LSH schemes is that it can auto-tune to achieve the aforementioned clustering objective with high accuracy even when the workloads change. Experimental evaluation of Tunable-LSH in an RDF data management system as well as in a standalone hashtable shows endto-end performance gains over existing solutions.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/STRH7FG2/VJ_2019_Building self-clustering RDF databases using Tunable-LSH.pdf}
}

@misc{alucClusteringRDFDatabases2015,
  title = {Clustering {{RDF Databases Using Tunable-LSH}}},
  author = {Alu{\c c}, G{\"u}ne{\c s} and {\"O}zsu, M. Tamer and Daudjee, Khuzaima},
  year = {2015},
  month = apr,
  number = {arXiv:1504.02523},
  eprint = {1504.02523},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1504.02523},
  urldate = {2024-09-29},
  abstract = {The Resource Description Framework (RDF) is a W3C standard for representing graph-structured data, and SPARQL is the standard query language for RDF. Recent advances in Information Extraction, Linked Data Management and the Semantic Web have led to a rapid increase in both the volume and the variety of RDF data that are publicly available. As businesses start to capitalize on RDF data, RDF data management systems are being exposed to workloads that are far more diverse and dynamic than what they were designed to handle. Consequently, there is a growing need for developing workload-adaptive and self-tuning RDF data management systems. To realize this vision, we introduce a fast and efficient method for dynamically clustering records in an RDF data management system. Specifically, we assume nothing about the workload upfront, but as SPARQL queries are executed, we keep track of records that are co-accessed by the queries in the workload and physically cluster them. To decide dynamically (hence, in constant-time) where a record needs to be placed in the storage system, we develop a new locality-sensitive hashing (LSH) scheme, TUNABLE-LSH. Using TUNABLE-LSH, records that are co-accessed across similar sets of queries can be hashed to the same or nearby physical pages in the storage system. What sets TUNABLE-LSH apart from existing LSH schemes is that it can auto-tune to achieve the aforementioned clustering objective with high accuracy even when the workloads change. Experimental evaluation of TUNABLE-LSH in our prototype RDF data management system, chameleon-db, as well as in a standalone hashtable shows significant end-to-end improvements over existing solutions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/7C8V5INI/Pre_2015_Clustering RDF Databases Using Tunable-LSH.pdf}
}

@misc{andriopoulosAugmentingLLMsKnowledge2023,
  title = {Augmenting {{LLMs}} with {{Knowledge}}: {{A}} Survey on Hallucination Prevention},
  shorttitle = {Augmenting {{LLMs}} with {{Knowledge}}},
  author = {Andriopoulos, Konstantinos and Pouwelse, Johan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16459},
  eprint = {2309.16459},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.16459},
  urldate = {2024-09-29},
  abstract = {Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {15 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/WZX9HLIQ/Pre_2023_Augmenting LLMs with Knowledge.pdf}
}

@misc{anthropic2024contextualretrieval,
  title = {Introducing Contextual Retrieval},
  author = {PBC, Anthropic},
  year = {2024},
  month = sep,
  url = {https://www.anthropic.com},
  keywords = {,Not citable but relevant,Read},
  file = {/Users/janwardenga/Zotero/storage/Q6WWH9AG/PBC - 2024 - Introducing contextual retrieval.pdf}
}

@inproceedings{araClosingKnowledgeGap2024Proc.29thInt.Conf.Intell.UserInterfaces,
  title = {Closing the {{Knowledge Gap}} in {{Designing Data Annotation Interfaces}} for {{AI-powered Disaster Management Analytic Systems}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Ara, Zinat and Salemi, Hossein and Hong, Sungsoo Ray and Senarath, Yasas and Peterson, Steve and Hughes, Amanda Lee and Purohit, Hemant},
  year = {2024},
  month = mar,
  pages = {405--418},
  publisher = {ACM},
  address = {Greenville SC USA},
  doi = {10.1145/3640543.3645214},
  urldate = {2024-09-29},
  abstract = {Data annotation interfaces predominantly leverage ground truth labels to guide annotators toward accurate responses. With the growing adoption of Artificial Intelligence (AI) in domain-specific professional tasks, it has become increasingly important to help beginning annotators identify how their early-stage knowledge can lead to inaccurate answers, which in turn, helps to ensure quality annotations at scale. To investigate this issue, we conducted a formative study involving eight individuals from the field of disaster management, each possessing varying levels of expertise. The goal was to understand the prevalent factors contributing to disagreements among annotators when classifying Twitter messages related to disasters and to analyze their respective responses. Our analysis identified two primary causes of disagreement between expert and beginner annotators: 1) a lack of contextual knowledge or uncertainty about the situation, and 2) the absence of visual or supplementary cues. Based on these findings, we designed a Context interface, which generates aids that help beginners identify potential mistakes and provide the hidden context of the presented tweet. The summative study compares Context design with two widely used designs in data annotation UI, Highlight and Reasoningbased interfaces. We found significant differences between these designs in terms of attitudinal and behavioral data. We conclude with implications for designing future interfaces aiming at closing the knowledge gap among annotators.},
  isbn = {9798400705083},
  langid = {english},
  keywords = {Jab/IICIUI,Unread},
  annotation = {2 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/EJ734YAH/IICIUI_2024_Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems.pdf}
}

@article{arenasFacetedSearchRDFBased2016SSRNJournal,
  title = {Faceted {{Search Over RDF-Based Knowledge Graphs}}},
  author = {Arenas, Marcelo and Cuenca Grau, Bernardo and Kharlamov, Evgeny and Marciuska, Sarunas and Zheleznyakov, Dimitriy},
  year = {2016},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3199228},
  urldate = {2024-09-26},
  langid = {english},
  keywords = {Jab/SEJ,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/4T366AFY/SEJ_2016_Faceted Search Over RDF-Based Knowledge Graphs.pdf}
}

@article{armantCanKnowledgeGraphs,
  title = {Can {{Knowledge Graphs}} and {{Retrieval-Augmented Generation}} Be Combined to {{Explain Query}}/{{Answer Relationships Truthfully}}?},
  author = {Armant, Vincent and Mouakher, Amira and {Vargas-Rojas}, Felipe and Symeonidou, Danai and Gu{\'e}rin, Joris and Mougenot, Isabelle and Desconnets, Jean-Christophe},
  abstract = {In recent years, there has been a significant increase in the adoption of Large Language Models (LLMs) by users in both academic and industrial fields. These powerful tools are progressively challenging the dominance of traditional keyword-based search engines in various fields. While advancements like Retrieval-Augmented Generation (RAG) are enabling LLMs to provide provenance and explanations, their widespread adoption remains hindered by some well-known limitations, including hallucinations (factual inconsistencies), outdated knowledge, and answer precision. In contrast, classical search engines do not suffer from these issues, thanks to recent progress that has made them both efficient and accurate. However, their output may lack interpretability compared to LLMs. This vision paper proposes a novel explanation system that bridges this gap. By integrating Knowledge Graphs with RAG, we aim to elucidate the semantic relationships between retrieved resources and user queries. Addressing this research question has the potential to enhance user trust and confidence in the utilization of explainable search engines.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/KXP2E3ZB/Armant et al. - Can Knowledge Graphs and Retrieval-Augmented Generation be combined to Explain QueryAnswer Relation.pdf}
}

@article{arnaoutEffectiveSearchingRDF2018SSRNJournal,
  title = {Effective {{Searching}} of {{RDF Knowledge Graphs}}},
  author = {Arnaout, Hiba and Elbassuoni, Shady},
  year = {2018},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3199315},
  urldate = {2024-09-26},
  abstract = {RDF knowledge graphs are typically searched using triple-pattern queries. Often, triple-pattern queries will return too many or too few results, making it difficult for users to find relevant answers to their information needs. To remedy this, we propose a general framework for effective searching of RDF knowledge graphs. Our framework extends both the searched knowledge graph and triple-pattern queries with keywords to allow users to form a wider range of queries. In addition, it provides result ranking based on statistical machine translation, and performs automatic query relaxation to improve query recall. Finally, we also define a notion of result diversity in the setting of RDF data and provide mechanisms to diversify RDF search results using Maximal Marginal Relevance. We evaluate the effectiveness of our retrieval framework using various carefully-designed user studies on DBpedia, a large and real-world RDF knowledge graph.},
  langid = {english},
  keywords = {,Jab/SEJ,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/P9CG3LEM/SEJ_2018_Effective Searching of RDF Knowledge Graphs.pdf}
}

@incollection{auerDBpediaNucleusWeb2007TheSemanticWeb,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  volume = {4825},
  pages = {722--735},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-76298-0_52},
  urldate = {2025-01-27},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76297-3 978-3-540-76298-0},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/JE87RZK3/Auer et al. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf}
}

@incollection{auerLODStatsExtensibleFramework2012KnowledgeEngineeringandKnowledgeManagement,
  title = {{{LODStats}} -- {{An Extensible Framework}} for {{High-Performance Dataset Analytics}}},
  booktitle = {Knowledge {{Engineering}} and {{Knowledge Management}}},
  author = {Auer, S{\"o}ren and Demter, Jan and Martin, Michael and Lehmann, Jens},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ten Teije, Annette and V{\"o}lker, Johanna and Handschuh, Siegfried and Stuckenschmidt, Heiner and {d'Acquin}, Mathieu and Nikolov, Andriy and {Aussenac-Gilles}, Nathalie and Hernandez, Nathalie},
  year = {2012},
  volume = {7603},
  pages = {353--362},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33876-2_31},
  urldate = {2025-01-23},
  abstract = {One of the major obstacles for a wider usage of web data is the difficulty to obtain a clear picture of the available datasets. In order to reuse, link, revise or query a dataset published on the Web it is important to know the structure, coverage and coherence of the data. In order to obtain such information we developed LODStats -- a statement-stream-based approach for gathering comprehensive statistics about datasets adhering to the Resource Description Framework (RDF). LODStats is based on the declarative description of statistical dataset characteristics. Its main advantages over other approaches are a smaller memory footprint and significantly better performance and scalability. We integrated LODStats with the CKAN dataset metadata registry and obtained a comprehensive picture of the current state of a significant part of the Data Web.},
  isbn = {978-3-642-33875-5 978-3-642-33876-2},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/A4V3JYIQ/Auer et al. - 2012 - LODStats – An Extensible Framework for High-Performance Dataset Analytics.pdf}
}

@misc{bahrKnowledgeGraphEnhanced2024,
  title = {Knowledge {{Graph Enhanced Retrieval-Augmented Generation}} for {{Failure Mode}} and {{Effects Analysis}}},
  author = {Bahr, Lukas and Wehner, Christoph and Wewerka, Judith and Bittencourt, Jos{\'e} and Schmid, Ute and Daub, R{\"u}diger},
  year = {2024},
  month = jul,
  number = {arXiv:2406.18114},
  eprint = {2406.18114},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.18114},
  urldate = {2024-09-30},
  abstract = {Failure mode and effects analysis (FMEA) is a critical tool for mitigating potential failures, particular during ramp-up phases of new products. However, its effectiveness is often limited by the missing reasoning capabilities of the FMEA tools, which are usually tabular structured. Meanwhile, large language models (LLMs) offer novel prospects for fine-tuning on custom datasets for reasoning within FMEA contexts. However, LLMs face challenges in tasks that require factual knowledge, a gap that retrieval-augmented generation (RAG) approaches aim to fill. RAG retrieves information from a non-parametric data store and uses a language model to generate responses. Building on this idea, we propose to advance the non-parametric data store with a knowledge graph (KG). By enhancing the RAG framework with a KG, our objective is to leverage analytical and semantic question-answering capabilities on FMEA data. This paper contributes by presenting a new ontology for FMEA observations, an algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced RAG framework. Our approach is validated through a human study and we measure the performance of the context retrieval recall and precision.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/PR3RFLRQ/Bahr et al. - 2024 - Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode and Effects Analysis.pdf}
}

@article{beltranAIAppliedKnowledge,
  title = {{{AI}} Applied to Knowledge Graphs: {{NLP}} as a Mean to Enhance the Graph},
  author = {Beltr{\'a}n, Iv{\'a}n Llopis},
  langid = {english},
  keywords = {No DOI found,Not citable but relevant,Read},
  file = {/Users/janwardenga/Zotero/storage/ZUSC5MU6/Pre__AI applied to knowledge graphs NLP as a mean to enhance the graph.pdf}
}

@article{benellefiRDFDatasetProfiling2018SW,
  title = {{{RDF}} Dataset Profiling -- a Survey of Features, Methods, Vocabularies and Applications},
  author = {Ben Ellefi, Mohamed and Bellahsene, Zohra and Breslin, John G. and Demidova, Elena and Dietze, Stefan and Szyma{\'n}ski, Julian and Todorov, Konstantin},
  editor = {Aroyo, Lora},
  year = {2018},
  month = aug,
  journal = {Semantic Web},
  volume = {9},
  number = {5},
  pages = {677--705},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-180294},
  urldate = {2025-01-23},
  abstract = {The Web of Data, and in particular Linked Data, has seen tremendous growth over the past years. However, reuse and take-up of these rich data sources is often limited and focused on a few well-known and established RDF datasets. This can be partially attributed to the lack of reliable and up-to-date information about the characteristics of available datasets. While RDF datasets vary heavily with respect to the features related to quality, provenance, interlinking, licenses, statistics and dynamics, reliable information about such features is essential to enable dataset discovery and selection in tasks such as entity linking, distributed query, search or question answering. Even though there exists a wealth of works contributing to the task of dataset profiling in general, these works are spread across a wide range of communities. In this survey, we provide a first comprehensive overview of the RDF dataset profiling features, methods, tools and vocabularies. We organize these building blocks of dataset profiling in a taxonomy and illustrate the links between the dataset profiling and feature extraction approaches and several application domains. This survey is aimed towards data practitioners, data providers and scientists, spanning a large range of communities and drawing from different fields such as dataset profiling, assessment, summarization and characterization. Ultimately, this work is intended to facilitate the reader to identify the relevant features for building a dataset profile for intended applications together with the methods and tools capable of extracting these features from the datasets as well as vocabularies to describe the extracted features and make them available.},
  langid = {english},
  keywords = {Unread},
  annotation = {70 citations (Semantic Scholar/DOI) [2025-02-15]\\
70 citations (Semantic Scholar/DOI) [2025-01-23]\\
TLDR: A first comprehensive survey of the RDF dataset profile features, methods, tools and vocabularies is provided, organized in a taxonomy and emphasized the links between the dataset profiling and feature extraction approaches and several application domains.},
  file = {/Users/janwardenga/Zotero/storage/2JX93YQ4/Ben Ellefi et al. - 2018 - RDF dataset profiling – a survey of features, methods, vocabularies and applications.pdf}
}

@inproceedings{bennettSupportVectorDecision1999IJCNN99Int.Jt.Conf.NeuralNetw.Proc.CatNo99CH36339,
  title = {On Support Vector Decision Trees for Database Marketing},
  booktitle = {{{IJCNN}}'99. {{International Joint Conference}} on {{Neural Networks}}. {{Proceedings}} ({{Cat}}. {{No}}.{{99CH36339}})},
  author = {Bennett, K.P. and Wu, S. and Auslender, L.},
  year = {1999},
  volume = {2},
  pages = {904--909},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  doi = {10.1109/IJCNN.1999.831073},
  urldate = {2024-09-29},
  isbn = {978-0-7803-5529-3},
  langid = {english},
  keywords = {Jab/ICNN,Unread},
  annotation = {41 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A support vector decision tree method for customer targeting in the framework of large databases (database marketing) is introduced to provide a tool to identify the best customers based on historical data.},
  file = {/Users/janwardenga/Zotero/storage/ZUSZEQRX/ICNN_1999_On support vector decision trees for database marketing.pdf}
}

@misc{benschAIAssistantsSpaceflight2024,
  title = {{{AI Assistants}} for {{Spaceflight Procedures}}: {{Combining Generative Pre-Trained Transformer}} and {{Retrieval-Augmented Generation}} on {{Knowledge Graphs With Augmented Reality Cues}}},
  shorttitle = {{{AI Assistants}} for {{Spaceflight Procedures}}},
  author = {Bensch, Oliver and Bensch, Leonie and Nilsson, Tommy and Saling, Florian and Bewer, Bernd and Jentzsch, Sophie and Hecking, Tobias and Kutz, J. Nathan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.14206},
  eprint = {2409.14206},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.14206},
  urldate = {2024-09-30},
  abstract = {This paper describes the capabilities and potential of the intelligent personal assistant (IPA) CORE (Checklist Organizer for Research and Exploration), designed to support astronauts during procedures onboard the International Space Station (ISS), the Lunar Gateway station, and beyond. We reflect on the importance of a reliable and flexible assistant capable of offline operation and highlight the usefulness of audiovisual interaction using augmented reality elements to intuitively display checklist information. We argue that current approaches to the design of IPAs in space operations fall short of meeting these criteria. Therefore, we propose CORE as an assistant that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements to ensure an intuitive understanding of procedure steps, reliability, offline availability, and flexibility in terms of response style and procedure updates.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T01 68T20 68T30 68T50 68T05,Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,H.5,I.2,Unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/2K8QSC4B/Bensch et al. - 2024 - AI Assistants for Spaceflight Procedures Combining Generative Pre-Trained Transformer and Retrieval.pdf}
}

@article{bestaGraphThoughtsSolving2024AAAI,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  eprint = {2308.09687},
  primaryclass = {cs},
  pages = {17682--17690},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29720},
  urldate = {2024-11-17},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (``LLM thoughts'') are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Jab/PACAI,Unread},
  annotation = {429 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: Graph of Thoughts is introduced: a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts, and is ensured that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes.},
  file = {/Users/janwardenga/Zotero/storage/RU2DFTHH/PACAI_2024_Graph of Thoughts.pdf}
}

@article{blagecCuratedOntologybasedLargescale2022SciData,
  title = {A Curated, Ontology-Based, Large-Scale Knowledge Graph of Artificial Intelligence Tasks and Benchmarks},
  author = {Blagec, Kathrin and {Barbosa-Silva}, Adriano and Ott, Simon and Samwald, Matthias},
  year = {2022},
  month = jun,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {322},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01435-x},
  urldate = {2024-09-29},
  abstract = {Abstract             Research in artificial intelligence (AI) is addressing a growing number of tasks through a rapidly growing number of models and methodologies. This makes it difficult to keep track of where novel AI methods are successfully -- or still unsuccessfully -- applied, how progress is measured, how different advances might synergize with each other, and how future research should be prioritized. To help address these issues, we created the Intelligence Task Ontology and Knowledge Graph (ITO), a comprehensive, richly structured and manually curated resource on artificial intelligence tasks, benchmark results and performance metrics. The current version of ITO contains 685,560 edges, 1,100 classes representing AI processes and 1,995 properties representing performance metrics. The primary goal of ITO is to enable analyses of the global landscape of AI tasks and capabilities. ITO is based on technologies that allow for easy integration and enrichment with external data, automated inference and continuous, collaborative expert curation of underlying ontological models. We make the ITO dataset and a collection of Jupyter notebooks utilizing ITO openly available.},
  langid = {english},
  keywords = {Jab/SD,Unread},
  annotation = {24 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: The Intelligence Task Ontology and Knowledge Graph is created, a comprehensive, richly structured and manually curated resource on artificial intelligence tasks, benchmark results and performance metrics, to enable analyses of the global landscape of AI tasks and capabilities.},
  file = {/Users/janwardenga/Zotero/storage/RXJRNDA7/SD_2022_A curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks.pdf}
}

@article{bordesLearningStructuredEmbeddings2011AAAI,
  title = {Learning {{Structured Embeddings}} of {{Knowledge Bases}}},
  author = {Bordes, Antoine and Weston, Jason and Collobert, Ronan and Bengio, Yoshua},
  year = {2011},
  month = aug,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {25},
  number = {1},
  pages = {301--306},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v25i1.7917},
  urldate = {2024-09-29},
  abstract = {Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigid symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like natural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning methods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.},
  langid = {english},
  keywords = {Unread},
  annotation = {895 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/QVUF3JQ8/PACAI_2011_Learning Structured Embeddings of Knowledge Bases.pdf}
}

@article{braticCentralizedDatabaseAccess2024ASI,
  title = {Centralized {{Database Access}}: {{Transformer Framework}} and {{LLM}}/{{Chatbot Integration-Based Hybrid Model}}},
  shorttitle = {Centralized {{Database Access}}},
  author = {Brati{\'c}, Diana and {\v S}apina, Marko and Jure{\v c}i{\'c}, Denis and {\v Z}iljak Gr{\v s}i{\'c}, Jana},
  year = {2024},
  month = feb,
  journal = {Applied System Innovation},
  volume = {7},
  number = {1},
  pages = {17},
  issn = {2571-5577},
  doi = {10.3390/asi7010017},
  urldate = {2024-09-29},
  abstract = {This paper addresses the challenges associated with the centralized storage of educational materials in the context of a fragmented and disparate database. In response to the increasing demands of modern education, efficient and accessible retrieval of materials for educators and students is essential. This paper presents a hybrid model based on the transformer framework and utilizing an API for an existing large language model (LLM)/chatbot. This integration ensures precise responses drawn from a comprehensive educational materials database. The model architecture uses mathematically defined algorithms for precise functions that enable deep text processing through advanced word embedding methods. This approach improves accuracy in natural language processing and ensures both high efficiency and adaptability. Therefore, this paper not only provides a technical solution to a prevalent problem but also highlights the potential for the continued development and integration of emerging technologies in education. The aim is to create a more efficient, transparent, and accessible educational environment. The importance of this research lies in its ability to streamline material access, benefiting the global scientific community and contributing to the continuous advancement of educational technology.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Jab/ASI,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/5J6PCJ7G/ASI_2024_Centralized Database Access.pdf}
}

@article{broekstraEnablingKnowledgeRepresentation2002ComputerNetworks,
  title = {Enabling Knowledge Representation on the {{Web}} by Extending {{RDF Schema}}},
  author = {Broekstra, Jeen and Klein, Michel and Decker, Stefan and Fensel, Dieter and Van Harmelen, Frank and Horrocks, Ian},
  year = {2002},
  month = aug,
  journal = {Computer Networks},
  volume = {39},
  number = {5},
  pages = {609--634},
  issn = {13891286},
  doi = {10.1016/S1389-1286(02)00217-7},
  urldate = {2024-09-29},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/BVYQJAE9/Pre__Enabling knowledge representation on the Web by extending RDF Schema.pdf;/Users/janwardenga/Zotero/storage/FUDVRWLC/Broekstra et al. - 2002 - Enabling knowledge representation on the Web by extending RDF Schema.pdf}
}

@misc{bruckhausRAGDoesNot2024,
  title = {{{RAG Does Not Work}} for {{Enterprises}}},
  author = {Bruckhaus, Tilmann},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.04369},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of large language model outputs by incorporating knowledge retrieval. However, implementing RAG in enterprises poses challenges around data security, accuracy, scalability, and integration. This paper explores the unique requirements for enterprise RAG, surveys current approaches and limitations, and discusses potential advances in semantic search, hybrid queries, and optimized retrieval. It proposes an evaluation framework to validate enterprise RAG solutions, including quantitative testing, qualitative analysis, ablation studies, and industry case studies. This framework aims to help demonstrate the ability of purpose-built RAG architectures to deliver accuracy and relevance improvements with enterprise-grade security, compliance and integration. The paper concludes with implications for enterprise deployments, limitations, and future research directions. Close collaboration between researchers and industry partners may accelerate progress in developing and deploying retrieval-augmented generation technology.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {,Read,Software Engineering (cs.SE)},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-20]\\
TLDR: This paper proposes an evaluation framework to validate enterprise RAG solutions, including quantitative testing, qualitative analysis, ablation studies, and industry case studies, to help demonstrate the ability of purpose-built RAG architectures to deliver accuracy and relevance improvements with enterprise-grade security, compliance and integration.},
  file = {/Users/janwardenga/Zotero/storage/QJHSQBIP/Pre__RAG Does Not Work for Enterprises.pdf}
}

@article{buehlerGenerativeRetrievalAugmentedOntologic2024ACSEng.Au,
  title = {Generative {{Retrieval-Augmented Ontologic Graph}} and {{Multiagent Strategies}} for {{Interpretive Large Language Model-Based Materials Design}}},
  author = {Buehler, Markus J.},
  year = {2024},
  month = apr,
  journal = {ACS Engineering Au},
  volume = {4},
  number = {2},
  pages = {241--277},
  issn = {2694-2488, 2694-2488},
  doi = {10.1021/acsengineeringau.3c00058},
  urldate = {2024-11-14},
  abstract = {Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design, and manufacturing, including their capacity to work effectively with human language, symbols, code, and numerical data. Here, we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. Moreover, when used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem-solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how fine-tuning endows LLMs with a reasonable understanding of subject area knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty recalling correct information and may hallucinate. We show how this can be addressed using retrievalaugmented Ontological Knowledge Graph strategies. The graph-based strategy helps us not only to discern how the model understands what concepts are important but also how they are related, which significantly improves generative performance and also naturally allows for injection of new and augmented data sources into generative AI algorithms. We find that the additional feature of relatedness provides advantages over regular retrieval augmentation approaches and not only improves LLM performance but also provides mechanistic insights for exploration of a material design process. Illustrated for a use case of relating distinct areas of knowledge, here, music and proteins, such strategies can also provide an interpretable graph structure with rich information at the node, edge, and subgraph level that provides specific insights into mechanisms and relationships. We discuss other approaches to improve generative qualities, including nonlinear sampling strategies and agent-based modeling that offer enhancements over singleshot generations, whereby LLMs are used to both generate content and assess content against an objective target. Examples provided include complex question answering, code generation, and execution in the context of automated force-field development from actively learned density functional theory (DFT) modeling and data analysis.},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {21 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work explores the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation.},
  file = {/Users/janwardenga/Zotero/storage/TNCJDSD9/Buehler - 2024 - Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Lang.pdf}
}

@inproceedings{buiCrossDataKnowledgeGraph2024Proc.1stACMWorkshopAI-PoweredQASyst.Multimed.,
  title = {Cross-{{Data Knowledge Graph Construction}} for {{LLM-enabled Educational Question-Answering System}}: {{A Case Study}} at {{HCMUT}}},
  shorttitle = {Cross-{{Data Knowledge Graph Construction}} for {{LLM-enabled Educational Question-Answering System}}},
  booktitle = {Proceedings of the 1st {{ACM Workshop}} on {{AI-Powered Q}}\&{{A Systems}} for {{Multimedia}}},
  author = {Bui, Tuan and Tran, Oanh and Nguyen, Phuong and Ho, Bao and Nguyen, Long and Bui, Thang and Quan, Tho},
  year = {2024},
  month = jun,
  eprint = {2404.09296},
  primaryclass = {cs},
  pages = {36--43},
  doi = {10.1145/3643479.3662055},
  urldate = {2024-09-29},
  abstract = {In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domainspecific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {14 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/95JGDZSA/Pre_2024_Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System.pdf}
}

@misc{bulgakovDimensionalityReductionSentence2024,
  title = {Dimensionality {{Reduction}} in {{Sentence Transformer Vector Databases}} with {{Fast Fourier Transform}}},
  author = {Bulgakov, Vitaly and Segal, Alec},
  year = {2024},
  month = apr,
  number = {arXiv:2404.06278},
  eprint = {2404.06278},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.06278},
  urldate = {2024-09-29},
  abstract = {Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/8W8TQQ3M/Pre_2024_Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform.pdf}
}

@misc{bustamanteSPARQLGenerationEntity2024,
  title = {{{SPARQL Generation}} with {{Entity Pre-trained GPT}} for {{KG Question Answering}}},
  author = {Bustamante, Diego and Takeda, Hideaki},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00969},
  eprint = {2402.00969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.00969},
  urldate = {2025-02-26},
  abstract = {Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703\% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Information Retrieval,Jab/Pre,Read},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-26]\\
TLDR: This work managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and proposed pre-training on all entities (under CWA) to improve the performance.},
  file = {/Users/janwardenga/Zotero/storage/SVQV8TBF/Pre_2024_SPARQL Generation with Entity Pre-trained GPT for KG Question Answering.pdf;/Users/janwardenga/Zotero/storage/GLYS26WL/2402.html}
}

@inproceedings{caffagniWikiLLaVAHierarchicalRetrievalAugmented20242024IEEECVFConf.Comput.Vis.PatternRecognit.WorkshopCVPRW,
  title = {Wiki-{{LLaVA}}: {{Hierarchical Retrieval-Augmented Generation}} for {{Multimodal LLMs}}},
  shorttitle = {Wiki-{{LLaVA}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Caffagni, Davide and Cocchi, Federico and Moratelli, Nicholas and Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  year = {2024},
  month = jun,
  pages = {1818--1826},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPRW63382.2024.00188},
  urldate = {2024-09-29},
  abstract = {Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350365474},
  langid = {english},
  keywords = {Jab/CVPRW,Unread},
  annotation = {24 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/MDAMREB4/CVPRW_2024_Wiki-LLaVA.pdf}
}

@misc{cakalogluTextEmbeddingsRetrieval2019,
  title = {Text {{Embeddings}} for {{Retrieval From}} a {{Large Knowledge Base}}},
  author = {Cakaloglu, Tolgahan and Szegedy, Christian and Xu, Xiaowei},
  year = {2019},
  month = may,
  number = {arXiv:1810.10176},
  eprint = {1810.10176},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1810.10176},
  urldate = {2024-09-29},
  abstract = {Text embedding representing natural language documents in a semantic vector space can be used for document retrieval using nearest neighbor lookup. In order to study the feasibility of neural models specialized for retrieval in a semantically meaningful way, we suggest the use of the Stanford Question Answering Dataset (SQuAD) in an open-domain question answering context, where the first task is to find paragraphs useful for answering a given question. First, we compare the quality of various text-embedding methods on the performance of retrieval and give an extensive empirical comparison on the performance of various non-augmented base embedding with, and without IDF weighting. Our main results are that by training deep residual neural models, specifically for retrieval purposes, can yield significant gains when it is used to augment existing embeddings. We also establish that deeper models are superior to this task. The best base baseline embeddings augmented by our learned neural approach improves the top-1 paragraph recall of the system by 14\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {13 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/SSG7KIX2/Pre_2019_Text Embeddings for Retrieval From a Large Knowledge Base.pdf}
}

@misc{changCommunityKGRAGLeveragingCommunity2024,
  title = {{{CommunityKG-RAG}}: {{Leveraging Community Structures}} in {{Knowledge Graphs}} for {{Advanced Retrieval-Augmented Generation}} in {{Fact-Checking}}},
  shorttitle = {{{CommunityKG-RAG}}},
  author = {Chang, Rong-Ching and Zhang, Jiawei},
  year = {2024},
  month = aug,
  number = {arXiv:2408.08535},
  eprint = {2408.08535},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.08535},
  urldate = {2024-09-30},
  abstract = {Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems, their effectiveness is often hindered by a lack of integration with entity relationships and community structures, limiting their ability to provide contextually rich and accurate information retrieval for fact-checking. We introduce CommunityKG-RAG (Community Knowledge Graph-Retrieval Augmented Generation), a novel zero-shot framework that integrates community structures within Knowledge Graphs (KGs) with RAG systems to enhance the fact-checking process. Capable of adapting to new domains and queries without additional training, CommunityKG-RAG utilizes the multi-hop nature of community structures within KGs to significantly improve the accuracy and relevance of information retrieval. Our experimental results demonstrate that CommunityKG-RAG outperforms traditional methods, representing a significant advancement in fact-checking by offering a robust, scalable, and efficient solution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/3WCXRUQ4/Chang and Zhang - 2024 - CommunityKG-RAG Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmente.pdf}
}

@misc{chanRQRAGLearningRefine2024,
  title = {{{RQ-RAG}}: {{Learning}} to {{Refine Queries}} for {{Retrieval Augmented Generation}}},
  shorttitle = {{{RQ-RAG}}},
  author = {Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
  year = {2024},
  month = mar,
  number = {arXiv:2404.00610},
  eprint = {2404.00610},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.00610},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {36 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/R84QIRX4/Pre_2024_RQ-RAG.pdf}
}

@article{chatzakisRDFsimSimilarityBasedBrowsing2021Information,
  title = {{{RDFsim}}: {{Similarity-Based Browsing}} over {{DBpedia Using Embeddings}}},
  shorttitle = {{{RDFsim}}},
  author = {Chatzakis, Manos and Mountantonakis, Michalis and Tzitzikas, Yannis},
  year = {2021},
  month = oct,
  journal = {Information},
  volume = {12},
  number = {11},
  pages = {440},
  issn = {2078-2489},
  doi = {10.3390/info12110440},
  urldate = {2024-09-29},
  abstract = {Browsing has been the core access method for the Web from its beginning. Analogously, one good practice for publishing data on the Web is to support dereferenceable URIs, to also enable plain web browsing by users. The information about one URI is usually presented through HTML tables (such as DBpedia and Wikidata pages) and graph representations (by using tools such as LODLive and LODMilla). In most cases, for an entity, the user gets all triples that have that entity as subject or as object. However, sometimes the number of triples is numerous. To tackle this issue, and to reveal similarity (and thus facilitate browsing), in this article we introduce an interactive similarity-based browsing system, called RDFsim, that offers ``Parallel Browsing'', that is, it enables the user to see and browse not only the original data of the entity in focus, but also the K most similar entities of the focal entity. The similarity of entities is founded on knowledge graph embeddings; however, the indexes that we introduce for enabling real-time interaction do not depend on the particular method for computing similarity. We detail an implementation of the approach over specific subsets of DBpedia (movies, philosophers and others) and we showcase the benefits of the approach. Finally, we report detailed performance results and we describe several use cases of RDFsim.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: An interactive similarity-based browsing system, called RDFsim, that offers ``Parallel Browsing'', that enables the user to see and browse not only the original data of the entity in focus, but also the most similar entities of the focal entity.},
  file = {/Users/janwardenga/Zotero/storage/TKJXLCEV/I_2021_RDFsim.pdf}
}

@article{chaudhriKnowledgeGraphsIntroduction2022AIMagazine,
  title = {Knowledge Graphs: {{Introduction}}, History, and Perspectives},
  shorttitle = {Knowledge Graphs},
  author = {Chaudhri, Vinay K. and Baru, Chaitanya and Chittar, Naren and Dong, Xin Luna and Genesereth, Michael and Hendler, James and Kalyanpur, Aditya and Lenat, Douglas B. and Sequeda, Juan and Vrande{\v c}i{\'c}, Denny and Wang, Kuansan},
  year = {2022},
  month = mar,
  journal = {AI Magazine},
  volume = {43},
  number = {1},
  pages = {17--29},
  issn = {0738-4602, 2371-9621},
  doi = {10.1002/aaai.12033},
  urldate = {2024-09-29},
  abstract = {Knowledge graphs (KGs) have emerged as a compelling abstraction for organizing the world's structured knowledge and for integrating information extracted from multiple data sources. They are also beginning to play a central role in representing information extracted by AI systems, and for improving the predictions of AI systems by giving them knowledge expressed in KGs as input. The goals of this article are to (a) introduce KGs and discuss important areas of application that have gained recent prominence; (b) situate KGs in the context of the prior work in AI; and (c) present a few contrasting perspectives that help in better understanding KGs in relation to related technologies.},
  langid = {english},
  keywords = {Jab/AM,Unread},
  file = {/Users/janwardenga/Zotero/storage/5L6VQ9X3/AM_2022_Knowledge graphs.pdf}
}

@misc{chawlaKARLTransNERKnowledgeAware2021,
  title = {{{KARL-Trans-NER}}: {{Knowledge Aware Representation Learning}} for {{Named Entity Recognition}} Using {{Transformers}}},
  shorttitle = {{{KARL-Trans-NER}}},
  author = {Chawla, Avi and Mulay, Nidhi and Bishnoi, Vikas and Dhama, Gaurav},
  year = {2021},
  month = nov,
  number = {arXiv:2111.15436},
  eprint = {2111.15436},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2111.15436},
  urldate = {2025-02-25},
  abstract = {The inception of modeling contextual information using models such as BERT, ELMo, and Flair has significantly improved representation learning for words. It has also given SOTA results in almost every NLP task - Machine Translation, Text Summarization and Named Entity Recognition, to name a few. In this work, in addition to using these dominant context-aware representations, we propose a Knowledge Aware Representation Learning (KARL) Network for Named Entity Recognition (NER). We discuss the challenges of using existing methods in incorporating world knowledge for NER and show how our proposed methods could be leveraged to overcome those challenges. KARL is based on a Transformer Encoder that utilizes large knowledge bases represented as fact triplets, converts them to a graph context, and extracts essential entity information residing inside to generate contextualized triplet representation for feature augmentation. Experimental results show that the augmentation done using KARL can considerably boost the performance of our NER system and achieve significantly better results than existing approaches in the literature on three publicly available NER datasets, namely CoNLL 2003, CoNLL++, and OntoNotes v5. We also observe better generalization and application to a real-world setting from KARL on unseen entities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-25]},
  file = {/Users/janwardenga/Zotero/storage/9M3YH88Q/Chawla et al. - 2021 - KARL-Trans-NER Knowledge Aware Representation Learning for Named Entity Recognition using Transform.pdf;/Users/janwardenga/Zotero/storage/MAZU8UJ3/2111.html}
}

@article{chenBenchmarkingLargeLanguage2024AAAI,
  title = {Benchmarking {{Large Language Models}} in {{Retrieval-Augmented Generation}}},
  author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {17754--17762},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29728},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/PT2L9J9D/PACAI_2024_Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf}
}

@misc{chengLiftYourselfRetrievalaugmented2023,
  title = {Lift {{Yourself Up}}: {{Retrieval-augmented Text Generation}} with {{Self Memory}}},
  shorttitle = {Lift {{Yourself Up}}},
  author = {Cheng, Xin and Luo, Di and Chen, Xiuying and Liu, Lemao and Zhao, Dongyan and Yan, Rui},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.02437},
  urldate = {2024-09-29},
  abstract = {With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation{\textasciitilde}(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Read},
  annotation = {69 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper proposes a novel framework, selfmem, which addresses the duality of the primal problem: better generation also prompts better memory by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round.},
  file = {/Users/janwardenga/Zotero/storage/FMJ2JM49/Pre__Lift Yourself Up Retrieval-augmented Text Generation with Self-Memory.pdf}
}

@misc{chengUnifiedActiveRetrieval2024,
  title = {Unified {{Active Retrieval}} for {{Retrieval Augmented Generation}}},
  author = {Cheng, Qinyuan and Li, Xiaonan and Li, Shimin and Zhu, Qin and Yin, Zhangyue and Shao, Yunfan and Li, Linyang and Sun, Tianxiang and Yan, Hang and Qiu, Xipeng},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12534},
  eprint = {2406.12534},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.12534},
  urldate = {2024-09-29},
  abstract = {In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and applying it to every instruction is sub-optimal. Therefore, determining whether to retrieve is crucial for RAG, which is usually referred to as Active Retrieval. However, existing active retrieval methods face two challenges: 1. They usually rely on a single criterion, which struggles with handling various types of instructions. 2. They depend on specialized and highly differentiated procedures, and thus combining them makes the RAG system more complicated and leads to higher response latency. To address these challenges, we propose Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts them into plug-and-play classification tasks, which achieves multifaceted retrieval timing judgements with negligible extra inference cost. We further introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to process diverse active retrieval scenarios through a standardized procedure. Experiments on four representative types of user instructions show that UAR significantly outperforms existing work on the retrieval timing judgement and the performance of downstream tasks, which shows the effectiveness of UAR and its helpfulness to downstream tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CQJAEFTF/Pre_2024_Unified Active Retrieval for Retrieval Augmented Generation.pdf}
}

@misc{chenIntegratingMultiHeadConvolutional2024,
  title = {Integrating {{Multi-Head Convolutional Encoders}} with {{Cross-Attention}} for {{Improved SPARQL Query Translation}}},
  author = {Chen, Yi-Hui and Lu, Eric Jui-Lin and Cheng, Kwan-Ho},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2408.13432},
  urldate = {2025-02-20},
  abstract = {The main task of the KGQA system (Knowledge Graph Question Answering) is to convert user input questions into query syntax (such as SPARQL). With the rise of modern popular encoders and decoders like Transformer and ConvS2S, many scholars have shifted the research direction of SPARQL generation to the Neural Machine Translation (NMT) architecture or the generative AI field of Text-to-SPARQL. In NMT-based QA systems, the system treats knowledge base query syntax as a language. It uses NMT-based translation models to translate natural language questions into query syntax. Scholars use popular architectures equipped with cross-attention, such as Transformer, ConvS2S, and BiLSTM, to train translation models for query syntax. To achieve better query results, this paper improved the ConvS2S encoder and added multi-head attention from the Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the n-gram language model. The principle is to use convolutional layers to capture local hidden features in the input sequence with different receptive fields, using multi-head attention to calculate dependencies between them. Ultimately, we found that the translation model based on the Multi-Head Conv encoder achieved better performance than other encoders, obtaining 76.52{\textbackslash}\% and 83.37{\textbackslash}\% BLEU-1 (BiLingual Evaluation Understudy) on the QALD-9 and LC-QuAD-1.0 datasets, respectively. Additionally, in the end-to-end system experiments on the QALD-9 and LC-QuAD-1.0 datasets, we achieved leading results over other KGQA systems, with Macro F1-measures reaching 52{\textbackslash}\% and 66{\textbackslash}\%, respectively. Moreover, the experimental results show that with limited computational resources, if one possesses an excellent encoder-decoder architecture and cross-attention, experts and scholars can achieve outstanding performance equivalent to large pre-trained models using only general embeddings.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: Improved the ConvS2S encoder and added multi-head attention from the Transformer, proposing a Multi-Head Conv encoder (MHC encoder) based on the n-gram language model, finding that the translation model based on the Multi-Head Conv encoder achieved better performance than other encoders.}
}

@article{chenIntelligentSPARQLQuery2021IEEEAccess,
  title = {Intelligent {{SPARQL Query Generation}} for {{Natural Language Processing Systems}}},
  author = {Chen, Yi-Hui and Lu, Eric Jui-Lin and Ou, Ting-An},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {158638--158650},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3130667},
  urldate = {2025-02-20},
  abstract = {Developing question answering (QA) systems that process natural language is a popular research topic. Conventionally, when QA systems receive a natural language question, they choose useful words or phrases based on their parts-of-speech (POS) tags. In general, words tagged as nouns are mapped to class entities, words tagged as verbs are mapped to property entities, and words tagged as proper nouns are mapped to named entities, although the accuracy of entity type identification remains low. Afterward, the relationship between entity types as RDF types determines the first element to be a pivot word to generate the SPARQL (acronym for SPARQL protocol and RDF query language) query on the basis of the sequences by a specific graph or tree structure, such as dependence tree or directed acyclic graph (DAG). However, the generated SPARQL query is difficult to adapt to the given query request in that the sequences are decided by a fixed structure. Unlike in previous research, SPARQL generation occurs automatically according to the entity type identification and RDF type identification results. This study attempts to design a method that leverages machine learning to learn human experiences in entity type identification as well as RDF-type identification. We approach the problem as a multiclass classification problem and propose a two-stage maximum-entropy Markov model (MEMM). The first stage identifies the entity type and the second identifies the RDF type for the purpose of generating appropriate SPARQL queries to meet the query request. Along with the templates designed for the two-stage MEMM model, we develop an automatic question answering prototype system called QAWizard. The experimental results show that QAWizard outperforms all other systems in question answering when evaluated on Linked Data version 8 (QALD-8) metrics.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  keywords = {Jab/A,Read},
  annotation = {11 citations (Semantic Scholar/DOI) [2025-02-20]\\
11 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: This study attempts to design a method that leverages machine learning to learn human experiences in entity type identification as well as RDF-type identification and develops an automatic question answering prototype system called QAWizard.},
  file = {/Users/janwardenga/Zotero/storage/D5HUFXGW/A_2021_Intelligent SPARQL Query Generation for Natural Language Processing Systems.pdf}
}

@inproceedings{chenIvectorBasedLanguage20142014IEEEInt.Conf.Acoust.SpeechSignalProcess.ICASSP,
  title = {I-Vector Based Language Modeling for Spoken Document Retrieval},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Kuan-Yu and Lee, Hung-Shin and Wang, Hsin-Min and Chen, Berlin and Chen, Hsin-Hsi},
  year = {2014},
  month = may,
  pages = {7083--7088},
  publisher = {IEEE},
  address = {Florence, Italy},
  doi = {10.1109/ICASSP.2014.6854974},
  urldate = {2024-09-29},
  abstract = {Since more and more multimedia data associated with spoken documents have been made available to the public, spoken document retrieval (SDR) has become an important research subject in the past two decades. The i-vector based framework has been proposed and introduced to language identification (LID) and speaker recognition (SR) tasks recently. The major contribution of the i-vector framework is to reduce a series of acoustic feature vectors of a speech utterance to a low-dimensional vector representation, and then numbers of well-developed postprocessing techniques (such as probabilistic linear discriminative analysis, PLDA) can be readily and effectively used. However, to our best knowledge, there is no research up to date on applying the i-vector framework for SDR or information retrieval (IR). In this paper, we make a step forward to formulate an i-vector based language modeling (IVLM) framework for SDR. Furthermore, we evaluate the proposed IVLM framework with both inductive and transductive learning strategies. We also exploit multi-levels of index features, including word- and subword-level units, in concert with the proposed framework. The results of SDR experiments conducted on the TDT-2 (Topic Detection and Tracking) collection demonstrate the performance merits of our proposed framework when compared to several existing approaches.},
  isbn = {978-1-4799-2893-4},
  langid = {english},
  keywords = {Jab/ICASSP,Unread},
  annotation = {16 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The results of SDR experiments conducted on the TDT-2 (Topic Detection and Tracking) collection demonstrate the performance merits of the i-vector based language modeling (IVLM) framework when compared to several existing approaches.},
  file = {/Users/janwardenga/Zotero/storage/3IZBBA48/ICASSP_2014_I-vector based language modeling for spoken document retrieval.pdf}
}

@misc{chenLifelongKnowledgeEditing2024,
  title = {Lifelong {{Knowledge Editing}} for {{LLMs}} with {{Retrieval-Augmented Continuous Prompt Learning}}},
  author = {Chen, Qizhou and Zhang, Taolin and He, Xiaofeng and Li, Dongyang and Wang, Chengyu and Huang, Longtao and Xue, Hui},
  year = {2024},
  month = may,
  number = {arXiv:2405.03279},
  eprint = {2405.03279},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.03279},
  urldate = {2024-09-29},
  abstract = {Model editing aims to correct outdated or erroneous knowledge in large language models (LLMs) without the need for costly retraining. Lifelong model editing is the most challenging task that caters to the continuous editing requirements of LLMs. Prior works primarily focus on single or batch editing; nevertheless, these methods fall short in lifelong editing scenarios due to catastrophic knowledge forgetting and the degradation of model performance. Although retrieval-based methods alleviate these issues, they are impeded by slow and cumbersome processes of integrating the retrieved knowledge into the model. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous Prompt lEarning method, to boost editing efficacy and inference efficiency in lifelong learning. RECIPE first converts knowledge statements into short and informative continuous prompts, prefixed to the LLM's input query embedding, to efficiently refine the response grounded on the knowledge. It further integrates the Knowledge Sentinel (KS) that acts as an intermediary to calculate a dynamic threshold, determining whether the retrieval repository contains relevant knowledge. Our retriever and prompt encoder are jointly trained to achieve editing properties, i.e., reliability, generality, and locality. In our experiments, RECIPE is assessed extensively across multiple LLMs and editing datasets, where it achieves superior editing performance. RECIPE also demonstrates its capability to maintain the overall performance of LLMs alongside showcasing fast editing and inference speed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/22AB87QQ/Pre_2024_Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning.pdf}
}

@misc{chirkovaRetrievalaugmentedGenerationMultilingual2024,
  title = {Retrieval-Augmented Generation in Multilingual Settings},
  author = {Chirkova, Nadezhda and Rau, David and D{\'e}jean, Herv{\'e} and Formal, Thibault and Clinchant, St{\'e}phane and Nikoulina, Vassilina},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01463},
  eprint = {2407.01463},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.01463},
  urldate = {2024-09-29},
  abstract = {Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/QK6A6JNJ/Pre_2024_Retrieval-augmented generation in multilingual settings.pdf}
}

@inproceedings{chongImprovingRDFQuery20192019IEEE35thInt.Conf.DataEng.ICDE,
  title = {Improving {{RDF Query Performance Using In-memory Virtual Columns}} in {{Oracle Database}}},
  booktitle = {2019 {{IEEE}} 35th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Chong, Eugene Inseok and Perry, Matthew and Das, Souripriya},
  year = {2019},
  month = apr,
  pages = {1814--1819},
  publisher = {IEEE},
  address = {Macao, Macao},
  doi = {10.1109/ICDE.2019.00197},
  urldate = {2024-09-29},
  abstract = {Many RDF Knowledge Graph Stores use IDs to represent triples to save storage and for ease of maintenance. Oracle is no exception. While this design is good for a small footprint on disk, it incurs overhead in query processing, as it requires joins with the value table to return results or process aggregates/filter/order-by queries. It becomes especially problematic as the result size increases or the number of projected variables increases. Depending on queries, the value table join could take up most of the query processing time. In this paper, we propose to use in-memory virtual columns to avoid value table joins. It has advantages in that it does not increase the footprint on disk, and at the same time it avoids the value table joins by utilizing in-memory virtual columns. The idea is to materialize the values only in memory and utilize the compression and vector processing that come with Oracle Database In-Memory technology. Typically, the value table in RDF is small compared to the triples table. Therefore, its footprint is manageable in memory, especially with compression in columnar format. The same mechanism can be applied to any application where there exists a one-to-one mapping between an ID and its value, such as data warehousing or data marts. The mechanism has been implemented in Oracle 18c. Experimental results using the LUBM1000 benchmark show up to two orders of magnitude query performance improvement.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-5386-7474-1},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/89K3SQ4T/ICDE_2019_Improving RDF Query Performance Using In-memory Virtual Columns in Oracle Database.pdf}
}

@incollection{cimminoAstreaAutomaticGeneration2020TheSemanticWeb,
  title = {Astrea: {{Automatic Generation}} of {{SHACL Shapes}} from {{Ontologies}}},
  shorttitle = {Astrea},
  booktitle = {The {{Semantic Web}}},
  author = {Cimmino, Andrea and {Fern{\'a}ndez-Izquierdo}, Alba and {Garc{\'i}a-Castro}, Ra{\'u}l},
  editor = {Harth, Andreas and Kirrane, Sabrina and Ngonga Ngomo, Axel-Cyrille and Paulheim, Heiko and Rula, Anisa and Gentile, Anna Lisa and Haase, Peter and Cochez, Michael},
  year = {2020},
  volume = {12123},
  pages = {497--513},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-49461-2_29},
  urldate = {2025-02-02},
  abstract = {Knowledge Graphs (KGs) that publish RDF data modelled using ontologies in a wide range of domains have populated the Web. The SHACL language is a W3C recommendation that has been endowed to encode a set of either value or model data restrictions that aim at validating KG data, ensuring data quality. Developing shapes is a complex and time consuming task that is not feasible to achieve manually. This article presents two resources that aim at generating automatically SHACL shapes for a set of ontologies: (1) Astrea-KG, a KG that publishes a set of mappings that encode the equivalent conceptual restrictions among ontology constraint patterns and SHACL constraint patterns, and (2) Astrea, a tool that automatically generates SHACL shapes from a set of ontologies by executing the mappings from the Astrea-KG. These two resources are openly available at Zenodo, GitHub, and a web application. In contrast to other proposals, these resources cover a large number of SHACL restrictions producing both value and model data restrictions, whereas other proposals consider only a limited number of restrictions or focus only on value or model restrictions.},
  isbn = {978-3-030-49460-5 978-3-030-49461-2},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/6HV3S838/Cimmino et al. - 2020 - Astrea Automatic Generation of SHACL Shapes from Ontologies.pdf}
}

@inproceedings{cochezBiasedGraphWalks2017Proc.7thInt.Conf.WebIntell.Min.Semant.,
  title = {Biased Graph Walks for {{RDF}} Graph Embeddings},
  booktitle = {Proceedings of the 7th {{International Conference}} on {{Web Intelligence}}, {{Mining}} and {{Semantics}}},
  author = {Cochez, Michael and Ristoski, Petar and Ponzetto, Simone Paolo and Paulheim, Heiko},
  year = {2017},
  month = jun,
  pages = {1--12},
  publisher = {ACM},
  address = {Amantea Italy},
  doi = {10.1145/3102254.3102279},
  urldate = {2024-09-29},
  abstract = {Knowledge Graphs have been recognized as a valuable source for background information in many data mining, information retrieval, natural language processing, and knowledge extraction tasks. However, obtaining a suitable feature vector representation from RDF graphs is a challenging task. In this paper, we extend the RDF2Vec approach, which leverages language modeling techniques for unsupervised feature extraction from sequences of entities. We generate sequences by exploiting local information from graph substructures, harvested by graph walks, and learn latent numerical representations of entities in RDF graphs. We extend the way we compute feature vector representations by comparing twelve different edge weighting functions for performing biased walks on the RDF graph, in order to generate higher quality graph embeddings. We evaluate our approach using different machine learning, as well as entity and document modeling benchmark data sets, and show that the naive RDF2Vec approach can be improved by exploiting Biased Graph Walks.},
  isbn = {978-1-4503-5225-3},
  langid = {english},
  keywords = {Unread},
  annotation = {61 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The RDF2Vec approach is extended, which leverages language modeling techniques for unsupervised feature extraction from sequences of entities, and sequences are generated by exploiting local information from graph substructures, harvested by graph walks, and learn latent numerical representations of entities in RDF graphs.},
  file = {/Users/janwardenga/Zotero/storage/7ZAWJ6XX/WICWIMS_2017_Biased graph walks for RDF graph embeddings.pdf}
}

@incollection{cochezGlobalRDFVector2017TheSemanticWeb-ISWC2017,
  title = {Global {{RDF Vector Space Embeddings}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2017},
  author = {Cochez, Michael and Ristoski, Petar and Ponzetto, Simone Paolo and Paulheim, Heiko},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  volume = {10587},
  pages = {190--207},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-68288-4_12},
  urldate = {2024-09-29},
  abstract = {Vector space embeddings have been shown to perform well when using RDF data in data mining and machine learning tasks. Existing approaches, such as RDF2Vec, use local information, i.e., they rely on local sequences generated for nodes in the RDF graph. For word embeddings, global techniques, such as GloVe, have been proposed as an alternative. In this paper, we show how the idea of global embeddings can be transferred to RDF embeddings, and show that the results are competitive with traditional local techniques like RDF2Vec.},
  isbn = {978-3-319-68287-7 978-3-319-68288-4},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/5DKUQGYR/Book_2017_Global RDF Vector Space Embeddings.pdf}
}

@article{cohenEvaluationSPARQLQuery2013ProcConfAssocComputLinguistMeet,
  title = {Evaluation of {{SPARQL}} Query Generation from Natural Language Questions},
  author = {Cohen, K. Bretonnel and Kim, Jin-Dong},
  year = {2013},
  month = sep,
  journal = {Proceedings of the Conference. Association for Computational Linguistics. Meeting},
  volume = {2013},
  pages = {3--7},
  issn = {0736-587X},
  abstract = {SPARQL queries have become the standard for querying linked open data knowledge bases, but SPARQL query construction can be challenging and time-consuming even for experts. SPARQL query generation from natural language questions is an attractive modality for interfacing with LOD. However, how to evaluate SPARQL query generation from natural language questions is a mostly open research question. This paper presents some issues that arise in SPARQL query generation from natural language, a test suite for evaluating performance with respect to these issues, and a case study in evaluating a system for SPARQL query generation from natural language questions.},
  langid = {english},
  pmcid = {PMC7581285},
  pmid = {33100503},
  keywords = {Unread}
}

@misc{deepseek-aiDeepSeekV3TechnicalReport2024,
  title = {{{DeepSeek-V3 Technical Report}}},
  author = {{DeepSeek-AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
  year = {2024},
  month = dec,
  number = {arXiv:2412.19437},
  eprint = {2412.19437},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.19437},
  urldate = {2025-02-05},
  abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Read},
  annotation = {31 citations (Semantic Scholar/arXiv) [2025-02-15]\\
17 citations (Semantic Scholar/DOI) [2025-02-05]\\
TLDR: Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.},
  file = {/Users/janwardenga/Zotero/storage/Y4ZLTGCD/DeepSeek-AI et al. - 2024 - DeepSeek-V3 Technical Report.pdf}
}

@misc{dialloComprehensiveEvaluationNeural2024,
  title = {A {{Comprehensive Evaluation}} of {{Neural SPARQL Query Generation}} from {{Natural Language Questions}}},
  author = {Diallo, Papa Abdou Karim Karou and Reyd, Samuel and Zouaq, Amal},
  year = {2024},
  month = jan,
  number = {arXiv:2304.07772},
  eprint = {2304.07772},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.07772},
  urldate = {2025-01-26},
  abstract = {In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed significant growth. Incorporating the copy mechanism with traditional encoder-decoder architectures and using pre-trained encoderdecoders and large language models have set new performance benchmarks. This paper presents various experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained language models (PLMs), non-pretrained language models (NPLMs), and large language models (LLMs), highlighting the impact of question annotation and the copy mechanism and testing various fine-tuning methods using LLMs. In particular, we provide a systematic error analysis of the models and test their generalization ability. Our study demonstrates that the copy mechanism yields significant performance enhancements for most PLMs and NPLMs. Annotating the data is pivotal to generating correct URIs, with the "tag-within" strategy emerging as the most effective approach. Additionally, our findings reveal that the primary source of errors stems from incorrect URIs in SPARQL queries that are sometimes replaced with hallucinated URIs when using base models. This does not happen using the copy mechanism, but it sometimes leads to selecting wrong URIs among candidates. Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,Computer Science - Machine Learning,Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/JG9I4CBT/Diallo et al. - 2024 - A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions.pdf}
}

@misc{diomediQuestionAnsweringKnowledge2021,
  title = {Question {{Answering}} over {{Knowledge Graphs}} with {{Neural Machine Translation}} and {{Entity Linking}}},
  author = {Diomedi, Daniel and Hogan, Aidan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.02865},
  eprint = {2107.02865},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.02865},
  urldate = {2025-01-26},
  abstract = {The goal of Question Answering over Knowledge Graphs (KGQA) is to find answers for natural language questions over a knowledge graph. Recent KGQA approaches adopt a neural machine translation (NMT) approach, where the natural language question is translated into a structured query language. However, NMT suffers from the out-of-vocabulary problem, where terms in a question may not have been seen during training, impeding their translation. This issue is particularly problematic for the millions of entities that large knowledge graphs describe. We rather propose a KGQA approach that delegates the processing of entities to entity linking (EL) systems. NMT is then used to create a query template with placeholders that are filled by entities identified in an EL phase. Slot filling is used to decide which entity fills which placeholder. Experiments for QA over Wikidata show that our approach outperforms pure NMT: while there remains a strong dependence on having seen similar query templates during training, errors relating to entities are greatly reduced.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Unread},
  annotation = {84 citations (Semantic Scholar/arXiv) [2025-02-15]\\
13 citations (Semantic Scholar/arXiv) [2025-01-26]},
  file = {/Users/janwardenga/Zotero/storage/JVIBFWUY/Diomedi and Hogan - 2021 - Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking.pdf}
}

@article{dividinoQueryingProvenanceTrust2009JournalofWebSemantics,
  title = {Querying for Provenance, Trust, Uncertainty and Other Meta Knowledge in {{RDF}}},
  author = {Dividino, Renata and Sizov, Sergej and Staab, Steffen and Schueler, Bernhard},
  year = {2009},
  month = sep,
  journal = {Journal of Web Semantics},
  volume = {7},
  number = {3},
  pages = {204--219},
  issn = {15708268},
  doi = {10.1016/j.websem.2009.07.004},
  urldate = {2024-09-29},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Read},
  annotation = {41 citations (Semantic Scholar/DOI) [2025-02-15]\\
290 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper presents an original, generic, formalized and implemented approach for managing many dimensions of meta knowledge, like source, authorship, certainty and others, and extends SPARQL query processing in such a way that given a SParQL query for data, one may request meta knowledge without modifying the query proper.},
  file = {/Users/janwardenga/Zotero/storage/FQG9ESWD/Dividino et al. - 2009 - Querying for provenance, trust, uncertainty and other meta knowledge in RDF.pdf;/Users/janwardenga/Zotero/storage/G6JK27W8/Pre__Querying for Provenance, Trust, Uncertainty and other Meta Knowledge in RDF.pdf}
}

@misc{dongSurveyIncontextLearning2024,
  title = {A {{Survey}} on {{In-context Learning}}},
  author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Ma, Jingyuan and Li, Rui and Xia, Heming and Xu, Jingjing and Wu, Zhiyong and Liu, Tianyu and Chang, Baobao and Sun, Xu and Li, Lei and Sui, Zhifang},
  year = {2024},
  month = oct,
  number = {arXiv:2301.00234},
  eprint = {2301.00234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.00234},
  urldate = {2025-03-03},
  abstract = {With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Unread},
  annotation = {TLDR: The progress, challenges, and future work in ICL are summarized and a formal definition of ICL is presented and its correlation to related studies are clarified and potential directions for further research are provided.},
  file = {/Users/janwardenga/Zotero/storage/4YPNJ765/Dong et al. - 2024 - A Survey on In-context Learning.pdf;/Users/janwardenga/Zotero/storage/NVGKDLGI/2301.html}
}

@incollection{dubeyLCQuAD20Large2019TheSemanticWeb-ISWC2019,
  title = {{{LC-QuAD}} 2.0: {{A Large Dataset}} for {{Complex Question Answering}} over {{Wikidata}} and {{DBpedia}}},
  shorttitle = {{{LC-QuAD}} 2.0},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2019},
  author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
  editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Sv{\'a}tek, Vojt{\v e}ch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefran{\c c}ois, Maxime and Gandon, Fabien},
  year = {2019},
  volume = {11779},
  pages = {69--78},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-30796-7_5},
  urldate = {2025-02-27},
  abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like WebQuestions, QALD and LCQuAD have been published in the past. The biggest data set available for complex questions (LCQuAD) over knowledge graphs contains five thousand questions. We now provide LC-QuAD 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding SPARQL queries. LC-QuAD 2.0 is compatible with both Wikidata and DBpedia 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
  isbn = {978-3-030-30795-0 978-3-030-30796-7},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/RDQB8LXX/Dubey et al. - 2019 - LC-QuAD 2.0 A Large Dataset for Complex Question Answering over Wikidata and DBpedia.pdf}
}

@article{echihabiNewTrendsHighD2021Proc.VLDBEndow.,
  title = {New Trends in High-{{D}} Vector Similarity Search: Al-Driven, Progressive, and Distributed},
  shorttitle = {New Trends in High-{{D}} Vector Similarity Search},
  author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis},
  year = {2021},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {14},
  number = {12},
  pages = {3198--3201},
  issn = {2150-8097},
  doi = {10.14778/3476311.3476407},
  urldate = {2024-09-29},
  abstract = {Similarity search is a core operation of many critical applications, involving massive collections of high-dimensional (high-d) objects. Objects can be data series, text, multimedia, graphs, database tables or deep network embeddings. In this tutorial, we revisit the similarity search problem in light of the recent advances in the field and the new big data landscape. We discuss key data science applications that require efficient high-d similarity search, we survey recent approaches and share surprising insights about their strengths and weaknesses, and we discuss open research problems, including the directions of AI-driven, progressive, and distributed high-d similarity search.},
  langid = {english},
  keywords = {Unread},
  annotation = {27 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6JMTXS9N/PVE_2021_New trends in high-D vector similarity search.pdf}
}

@misc{edgeLocalGlobalGraph2024,
  title = {From {{Local}} to {{Global}}: {{A Graph RAG Approach}} to {{Query-Focused Summarization}}},
  shorttitle = {From {{Local}} to {{Global}}},
  author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  year = {2024},
  month = apr,
  number = {arXiv:2404.16130},
  eprint = {2404.16130},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.16130},
  urldate = {2024-10-30},
  abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as ``What are the main themes in the dataset?'', since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na{\textasciidieresis}{\i}ve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Jab/Pre,Read},
  annotation = {155 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/7IZZ8NG2/Pre_2024_From Local to Global.pdf}
}

@misc{edwardsHybridContextRetrieval2024,
  title = {Hybrid {{Context Retrieval Augmented Generation Pipeline}}: {{LLM-Augmented Knowledge Graphs}} and {{Vector Database}} for {{Accreditation Reporting Assistance}}},
  shorttitle = {Hybrid {{Context Retrieval Augmented Generation Pipeline}}},
  author = {Edwards, Candace},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2405.15436},
  urldate = {2024-09-29},
  abstract = {In higher education, accreditation is a quality assurance process, where an institution demonstrates a commitment to delivering high quality programs and services to their students. For business schools nationally and internationally the Association to Advance Collegiate Schools of Business (AACSB) accreditation is the gold standard. For a business school to receive and subsequently maintain accreditation, the school must undertake a rigorous, time consuming reporting and peer review process, to demonstrate alignment with the AACSB Standards. For this project we create a hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation. We implement both a vector database and knowledge graph, as knowledge stores containing both institutional data and AACSB Standard data. The output of the pipeline can be used by institution stakeholders to build their accreditation report, dually grounded by the context from the knowledge stores. To develop our knowledge graphs we utilized both a manual construction process as well as an LLM Augmented Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework and observed optimal performance on answer relevancy and answer correctness metrics.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Information Retrieval (cs.IR),Read},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation, and can be used by institution stakeholders to build their accreditation report.},
  file = {/Users/janwardenga/Zotero/storage/J2RB34ZB/Pre__Hybrid Context Retrieval Augmented Generation Pipeline LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance.pdf}
}

@article{elbassuoni2010searching,
  title = {Searching {{RDF}} Graphs with {{SPARQL}} and Keywords.},
  author = {Elbassuoni, Shady and Ramanath, Maya and Schenkel, Ralf and Weikum, Gerhard},
  year = {2010},
  journal = {IEEE Data Eng. Bull.},
  volume = {33},
  number = {1},
  pages = {16--24},
  publisher = {Citeseer},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/Q58VLREQ/Pre__Searching RDF Graphs with SPARQL and Keywords.pdf}
}

@inproceedings{elbassuoniKeywordSearchRDF2011Proc.20thACMInt.Conf.Inf.Knowl.Manag.,
  title = {Keyword Search over {{RDF}} Graphs},
  booktitle = {Proceedings of the 20th {{ACM}} International Conference on {{Information}} and Knowledge Management},
  author = {Elbassuoni, Shady and Blanco, Roi},
  year = {2011},
  month = oct,
  pages = {237--242},
  publisher = {ACM},
  address = {Glasgow Scotland, UK},
  doi = {10.1145/2063576.2063615},
  urldate = {2024-09-29},
  abstract = {Large knowledge bases consisting of entities and relationships between them have become vital sources of information for many applications. Most of these knowledge bases adopt the SemanticWeb data model RDF as a representation model. Querying these knowledge bases is typically done using structured queries utilizing graph-pattern languages such as SPARQL. However, such structured queries require some expertise from users which limits the accessibility to such data sources. To overcome this, keyword search must be supported. In this paper, we propose a retrieval model for keyword queries over RDF graphs. Our model retrieves a set of subgraphs that match the query keywords, and ranks them based on statistical language models. We show that our retrieval model outperforms the-state-of-the-art IR and DB models for keyword search over structured data using experiments over two real-world datasets.},
  isbn = {978-1-4503-0717-8},
  langid = {english},
  keywords = {Jab/CICIKM,Unread},
  annotation = {126 citations (Semantic Scholar/DOI) [2025-02-20]\\
126 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This model retrieves a set of subgraphs that match the query keywords, and ranks them based on statistical language models, and shows that it outperforms the-state-of-the-art IR and DB models for keyword search over structured data.},
  file = {/Users/janwardenga/Zotero/storage/BMX7IUHJ/CICIKM_2011_Keyword search over RDF graphs.pdf}
}

@misc{emonetLLMbasedSPARQLQuery2024,
  title = {{{LLM-based SPARQL Query Generation}} from {{Natural Language}} over {{Federated Knowledge Graphs}}},
  author = {Emonet, Vincent and Bolleman, Jerven and Duvaud, Severine and de Farias, Tarcisio Mendes and Sima, Ana Claudia},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06062},
  eprint = {2410.06062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06062},
  urldate = {2025-01-26},
  abstract = {We introduce a Retrieval-Augmented Generation (RAG) system for translating user questions into accurate federated SPARQL queries over bioinformatics knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance accuracy and reduce hallucinations in query generation, our system utilises metadata from the KGs, including query examples and schema information, and incorporates a validation step to correct generated queries. The system is available online at chat.expasy.org.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Information Retrieval,Unread},
  annotation = {37 citations (Semantic Scholar/DOI) [2025-02-15]\\
0 citations (Semantic Scholar/DOI) [2025-01-26]},
  file = {/Users/janwardenga/Zotero/storage/KAFD8LSR/Emonet et al. - 2024 - LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs.pdf}
}

@misc{esRAGASAutomatedEvaluation2023,
  title = {{{RAGAS}}: {{Automated Evaluation}} of {{Retrieval Augmented Generation}}},
  shorttitle = {{{RAGAS}}},
  author = {Es, Shahul and James, Jithin and {Espinosa-Anke}, Luis and Schockaert, Steven},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.15217},
  urldate = {2024-09-29},
  abstract = {We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With RAGAs, we put forward a suite of metrics which can be used to evaluate these different dimensions {\textbackslash}textit\{without having to rely on ground truth human annotations\}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),Read},
  annotation = {121 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work introduces RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation pipelines, and posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.},
  file = {/Users/janwardenga/Zotero/storage/G4UG4TCA/Pre__RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf}
}

@inproceedings{fangREANOOptimisingRetrievalAugmented2024Proc.62ndAnnu.Meet.Assoc.Comput.Linguist.Vol.1LongPap.,
  title = {{{REANO}}: {{Optimising Retrieval-Augmented Reader Models}} through {{Knowledge Graph Generation}}},
  shorttitle = {{{REANO}}},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Fang, Jinyuan and Meng, Zaiqiao and MacDonald, Craig},
  year = {2024},
  pages = {2094--2112},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  doi = {10.18653/v1/2024.acl-long.115},
  urldate = {2024-09-30},
  abstract = {Open domain question answering (ODQA) aims to answer questions with knowledge from an external corpus. Fusion-in-Decoder (FiD) is an effective retrieval-augmented reader model to address this task. Given that FiD independently encodes passages, which overlooks the semantic relationships between passages, some studies use knowledge graphs (KGs) to establish dependencies among passages. However, they only leverage knowledge triples from existing KGs, which suffer from incompleteness and may lack certain information critical for answering given questions. To this end, in order to capture the dependencies between passages while tacking the issue of incompleteness in existing KGs, we propose to enhance the retrievalaugmented reader model with a knowledge graph generation module (REANO). Specifically, REANO consists of a KG generator and an answer predictor. The KG generator aims to generate KGs from the passages; the answer predictor then generates answers based on the passages and the generated KGs. Experimental results on five ODQA datasets indicate that compared with baselines, REANO1 can improve the exact match score by up to 2.7\% on the EntityQuestion dataset, with an average improvement of 1.8\% across all the datasets.},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/C6MZTV48/Fang et al. - 2024 - REANO Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation.pdf}
}

@misc{fangTRACEEvidenceConstructing2024,
  title = {{{TRACE}} the {{Evidence}}: {{Constructing Knowledge-Grounded Reasoning Chains}} for {{Retrieval-Augmented Generation}}},
  shorttitle = {{{TRACE}} the {{Evidence}}},
  author = {Fang, Jinyuan and Meng, Zaiqiao and Macdonald, Craig},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11460},
  eprint = {2406.11460},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.11460},
  urldate = {2024-09-30},
  abstract = {Retrieval-augmented generation (RAG) offers an effective approach for addressing question answering (QA) tasks. However, the imperfections of the retrievers in RAG models often result in the retrieval of irrelevant information, which could introduce noises and degrade the performance, especially when handling multihop questions that require multiple steps of reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose TRACE1. TRACE constructs knowledge-grounded reasoning chains, which are a series of logically connected knowledge triples, to identify and integrate supporting evidence from the retrieved documents for answering questions. Specifically, TRACE employs a KG Generator to create a knowledge graph (KG) from the retrieved documents, and then uses an Autoregressive Reasoning Chain Constructor to build reasoning chains. Experimental results on three multihop QA datasets show that TRACE achieves an average performance improvement of up to 14.03\% compared to using all the retrieved documents. Moreover, the results indicate that using reasoning chains as context, rather than the entire documents, is often sufficient to correctly answer questions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/AN5K9NS2/Fang et al. - 2024 - TRACE the Evidence Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generat.pdf}
}

@misc{fanSurveyRAGMeeting2024,
  title = {A {{Survey}} on {{RAG Meeting LLMs}}: {{Towards Retrieval-Augmented Large Language Models}}},
  shorttitle = {A {{Survey}} on {{RAG Meeting LLMs}}},
  author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  year = {2024},
  month = jun,
  number = {arXiv:2405.06211},
  eprint = {2405.06211},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.06211},
  urldate = {2024-09-29},
  abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-ofdate internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, RetrievalAugmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https:// advanced-recommendersystems.github.io/ RAG-Meets-LLMs/ 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {78 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/7KGNZTGF/Pre_2024_A Survey on RAG Meeting LLMs.pdf}
}

@misc{farberLinkedPapersCode2023,
  title = {Linked {{Papers With Code}}: {{The Latest}} in {{Machine Learning}} as an {{RDF Knowledge Graph}}},
  shorttitle = {Linked {{Papers With Code}}},
  author = {F{\"a}rber, Michael and Lamprecht, David},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20475},
  eprint = {2310.20475},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.20475},
  urldate = {2024-09-26},
  abstract = {In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC not only translates the latest advancements in machine learning into RDF format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. LPWC is openly accessible at https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple formats, from RDF dump files to a SPARQL endpoint for direct web queries, as well as a data source with resolvable URIs and links to the data sources SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph embeddings, enabling LPWC to be readily applied in machine learning applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]\\
38 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/DFDYKHDY/Pre_2023_Linked Papers With Code.pdf}
}

@article{fayeRDFTriplesManagement2011,
  title = {{{RDF}} Triples Management in {{roStore}}},
  author = {Faye, David and Cur{\'e}, Olivier and Blin, Guillaume and Thiam, Cheikh},
  year = {2011},
  langid = {english},
  keywords = {,No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/J465GCBZ/Pre_2011_RDF triples management in roStore.pdf}
}

@article{fayeSurveyRDFStorage2012Rev.Afr.Rech.EnInform.MathematiquesAppliquees,
  title = {A Survey of {{RDF}} Storage Approaches},
  author = {Faye, David C. and Cur{\'e}, Olivier and Blin, Guillaume},
  year = {2012},
  month = sep,
  journal = {Revue Africaine de Recherche en Informatique et Math{\'e}matiques Appliqu{\'e}es},
  volume = {Volume 15, 2012},
  pages = {1956},
  issn = {1638-5713},
  doi = {10.46298/arima.1956},
  urldate = {2024-09-29},
  abstract = {The Semantic Web extends the principles of the Web by allowing computers to understand and easily explore the Web. In recent years RDF has been a widespread data format for the Semantic Web. There is a real need to efficiently store and retrieve RDF data as the number and scale of Semantic Web in real-word applications in use increase. As datasets grow larger and more datasets are linked together, scalability becomes more important. Efficient data storage and query processing that can scale to large amounts of possibly schema-less data has become an important research topic. This paper gives an overview of the features of techniques for storing RDF data.},
  langid = {english},
  keywords = {Unread},
  annotation = {145 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: An overview of the features of techniques for storing RDF data is given for efficient data storage and query processing as the number and scale of Semantic Web in real-word applications in use increase and scalability becomes more important.},
  file = {/Users/janwardenga/Zotero/storage/EM7EY98I/RARIMA_2012_A survey of RDF storage approaches.pdf}
}

@article{feichterEffectVoluntaryManagerial2022SSRNJournal,
  title = {The {{Effect}} of {{Voluntary Managerial Pay Cuts}} on {{Employee Effort}}},
  author = {Feichter, Christoph and Wiernsperger, Martin},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4115631},
  urldate = {2024-12-05},
  langid = {english},
  keywords = {Unread},
  annotation = {100 citations (Semantic Scholar/DOI) [2025-02-15]\\
1 citations (Semantic Scholar/DOI) [2024-12-05]}
}

@misc{fengKnowledgeSolverTeaching2023,
  title = {Knowledge {{Solver}}: {{Teaching LLMs}} to {{Search}} for {{Domain Knowledge}} from {{Knowledge Graphs}}},
  shorttitle = {Knowledge {{Solver}}},
  author = {Feng, Chao and Zhang, Xinyu and Fei, Zichu},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03118},
  eprint = {2309.03118},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.03118},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, LLMs sometimes lack domainspecific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks (GNNs) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domainspecific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since LLMs' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence, which empowers LLMs with searching knowledge ability in zero-shot manner. Additionally, KSL is able to provide complete retrieval paths and therefore increase explainability of LLMs' reasoning processes. We conduct experiments on three datasets: CommonsenseQA (Talmor et al., 2018), OpenbookQA (Mihaylov et al., 2018), and MedQA-USMLE (Jin et al., 2021), and found that our approach improves LLM baseline performance by a relatively large margin.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {35 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/WKBDDD58/Pre_2023_Knowledge Solver.pdf}
}

@article{fernandez-alvarezAutomaticExtractionShapes2022Knowledge-BasedSystems,
  title = {Automatic Extraction of Shapes Using {{sheXer}}},
  author = {{Fernandez-{\'A}lvarez}, Daniel and {Labra-Gayo}, Jose Emilio and {Gayo-Avello}, Daniel},
  year = {2022},
  month = feb,
  journal = {Knowledge-Based Systems},
  volume = {238},
  pages = {107975},
  issn = {09507051},
  doi = {10.1016/j.knosys.2021.107975},
  urldate = {2025-02-02},
  langid = {english},
  keywords = {Jab/KS,Unread},
  annotation = {24 citations (Semantic Scholar/DOI) [2025-02-15]\\
24 citations (Semantic Scholar/DOI) [2025-02-02]},
  file = {/Users/janwardenga/Zotero/storage/ZLDD4N8C/KS_2022_Automatic extraction of shapes using sheXer.pdf}
}

@article{freitasDISTRIBUTIONALSTRUCTUREDSEMANTIC2011Int.J.SemanticComputing,
  title = {A {{DISTRIBUTIONAL STRUCTURED SEMANTIC SPACE FOR QUERYING RDF GRAPH DATA}}},
  author = {Freitas, Andr{\'e} and Curry, Edward and Oliveira, Jo{\~a}o Gabriel and O'Riain, Se{\'a}n},
  year = {2011},
  month = dec,
  journal = {International Journal of Semantic Computing},
  volume = {05},
  number = {04},
  pages = {433--462},
  issn = {1793-351X, 1793-7108},
  doi = {10.1142/S1793351X1100133X},
  urldate = {2024-09-29},
  abstract = {The vision of creating a Linked Data Web brings together the challenge of allowing queries across highly heterogeneous and distributed datasets. In order to query Linked Data on the Web today, end users need to be aware of which datasets potentially contain the data and also which data model describes these datasets. The process of allowing users to expressively query relationships in RDF while abstracting them from the underlying data model represents a fundamental problem for Web-scale Linked Data consumption. This article introduces a distributional structured semantic space which enables data model independent natural language queries over RDF data. The center of the approach relies on the use of a distributional semantic model to address the level of semantic interpretation demanded to build the data model independent approach. The article analyzes the geometric aspects of the proposed space, providing its description as a distributional structured vector space, which is built upon the Generalized Vector Space Model (GVSM). The final semantic space proved to be flexible and precise under real-world query conditions achieving mean reciprocal rank = 0.516, avg. precision = 0.482 and avg. recall = 0.491.},
  langid = {english},
  keywords = {Unread},
  annotation = {20 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A distributional structured semantic space which enables data model independent natural language queries over RDF data and is built upon the Generalized Vector Space Model (GVSM).},
  file = {/Users/janwardenga/Zotero/storage/U4JVEABH/IJSC_2011_A DISTRIBUTIONAL STRUCTURED SEMANTIC SPACE FOR QUERYING RDF GRAPH DATA.pdf}
}

@misc{freyBenchmarkingAbilitiesLarge2023,
  title = {Benchmarking the {{Abilities}} of {{Large Language Models}} for {{RDF Knowledge Graph Creation}} and {{Comprehension}}: {{How Well Do LLMs Speak Turtle}}?},
  shorttitle = {Benchmarking the {{Abilities}} of {{Large Language Models}} for {{RDF Knowledge Graph Creation}} and {{Comprehension}}},
  author = {Frey, Johannes and Meyer, Lars-Peter and Arndt, Natanael and Brei, Felix and Bulert, Kirill},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17122},
  eprint = {2309.17122},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.17122},
  urldate = {2024-09-26},
  abstract = {Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within RDF knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {9 citations (Semantic Scholar/arXiv) [2025-02-15]\\
1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/BDYSK873/Pre_2023_Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension.pdf}
}

@article{fusslKnowledgeGraphBasedExplainable2023Int.J.SemanticComputing,
  title = {Knowledge {{Graph-Based Explainable Artificial Intelligence}} for {{Business Process Analysis}}},
  author = {F{\"u}{\ss}l, Anne and Nissen, Volker and Heringklee, Stefan Horst},
  year = {2023},
  month = jun,
  journal = {International Journal of Semantic Computing},
  volume = {17},
  number = {02},
  pages = {173--197},
  issn = {1793-351X, 1793-7108},
  doi = {10.1142/S1793351X23600024},
  urldate = {2024-09-29},
  abstract = {For critical operational decisions (e.g. consulting services), explanations and interpretable results of powerful Artificial Intelligence (AI) systems are becoming increasingly important. Knowledge graphs possess a semantic model that integrates heterogeneous information sources and represents knowledge elements in a machine-readable form. The integration of knowledge graphs and machine learning methods represents a new form of hybrid intelligent systems that benefit from each other's strengths. Our research aims at an explainable system with a specific knowledge graph architecture that generates human-understandable results even when no suitable domain experts are available. Against this background, the interpretability of a knowledge graph-based explainable AI approach for business process analysis is focused. We design a framework of interpretation, show how interpretable models are generated by a single case study and evaluate the applicability of our approach by different expert interviews. Result paths on weaknesses and improvement measures related to a business process are used to produce stochastic decision trees, which improve the interpretability of results. This can lead to interesting consulting self-services for clients or be applied as a device for accelerating classical consulting projects.},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/V7J3KXDF/IJSC_2023_Knowledge Graph-Based Explainable Artificial Intelligence for Business Process Analysis.pdf}
}

@article{futiaIntegrationKnowledgeGraphs2020Information,
  title = {On the {{Integration}} of {{Knowledge Graphs}} into {{Deep Learning Models}} for a {{More Comprehensible AI}}---{{Three Challenges}} for {{Future Research}}},
  author = {Futia, Giuseppe and Vetr{\`o}, Antonio},
  year = {2020},
  month = feb,
  journal = {Information},
  volume = {11},
  number = {2},
  pages = {122},
  issn = {2078-2489},
  doi = {10.3390/info11020122},
  urldate = {2024-09-29},
  abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI---while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research---knowledge matching, cross-disciplinary explanations and interactive explanations.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {76 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/GBB9EADL/I_2020_On the Integration of Knowledge Graphs into Deep Learning Models for a More Comprehensible AI—Three Challenges for Future Research.pdf}
}

@article{galComputingContinuousSky2009TheorApplClimatol,
  title = {Computing Continuous Sky View Factors Using {{3D}} Urban Raster and Vector Databases: Comparison and Application to Urban Climate},
  shorttitle = {Computing Continuous Sky View Factors Using {{3D}} Urban Raster and Vector Databases},
  author = {G{\'a}l, T. and Lindberg, F. and Unger, J.},
  year = {2009},
  month = jan,
  journal = {Theoretical and Applied Climatology},
  volume = {95},
  number = {1-2},
  pages = {111--123},
  issn = {0177-798X, 1434-4483},
  doi = {10.1007/s00704-007-0362-9},
  urldate = {2024-09-29},
  abstract = {The use of high resolution 3D urban raster and vector databases in urban climatology is presented. It applies two different methods to the calculation of continuous sky view factors (SVF), compares their values and considers their usefulness and limitations in urban climate studies. It shows and evaluates the relationship between urban geometry, quantified by SVF, and intra-urban nocturnal temperature variations using areal means in the whole urban area of Szeged, a city located in southeast Hungary. Results from the vector and raster models shows similar SVF values (r2 {$\frac{1}{4}$} 0.9827). The usefulness of using areal means in SVFtemperature relations is confirmed. The vector and the raster approaches to the derivation of areal means of SVF are both shown to be powerful tools to obtain a general picture of the geometrical conditions of an urban environment.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {Jab/TAC,Unread},
  annotation = {219 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/KBT6N55P/TAC_2009_Computing continuous sky view factors using 3D urban raster and vector databases.pdf}
}

@misc{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  year = {2024},
  month = mar,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.10997},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domainspecific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-theart technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Read},
  annotation = {948 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/DQPXQ2PR/Pre_2024_Retrieval-Augmented Generation for Large Language Models.pdf}
}

@misc{garbisGeographicaBenchmarkGeospatial2013,
  title = {Geographica: {{A Benchmark}} for {{Geospatial RDF Stores}}},
  shorttitle = {Geographica},
  author = {Garbis, George and Kyzirakos, Kostis and Koubarakis, Manolis},
  year = {2013},
  month = may,
  number = {arXiv:1305.5653},
  eprint = {1305.5653},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1305.5653},
  urldate = {2024-09-29},
  abstract = {Geospatial extensions of SPARQL like GeoSPARQL and stSPARQL have recently been defined and corresponding geospatial RDF stores have been implemented. However, there is no widely used benchmark for evaluating geospatial RDF stores which takes into account recent advances to the state of the art in this area. In this paper, we develop a benchmark, called Geographica, which uses both real-world and synthetic data to test the offered functionality and the performance of some prominent geospatial RDF stores.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {76 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/PJ995WME/Pre_2013_Geographica.pdf}
}

@misc{gaurSemanticsBlackBoxCan2020,
  title = {Semantics of the {{Black-Box}}: {{Can}} Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?},
  shorttitle = {Semantics of the {{Black-Box}}},
  author = {Gaur, Manas and Faldu, Keyur and Sheth, Amit},
  year = {2020},
  month = dec,
  number = {arXiv:2010.08660},
  eprint = {2010.08660},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2010.08660},
  urldate = {2024-09-29},
  abstract = {The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. The DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, signal processing, and human-computer interactions. However, the Black-Box nature of DL models and their over-reliance on massive amounts of data condensed into labels and dense representations poses challenges for interpretability and explainability of the system. Furthermore, DLs have not yet been proven in their ability to effectively utilize relevant domain knowledge and experience critical to human understanding. This aspect is missing in early data-focused approaches and necessitated knowledge-infused learning and other strategies to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL methods using knowledge-infused learning, which is one of the strategies. We then discuss how this makes a fundamental difference in the interpretability and explainability of current approaches, and illustrate it with examples from natural language processing for healthcare and education applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {104 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/493XBKBA/Pre_2020_Semantics of the Black-Box.pdf}
}

@article{gavilanesUseLLMMethods,
  title = {Use of {{LLM}} for {{Methods}} of {{Information Retrieval}}},
  author = {Gavilanes, Jose and Bozhilov, Yancho and Dodeja, Ujjwal and Valtas, Georgios and Badrajan, Alen},
  abstract = {This paper focuses on the use of Large Language Models (LLM) in searching, sorting and retrieving information from large chunks of data. In the proposed prototype of the paper, the chunks of data consist of documents such as PDFs and text files. The purpose of such prototype is to show its capabilities on large amounts of data and how it compares to the traditional way of searching which involves manually navigating through folders and files and using your own cognitive methods to summarise an answer to what you are looking for. Just as the traditional way of searching through files, it is a crucial requirement to maintain the same level of security of the data for the LLM method.},
  langid = {english},
  keywords = {No DOI found,Not citable but relevant,Read},
  file = {/Users/janwardenga/Zotero/storage/CS8YICMY/Pre__Use of LLM for Methods of Information Retrieval.pdf}
}

@article{ghajariQueryingDepthsUnveiling,
  title = {Querying the {{Depths}}: {{Unveiling}} the {{Strengths}} and {{Struggles}} of {{Large Language Models}} in {{SPARQL Generation}}},
  author = {Ghajari, Adrian and Ros, Salvador},
  abstract = {The emergence of the Semantic Web has precipitated a proliferation of structured data manifested in the form of knowledge graphs, underscoring the imperative of natural language interfaces to enhance accessibility to these repositories of information. The capacity to articulate queries in natural language and subsequently retrieve data through SPARQL queries assumes paramount importance. In the present investigation, we have scrutinized the efficacy of in-context learning based on an agent-based architecture in facilitating the construction of SPARQL queries. Contrary to initial expectations, the augmentation of in-context learning prompts through agent-based mechanisms has been found to diminish the efficacy of Language Model-based Systems (LLMS), as it is perceived as extraneous ''noise,'' thereby delineating the constraints inherent in this approach. The results highlight the need to delve deeper into the intricacies of model training and fine-tuning, focusing on the relational aspects of ontology schemas.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/KI25PMQS/Ghajari and Ros - Querying the Depths Unveiling the Strengths and Struggles of Large Language Models in SPARQL Genera.pdf}
}

@misc{glassRobustRetrievalAugmented2021,
  title = {Robust {{Retrieval Augmented Generation}} for {{Zero-shot Slot Filling}}},
  author = {Glass, Michael and Rossiello, Gaetano and Chowdhury, Md Faisal Mahbub and Gliozzo, Alfio},
  year = {2021},
  month = sep,
  number = {arXiv:2108.13934},
  eprint = {2108.13934},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2108.13934},
  urldate = {2024-09-29},
  abstract = {Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [ENTITY, SLOT, ?], a system is asked to `fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both TREx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Read},
  annotation = {33 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/AXPSLD2U/Pre_2021_Robust Retrieval Augmented Generation for Zero-shot Slot Filling.pdf}
}

@incollection{goebelExplainableAINew2018MachineLearningandKnowledgeExtraction,
  title = {Explainable {{AI}}: {{The New}} 42?},
  shorttitle = {Explainable {{AI}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  volume = {11015},
  pages = {295--303},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-99740-7_21},
  urldate = {2024-09-29},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce's abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.},
  isbn = {978-3-319-99739-1 978-3-319-99740-7},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/ANSEIT3F/Book_2018_Explainable AI.pdf}
}

@article{guedjIndustrializingAIpoweredDrug2022ExpertOpiniononDrugDiscovery,
  title = {Industrializing {{AI-powered}} Drug Discovery: Lessons Learned from the {{{\emph{Patrimony}}}} Computing Platform},
  shorttitle = {Industrializing {{AI-powered}} Drug Discovery},
  author = {Guedj, Micka{\"e}l and Swindle, Jack and Hamon, Antoine and Hubert, Sandra and Desvaux, Emiko and Laplume, Jessica and Xuereb, Laura and Lefebvre, C{\'e}line and Haudry, Yannick and Gabarroca, Christine and Aussy, Audrey and Laigle, Laurence and {Dupin-Roger}, Isabelle and Moingeon, Philippe},
  year = {2022},
  month = aug,
  journal = {Expert Opinion on Drug Discovery},
  volume = {17},
  number = {8},
  pages = {815--824},
  issn = {1746-0441, 1746-045X},
  doi = {10.1080/17460441.2022.2095368},
  urldate = {2024-09-29},
  abstract = {Introduction: As a mid-size international pharmaceutical company, we initiated 4 years ago the launch of a dedicated high-throughput computing platform supporting drug discovery. The platform named `Patrimony' was built up on the initial predicate to capitalize on our proprietary data while leveraging public data sources in order to foster a Computational Precision Medicine approach with the power of artificial intelligence.},
  langid = {english},
  keywords = {Jab/EODD,Unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/T5YDRM58/EODD_2022_Industrializing AI-powered drug discovery.pdf}
}

@misc{guoLightRAGSimpleFast2024,
  title = {{{LightRAG}}: {{Simple}} and {{Fast Retrieval-Augmented Generation}}},
  shorttitle = {{{LightRAG}}},
  author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
  year = {2024},
  month = oct,
  number = {arXiv:2410.05779},
  eprint = {2410.05779},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2410.05779},
  urldate = {2024-11-04},
  abstract = {Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/PQTA4E8H/Guo et al. - 2024 - LightRAG Simple and Fast Retrieval-Augmented Generation.pdf}
}

@misc{guoManuCloudNative2022,
  title = {Manu: {{A Cloud Native Vector Database Management System}}},
  shorttitle = {Manu},
  author = {Guo, Rentong and Luan, Xiaofan and Xiang, Long and Yan, Xiao and Yi, Xiaomeng and Luo, Jigao and Cheng, Qianya and Xu, Weizhi and Luo, Jiarui and Liu, Frank and Cao, Zhenshan and Qiao, Yanliang and Wang, Ting and Tang, Bo and Xie, Charles},
  year = {2022},
  month = jun,
  number = {arXiv:2206.13843},
  eprint = {2206.13843},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2206.13843},
  urldate = {2024-09-29},
  abstract = {With the development of learning-based embedding models, embedding vectors are widely used for analyzing and searching unstructured data. As vector collections exceed billion-scale, fully managed and horizontally scalable vector databases are necessary. In the past three years, through interaction with our 1200+ industry users, we have sketched a vision for the features that next-generation vector databases should have, which include long-term evolvability, tunable consistency, good elasticity, and high performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {38 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/U94X5FF3/Pre_2022_Manu.pdf}
}

@article{gurgurovMultilingualLargeLanguage2024,
  title = {Multilingual {{Large Language Models}} and {{Curse}} of {{Multilinguality}}},
  author = {Gurgurov, Daniil and B{\"a}umel, Tanja and Anikina, Tatiana},
  year = {2024},
  eprint = {2406.10602},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.10602},
  urldate = {2025-03-03},
  abstract = {Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-03-03]\\
TLDR: This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects, and explores the unique features of different model types: encoder-only, decoder-only, and encoder-decoder models.},
  file = {/Users/janwardenga/Zotero/storage/4ZR9VBJ4/Gurgurov et al. - 2024 - Multilingual Large Language Models and Curse of Multilinguality.pdf;/Users/janwardenga/Zotero/storage/BM232QDD/2406.html}
}

@misc{gutierrez-basultoKnowledgeGraphEmbedding2018,
  title = {From {{Knowledge Graph Embedding}} to {{Ontology Embedding}}? {{An Analysis}} of the {{Compatibility}} between {{Vector Space Representations}} and {{Rules}}},
  shorttitle = {From {{Knowledge Graph Embedding}} to {{Ontology Embedding}}?},
  author = {{Guti{\'e}rrez-Basulto}, V{\'i}ctor and Schockaert, Steven},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1805.10461},
  urldate = {2024-09-29},
  abstract = {Recent years have witnessed the successful application of low-dimensional vector space representations of knowledge graphs to predict missing facts or find erroneous ones. However, it is not yet well-understood to what extent ontological knowledge, e.g. given as a set of (existential) rules, can be embedded in a principled way. To address this shortcoming, in this paper we introduce a general framework based on a view of relations as regions, which allows us to study the compatibility between ontological knowledge and different types of vector space embeddings. Our technical contribution is two-fold. First, we show that some of the most popular existing embedding methods are not capable of modelling even very simple types of rules, which in particular also means that they are not able to learn the type of dependencies captured by such rules. Second, we study a model in which relations are modelled as convex regions. We show particular that ontologies which are expressed using so-called quasi-chained existential rules can be exactly represented using convex regions, such that any set of facts which is induced using that vector space embedding is logically consistent and deductively closed with respect to the input ontology.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Read},
  annotation = {66 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/YHGW9YP7/Pre__From Knowledge Graph Embedding to Ontology Embedding An Analysis of the Compatibility between Vector Space Representations and Rules.pdf}
}

@misc{guuTraversingKnowledgeGraphs2015,
  title = {Traversing {{Knowledge Graphs}} in {{Vector Space}}},
  author = {Guu, Kelvin and Miller, John and Liang, Percy},
  year = {2015},
  month = aug,
  number = {arXiv:1506.01094},
  eprint = {1506.01094},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1506.01094},
  urldate = {2024-09-29},
  abstract = {Path queries on a knowledge graph can be used to answer compositional questions such as ``What languages are spoken by people living in Lisbon?''. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new ``compositional'' training objective, which dramatically improves all models' ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that compositional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43\%) and achieving new state-of-the-art results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {351 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/RZL27ZKV/Pre_2015_Traversing Knowledge Graphs in Vector Space.pdf}
}

@misc{hanComprehensiveSurveyVector2023,
  title = {A {{Comprehensive Survey}} on {{Vector Database}}: {{Storage}} and {{Retrieval Technique}}, {{Challenge}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Vector Database}}},
  author = {Han, Yikun and Liu, Chunjiang and Wang, Pengfei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11703},
  eprint = {2310.11703},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.11703},
  urldate = {2024-09-29},
  abstract = {A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {40 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/UJPA53VD/Pre_2023_A Comprehensive Survey on Vector Database.pdf}
}

@misc{heGRetrieverRetrievalAugmentedGeneration2024,
  title = {G-{{Retriever}}: {{Retrieval-Augmented Generation}} for {{Textual Graph Understanding}} and {{Question Answering}}},
  shorttitle = {G-{{Retriever}}},
  author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan},
  year = {2024},
  month = may,
  number = {arXiv:2402.07630},
  eprint = {2402.07630},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.07630},
  urldate = {2024-09-29},
  abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.{\textasciitilde}{\textbackslash}footnote\{Our codes and datasets are available at: {\textbackslash}url\{https://github.com/XiaoxinHe/G-Retriever\}\}},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Read},
  annotation = {38 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/XRWHYBMX/Pre_2024_G-Retriever.pdf}
}

@incollection{heimRelFinderRevealingRelationships2009SemanticMultimedia,
  title = {{{RelFinder}}: {{Revealing Relationships}} in {{RDF Knowledge Bases}}},
  shorttitle = {{{RelFinder}}},
  booktitle = {Semantic {{Multimedia}}},
  author = {Heim, Philipp and Hellmann, Sebastian and Lehmann, Jens and Lohmann, Steffen and Stegemann, Timo},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Chua, Tat-Seng and Kompatsiaris, Yiannis and M{\'e}rialdo, Bernard and Haas, Werner and Thallinger, Georg and Bailer, Werner},
  year = {2009},
  volume = {5887},
  pages = {182--187},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-10543-2_21},
  urldate = {2024-09-26},
  abstract = {The Semantic Web has recently seen a rise of large knowledge bases (such as DBpedia) that are freely accessible via SPARQL endpoints. The structured representation of the contained information opens up new possibilities in the way it can be accessed and queried. In this paper, we present an approach that extracts a graph covering relationships between two objects of interest. We show an interactive visualization of this graph that supports the systematic analysis of the found relationships by providing highlighting, previewing, and filtering features.},
  isbn = {978-3-642-10542-5 978-3-642-10543-2},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/JBIYGIND/Book_2009_RelFinder.pdf}
}

@incollection{hertelRDFStorageRetrieval2009HandbookonOntologies,
  title = {{{RDF Storage}} and {{Retrieval Systems}}},
  booktitle = {Handbook on {{Ontologies}}},
  author = {Hertel, Alice and Broekstra, Jeen and Stuckenschmidt, Heiner},
  editor = {Staab, Steffen and Studer, Rudi},
  year = {2009},
  pages = {489--508},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-92673-3_22},
  urldate = {2024-09-29},
  abstract = {Ontologies are often used to improve data access. For this purpose, existing data has to be linked to an ontology and appropriate access mechanisms have to be provided. In this chapter, we review RDF storage and retrieval technologies as a common approach for accessing ontology-based data. We discuss different storage models, typical functionalities of RDF middleware such as data model support and reasoning capabilities and RDF query languages with a special focus on SPARQL as an emerging standard. We also discuss some trends such as support for expressive ontology and rule languages.},
  isbn = {978-3-540-70999-2 978-3-540-92673-3},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/JZKYR4AV/Book_2009_RDF Storage and Retrieval Systems.pdf}
}

@article{hevnerDesignScienceInformation2004MISQuarterly,
  title = {Design {{Science}} in {{Information Systems Research}}},
  author = {{Hevner} and {March} and {Park} and {Ram}},
  year = {2004},
  journal = {MIS Quarterly},
  volume = {28},
  number = {1},
  eprint = {10.2307/25148625},
  eprinttype = {jstor},
  pages = {75},
  issn = {02767783},
  doi = {10.2307/25148625},
  urldate = {2024-11-25},
  abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioralscience paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application},
  keywords = {,Jab/MQ,Unread},
  annotation = {11461 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research.},
  file = {/Users/janwardenga/Zotero/storage/AHT4S8HL/MQ_2004_Design Science in Information Systems Research.pdf}
}

@inproceedings{hirigoyenCopyMechanismHandling2022Find.Assoc.Comput.Linguist.AACL-IJCNLP2022,
  title = {A {{Copy Mechanism}} for {{Handling Knowledge Base Elements}} in {{SPARQL Neural Machine Translation}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{AACL-IJCNLP}} 2022},
  author = {Hirigoyen, Rose and Zouaq, Amal and Reyd, Samuel},
  year = {2022},
  pages = {226--236},
  publisher = {Association for Computational Linguistics},
  address = {Online only},
  doi = {10.18653/v1/2022.findings-aacl.22},
  urldate = {2025-02-22},
  abstract = {Neural Machine Translation (NMT) models from English to SPARQL are a promising development for SPARQL query generation. However, current architectures are unable to integrate the knowledge base (KB) schema and handle questions on knowledge resources, classes, and properties unseen during training, rendering them unusable outside the scope of topics covered in the training set. Inspired by the performance gains in natural language processing tasks, we propose to integrate a copy mechanism for neural SPARQL query generation as a way to tackle this issue. We illustrate our proposal by adding a copy layer and a dynamic knowledge base vocabulary to two Seq2Seq architectures (CNNs and Transformers). This layer makes the models copy KB elements directly from the questions, instead of generating them. We evaluate our approach on state-of-the-art datasets, including datasets referencing unknown KB elements and measure the accuracy of the copy-augmented architectures. Our results show a considerable increase in performance on all datasets compared to noncopy architectures.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/TZ5VIQ5W/Hirigoyen et al. - 2022 - A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation.pdf}
}

@inproceedings{hofstatterFiDLightEfficientEffective2023Proc.46thInt.ACMSIGIRConf.Res.Dev.Inf.Retr.,
  title = {{{FiD-Light}}: {{Efficient}} and {{Effective Retrieval-Augmented Text Generation}}},
  shorttitle = {{{FiD-Light}}},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Hofst{\"a}tter, Sebastian and Chen, Jiecao and Raman, Karthik and Zamani, Hamed},
  year = {2023},
  month = jul,
  pages = {1437--1447},
  publisher = {ACM},
  address = {Taipei Taiwan},
  doi = {10.1145/3539618.3591687},
  urldate = {2024-09-29},
  abstract = {Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrievalaugmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.},
  isbn = {978-1-4503-9408-6},
  langid = {english},
  keywords = {Jab/SISCRDIR,Unread},
  annotation = {67 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/I3GWUWA3/SISCRDIR_2023_FiD-Light.pdf}
}

@article{hoganKnowledgeGraphs2022ACMComput.Surv.,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D'amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos{\'e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  year = {2022},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {4},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3447772},
  urldate = {2025-01-26},
  abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
  langid = {english},
  keywords = {Read},
  annotation = {2 citations (Semantic Scholar/DOI) [2025-02-15]\\
1396 citations (Semantic Scholar/DOI) [2025-01-26]\\
TLDR: This article provides a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data.},
  file = {/Users/janwardenga/Zotero/storage/E2BTGN67/Hogan et al. - 2022 - Knowledge Graphs.pdf}
}

@misc{hongNextGenerationDatabaseInterfaces2024,
  title = {Next-{{Generation Database Interfaces}}: {{A Survey}} of {{LLM-based Text-to-SQL}}},
  shorttitle = {Next-{{Generation Database Interfaces}}},
  author = {Hong, Zijin and Yuan, Zheng and Zhang, Qinggang and Chen, Hao and Dong, Junnan and Huang, Feiran and Huang, Xiao},
  year = {2024},
  month = jul,
  number = {arXiv:2406.08426},
  eprint = {2406.08426},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.08426},
  urldate = {2024-09-29},
  abstract = {Generating accurate SQL from natural language questions (text-to-SQL) is a long-standing challenge due to the complexities in user question understanding, database schema comprehension, and SQL generation. Conventional text-to-SQL systems, comprising human engineering and deep neural networks, have made substantial progress. Subsequently, pre-trained language models (PLMs) have been developed and utilized for text-to-SQL tasks, achieving promising performance. As modern databases become more complex, the corresponding user questions also grow more challenging, causing PLMs with parameter constraints to produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which, in turn, restricts the applications of PLM-based systems. Recently, large language models (LLMs) have demonstrated significant capabilities in natural language understanding as the model scale increases. Therefore, integrating LLM-based implementation can bring unique opportunities, improvements, and solutions to textto-SQL research. In this survey, we present a comprehensive review of LLM-based text-to-SQL. Specifically, we propose a brief overview of the technical challenges and the evolutionary process of text-to-SQL. Then, we provide a detailed introduction to the datasets and metrics designed to evaluate text-to-SQL systems. After that, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we discuss the remaining challenges in this field and propose expectations for future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {21 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/KVVKWIII/Pre_2024_Next-Generation Database Interfaces.pdf}
}

@book{huangAdvancedIntelligentComputing2024,
  title = {Advanced {{Intelligent Computing Technology}} and {{Applications}}: 20th {{International Conference}}, {{ICIC}} 2024, {{Tianjin}}, {{China}}, {{August}} 5--8, 2024, {{Proceedings}}, {{Part VI}}},
  shorttitle = {Advanced {{Intelligent Computing Technology}} and {{Applications}}},
  editor = {Huang, De-Shuang and Si, Zhanjun and Chen, Wei},
  year = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14880},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-97-5678-0},
  urldate = {2024-09-30},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-981-9756-77-3 978-981-9756-78-0},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/VT8AJ2T4/Huang et al. - 2024 - Advanced Intelligent Computing Technology and Applications 20th International Conference, ICIC 2024.pdf}
}

@misc{huangReasoningLargeLanguage2023,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  year = {2023},
  month = may,
  number = {arXiv:2212.10403},
  eprint = {2212.10403},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10403},
  urldate = {2025-03-03},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Read},
  annotation = {483 citations (Semantic Scholar/DOI) [2025-03-03]\\
TLDR: A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions are provided.},
  file = {/Users/janwardenga/Zotero/storage/2XWP24PZ/Huang and Chang - 2023 - Towards Reasoning in Large Language Models A Survey.pdf;/Users/janwardenga/Zotero/storage/XH7VNXG8/2212.html}
}

@inproceedings{huangRetrievalAugmentedGeneration2023Proc.13thInt.Jt.Conf.Nat.Lang.Process.3rdConf.Asia-Pac.ChapterAssoc.Comput.Linguist.Vol.1LongPap.,
  title = {Retrieval {{Augmented Generation}} with {{Rich Answer Encoding}}},
  booktitle = {Proceedings of the 13th {{International Joint Conference}} on {{Natural Language Processing}} and the 3rd {{Conference}} of the {{Asia-Pacific Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Wenyu and Lapata, Mirella and Vougiouklis, Pavlos and Papasarantopoulos, Nikos and Pan, Jeff},
  year = {2023},
  pages = {1012--1025},
  publisher = {Association for Computational Linguistics},
  address = {Nusa Dua, Bali},
  doi = {10.18653/v1/2023.ijcnlp-main.65},
  urldate = {2024-09-29},
  abstract = {Knowledge-intensive generation tasks like generative question answering require models to retrieve appropriate passages from external knowledge sources to support answer generation. The generation quality relies heavily on the retrieved passages, which serve as contextual information. State-of-the-art Retrieval Augmented Generation models with marginalized output dominate this area but focus too much on label-relevant passages, rather than question-relevant passages and answers. This work addresses this issue by incorporating rich answer encoding through Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC). We demonstrate the advantages of our proposed approach in open domain question answering (MSMARCO) and conversation (Wizard of Wikipedia) datasets, reporting both generation and retrieval metrics. In the MSMARCO development set, our best model achieves 12.1\% relative improvement1 on Recall@1 and 4.5\% relative improvement on BLEU-4 compared to the baseline model. In the KILT-WoW leaderboard, our best model achieves 8.9\% relative improvement on R-Precision and 13.3\% relative improvement on KILT-RL compared to the baseline model. Our codes and models are available at https://github.com/hwy9855/rag-ae.},
  langid = {english},
  keywords = {Jab/Volume 1: Long Papers,Unread},
  annotation = {13 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work incorporates rich answer encoding through Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC) and demonstrates the advantages of the proposed approach in open domain question answering (MSMARCO) and conversation datasets, reporting both generation and retrieval metrics.},
  file = {/Users/janwardenga/Zotero/storage/3W492GWU/Volume 1 Long Papers_2023_Retrieval Augmented Generation with Rich Answer Encoding.pdf}
}

@misc{huGRAGGraphRetrievalAugmented2024,
  title = {{{GRAG}}: {{Graph Retrieval-Augmented Generation}}},
  shorttitle = {{{GRAG}}},
  author = {Hu, Yuntong and Lei, Zhihan and Zhang, Zheng and Pan, Bo and Ling, Chen and Zhao, Liang},
  year = {2024},
  month = may,
  number = {arXiv:2405.16506},
  eprint = {2405.16506},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.16506},
  urldate = {2024-09-29},
  abstract = {While Retrieval-Augmented Generation (RAG) enhances the accuracy and relevance of responses by generative language models, it falls short in graph-based contexts where both textual and topological information are important. Naive RAG approaches inherently neglect the structural intricacies of textual graphs, resulting in a critical gap in the generation process. To address this challenge, we introduce Graph Retrieval-Augmented Generation (GRAG), which significantly enhances both the retrieval and generation processes by emphasizing the importance of subgraph structures. Unlike RAG approaches that focus solely on text-based entity retrieval, GRAG maintains an acute awareness of graph topology, which is crucial for generating contextually and factually coherent responses. Our GRAG approach consists of four main stages: indexing of k-hop ego-graphs, graph retrieval, soft pruning to mitigate the impact of irrelevant entities, and generation with pruned textual subgraphs. GRAG's core workflow---retrieving textual subgraphs followed by soft pruning---efficiently identifies relevant subgraph structures while avoiding the computational infeasibility typical of exhaustive subgraph searches, which are NP-hard. Moreover, we propose a novel prompting strategy that achieves lossless conversion from textual subgraphs to hierarchical text descriptions. Extensive experiments on graph multi-hop reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods while effectively mitigating hallucinations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Unread},
  annotation = {11 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/878GS8AA/Pre_2024_GRAG.pdf}
}

@inproceedings{iglesiasSDMRDFizerRMLInterpreter2020Proc.29thACMInt.Conf.Inf.Knowl.Manag.,
  title = {{{SDM-RDFizer}}: {{An RML Interpreter}} for the {{Efficient Creation}} of {{RDF Knowledge Graphs}}},
  shorttitle = {{{SDM-RDFizer}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Iglesias, Enrique and Jozashoori, Samaneh and {Chaves-Fraga}, David and Collarana, Diego and Vidal, Maria-Esther},
  year = {2020},
  month = oct,
  eprint = {2008.07176},
  primaryclass = {cs},
  pages = {3039--3046},
  doi = {10.1145/3340531.3412881},
  urldate = {2024-09-26},
  abstract = {IIn recent years, the amount of data has increased exponentially, and knowledge graphs have gained attention as data structures to integrate data and knowledge harvested from myriad data sources. However, data complexity issues like large volume, high-duplicate rate, and heterogeneity usually characterize these data sources, being required data management tools able to address the negative impact of these issues on the knowledge graph creation process. In this paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping Language (RML), to transform raw data in various formats into an RDF knowledge graph. SDM-RDFizer implements novel algorithms to execute the logical operators between mappings in RML, allowing thus to scale up to complex scenarios where data is not only broad but has a high-duplication rate. We empirically evaluate the SDM-RDFizer performance against diverse testbeds with diverse configurations of data volume, duplicates, and heterogeneity. The observed results indicate that SDM-RDFizer is two orders of magnitude faster than state of the art, thus, meaning that SDM-RDFizer an interoperable and scalable solution for knowledge graph creation. SDM-RDFizer is publicly available as a resource through a Github repository and a DOI.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {75 citations (Semantic Scholar/DOI) [2025-02-15]\\
30 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/HUBZK2WT/Pre_2020_SDM-RDFizer.pdf}
}

@incollection{inbook,
  title = {Voice-Interactive Semantic Search Interface with Vector Databases},
  author = {Bul{\'i}n, Martin and Fr{\'e}mund, Adam},
  year = {2024},
  month = may,
  pages = {25--26},
  isbn = {978-80-261-1228-0},
  keywords = {,Read},
  file = {/Users/janwardenga/Zotero/storage/T9IV2DXG/Pre__Voice-Interactive Semantic Search Interface with Vector Databases.pdf}
}

@inproceedings{InformationRetrievalCollaborative2012Proc.Int.Conf.Knowl.Eng.Ontol.Dev.,
  title = {Information {{Retrieval}} in {{Collaborative Engineering Projects}} - {{A Vector Space Model Approach}}:},
  shorttitle = {Information {{Retrieval}} in {{Collaborative Engineering Projects}} - {{A Vector Space Model Approach}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Knowledge Engineering}} and {{Ontology Development}}},
  year = {2012},
  pages = {233--238},
  publisher = {{SciTePress - Science and and Technology Publications}},
  address = {Barcelona, Spain},
  doi = {10.5220/0004139302330238},
  urldate = {2024-09-29},
  abstract = {This work introduces a conceptual framework and its current implementation to support the classification and discovery of knowledge sources, where every knowledge source is represented through a vector (named Semantic Vector - SV). The novelty of this work addresses the enrichment of such knowledge representations, using the classical vector space model concept extended with ontological support, which means to use ontological concepts and their relations to enrich each SV. Our approach takes into account three different but complementary processes using the following inputs: (1) the statistical relevance of keywords, (2) the ontological concepts, and (3) the ontological relations. SVs are compared against each other, in order to obtain their similarity index, and better support end users with a search/retrieval of knowledge sources capabilities. This paper presents the technical architecture (and respective implementation) supporting the conceptual framework, emphasizing the SV creation process. Moreover, it provides some examples detailing the indexation process of knowledge sources, results achieved so far and future goals pursued here are also presented.},
  isbn = {978-989-8565-30-3},
  langid = {english},
  keywords = {Jab/ICKEOD,Unread},
  annotation = {7 citations (Semantic Scholar/DOI) [2025-02-15]\\
7 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper presents the technical architecture (and respective implementation) supporting the conceptual framework, emphasizing the SV creation process, and provides some examples detailing the indexation process of knowledge sources.},
  file = {/Users/janwardenga/Zotero/storage/R6XGV4IF/ICKEOD_2012_Information Retrieval in Collaborative Engineering Projects - A Vector Space Model Approach.pdf}
}

@inproceedings{jameelMEmbERMaxMarginBased2017Proc.40thInt.ACMSIGIRConf.Res.Dev.Inf.Retr.,
  title = {{{MEmbER}}: {{Max-Margin Based Embeddings}} for {{Entity Retrieval}}},
  shorttitle = {{{MEmbER}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
  year = {2017},
  month = aug,
  pages = {783--792},
  publisher = {ACM},
  address = {Shinjuku Tokyo Japan},
  doi = {10.1145/3077136.3080803},
  urldate = {2024-09-29},
  abstract = {We propose a new class of methods for learning vector space embeddings of entities. While most existing methods focus on modelling similarity, our primary aim is to learn embeddings that are interpretable, in the sense that query terms have a direct geometric representation in the vector space. Intuitively, we want all entities that have some property (i.e. for which a given term is relevant) to be located in some well-defined region of the space. This is achieved by imposing max-margin constraints that are derived from a bagof-words representation of the entities. The resulting vector spaces provide us with a natural vehicle for identifying entities that have a given property (or ranking them according to how much they have the property), and conversely, to describe what a given set of entities have in common. As we show in our experiments, our models lead to a substantially beer performance in a range of entity-oriented search tasks, such as list completion and entity ranking.},
  isbn = {978-1-4503-5022-8},
  langid = {english},
  keywords = {Jab/SISIR,Unread},
  annotation = {24 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/QJLNE7BW/SISIR_2017_MEmbER.pdf}
}

@book{janevKnowledgeGraphsBig2020,
  title = {Knowledge {{Graphs}} and {{Big Data Processing}}},
  editor = {Janev, Valentina and Graux, Damien and Jabeen, Hajira and Sallinger, Emanuel},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12072},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-53199-7},
  urldate = {2025-02-15},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  isbn = {978-3-030-53198-0 978-3-030-53199-7},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/V6CSQ9ZN/Janev et al. - 2020 - Knowledge Graphs and Big Data Processing.pdf}
}

@article{jeongStudyImplementationGenerative2023AAIML,
  title = {A {{Study}} on the {{Implementation}} of {{Generative AI Services Using}} an {{Enterprise Data-Based LLM Application Architecture}}},
  author = {Jeong, Cheonsu},
  year = {2023},
  journal = {Advances in Artificial Intelligence and Machine Learning},
  volume = {03},
  number = {04},
  pages = {1588--1618},
  issn = {25829793},
  doi = {10.54364/AAIML.2023.1191},
  urldate = {2024-09-29},
  abstract = {This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation.},
  langid = {english},
  keywords = {Jab/AAIML,Unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/2ZWQTTW6/AAIML_2023_A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture.pdf}
}

@misc{jiangActiveRetrievalAugmented2023,
  title = {Active {{Retrieval Augmented Generation}}},
  author = {Jiang, Zhengbao and Xu, Frank F. and Gao, Luyu and Sun, Zhiqing and Liu, Qian and {Dwivedi-Yu}, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  year = {2023},
  month = oct,
  number = {arXiv:2305.06983},
  eprint = {2305.06983},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.06983},
  urldate = {2024-09-29},
  abstract = {Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {180 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6PAD7DVM/Pre_2023_Active Retrieval Augmented Generation.pdf}
}

@misc{jiangLongRAGEnhancingRetrievalAugmented2024,
  title = {{{LongRAG}}: {{Enhancing Retrieval-Augmented Generation}} with {{Long-context LLMs}}},
  shorttitle = {{{LongRAG}}},
  author = {Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu},
  year = {2024},
  month = sep,
  number = {arXiv:2406.15319},
  eprint = {2406.15319},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.15319},
  urldate = {2024-09-29},
  abstract = {In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the ``needle'' unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced ``heavy'' retriever and ``light'' reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a ``long retriever'' and a ``long reader''. In the two Wikipedia-based datasets, NQ and HotpotQA, where the average document size is less than 1K tokens, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents, making these units 30 times longer than before. By increasing the unit size, we significantly reduce the total number of units from 22M to 600K. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Compared to traditional RAG, which may require hundreds of short units to achieve similar retrieval performance, our approach minimizes the likelihood of retrieving hard negatives while maintaining semantic integrity of each unit. Then we feed these retrieved units ({$\approx$} 30K tokens) to an existing long-context LLM to perform zero-shot answer generation. Without requiring any training, LongRAG achieves an EM of 62.7\% on NQ and 64.3\% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en, where the average document length is already above 4K tokens. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9\% on Qasper (previously 22.5\%) and 57.5\% on MultiFieldQA-en (previously 51.2\%). Our study offers insights into the future roadmap for combining RAG with long-context LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {26 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/23HP4XW8/Pre_2024_LongRAG.pdf}
}

@article{jiFSTREFuzzySpatiotemporal2024IEEETrans.FuzzySyst.,
  title = {{{FSTRE}}: {{Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection}} and {{Rotation}}},
  shorttitle = {{{FSTRE}}},
  author = {Ji, Hao and Yan, Li and Ma, Zongmin},
  year = {2024},
  month = feb,
  journal = {IEEE Transactions on Fuzzy Systems},
  volume = {32},
  number = {2},
  pages = {435--444},
  issn = {1063-6706, 1941-0034},
  doi = {10.1109/TFUZZ.2023.3300295},
  urldate = {2024-09-26},
  abstract = {Knowledge graphs (KGs) use resource description framework (RDF) triples to model various crisp and static resources in the world. Meanwhile, knowledge embedded into vector space can imply more meanings. Much real-world information, however, is often uncertain and dynamic. Existing KG embedding (KGE) models are insufficient to deal with uncertain dynamic knowledge in vector spaces. To overcome this drawback, this article concentrates on an embedding module for the distributed representation of uncertain dynamic knowledge and proposes a strongly adaptive fuzzy spatiotemporal RDF embedding model (FSTRE). Specifically, we first propose a fine-grained fuzzy spatiotemporal RDF model, which provides the underlying representation framework for FSTRE. Then, within the complex vector space, spatial and temporal information is embedded by projection and rotation, respectively. Fine-grained fuzziness penetrates each element of the spatiotemporal five-tuples by a modal length of the anisotropic vectors. By using geometric operations as its transformation operator, FSTRE can capture the rich interaction between crisp and static knowledge and fuzzy spatiotemporal knowledge. We performed an experimental evaluation of FSTRE based on the built fuzzy spatiotemporal KG. It was shown that our FSTRE model is superior to state-of-the-art methods and can handle complex fuzzy spatiotemporal knowledge.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Unread},
  annotation = {8 citations (Semantic Scholar/DOI) [2025-02-15]\\
6 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/4EHLSS6L/TFS_2024_FSTRE.pdf}
}

@misc{jingWhenLargeLanguage2024,
  title = {When {{Large Language Models Meet Vector Databases}}: {{A Survey}}},
  shorttitle = {When {{Large Language Models Meet Vector Databases}}},
  author = {Jing, Zhi and Su, Yongye and Han, Yikun and Yuan, Bo and Xu, Haiyun and Liu, Chunjiang and Chen, Kehai and Zhang, Min},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01763},
  eprint = {2402.01763},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.01763},
  urldate = {2024-09-29},
  abstract = {This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {19 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/3NIZZ3D7/Pre_2024_When Large Language Models Meet Vector Databases.pdf}
}

@misc{jinLargeLanguageModels2024,
  title = {Large {{Language Models}} on {{Graphs}}: {{A Comprehensive Survey}}},
  shorttitle = {Large {{Language Models}} on {{Graphs}}},
  author = {Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and Ji, Heng and Han, Jiawei},
  year = {2024},
  month = oct,
  number = {arXiv:2312.02783},
  eprint = {2312.02783},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2312.02783},
  urldate = {2024-11-20},
  abstract = {Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Jab/Pre,Unread},
  annotation = {96 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/C8SQKBZT/Pre_2024_Large Language Models on Graphs.pdf;/Users/janwardenga/Zotero/storage/K6MQZ3ST/2312.html}
}

@misc{jinRAGCacheEfficientKnowledge2024,
  title = {{{RAGCache}}: {{Efficient Knowledge Caching}} for {{Retrieval-Augmented Generation}}},
  shorttitle = {{{RAGCache}}},
  author = {Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  year = {2024},
  month = apr,
  number = {arXiv:2404.12457},
  eprint = {2404.12457},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.12457},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4{\texttimes} and improves the throughput by up to 2.1{\texttimes} compared to vLLM integrated with Faiss.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Unread},
  annotation = {22 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/UADJW7UW/Pre_2024_RAGCache.pdf}
}

@article{jiSurveyHallucinationNatural2023ACMComput.Surv.,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Chen, Delong and Dai, Wenliang and Chan, Ho Shu and Madotto, Andrea and Fung, Pascale},
  year = {2023},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {12},
  eprint = {2202.03629},
  primaryclass = {cs},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3571730},
  urldate = {2025-01-02},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation; and (3) hallucinations in large language models (LLMs) 1. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG. CCS Concepts: {$\bullet$} Computing methodologies {$\rightarrow$} Natural language generation; Neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Read},
  annotation = {1864 citations (Semantic Scholar/DOI) [2025-02-15]\\
1741 citations (Semantic Scholar/DOI) [2025-01-02]\\
TLDR: A broad overview of the research progress and challenges in the hallucination problem in NLG is provided, including task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.},
  file = {/Users/janwardenga/Zotero/storage/SQ82GL77/Ji et al. - 2023 - Survey of Hallucination in Natural Language Generation.pdf}
}

@article{jovanovicConnectingAIMerging2023Computer,
  title = {Connecting {{AI}}: {{Merging Large Language Models}} and {{Knowledge Graph}}},
  shorttitle = {Connecting {{AI}}},
  author = {Jovanovi{\'c}, Mla{\dj}an and Campbell, Mark},
  year = {2023},
  month = nov,
  journal = {Computer},
  volume = {56},
  number = {11},
  pages = {103--108},
  issn = {0018-9162, 1558-0814},
  doi = {10.1109/MC.2023.3305206},
  urldate = {2024-09-29},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  keywords = {Jab/C,Unread},
  annotation = {8 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6FDI448A/C_2023_Connecting AI.pdf}
}

@incollection{kadilierakisKeywordSearchRDF2020TheSemanticWeb,
  title = {Keyword {{Search}} over {{RDF Using Document-Centric Information Retrieval Systems}}},
  booktitle = {The {{Semantic Web}}},
  author = {Kadilierakis, Giorgos and Fafalios, Pavlos and Papadakos, Panagiotis and Tzitzikas, Yannis},
  editor = {Harth, Andreas and Kirrane, Sabrina and Ngonga Ngomo, Axel-Cyrille and Paulheim, Heiko and Rula, Anisa and Gentile, Anna Lisa and Haase, Peter and Cochez, Michael},
  year = {2020},
  volume = {12123},
  pages = {121--137},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-49461-2_8},
  urldate = {2024-09-29},
  abstract = {For ordinary users, the task of accessing knowledge graphs through structured query languages like SPARQL is rather demanding. As a result, various approaches exploit the simpler and widely used keyword-based search paradigm, either by translating keyword queries to structured queries, or by adopting classical information retrieval (IR) techniques. In this paper, we study and adapt Elasticsearch, an out-ofthe-box document-centric IR system, for supporting keyword search over RDF datasets. Contrary to other works that mainly retrieve entities, we opt for retrieving triples, due to their expressiveness and informativeness. We specify the set of functional requirements and study the emerging questions related to the selection and weighting of the triple data to index, and the structuring and ranking of the retrieved results. Finally, we perform an extensive evaluation of the different factors that affect the IR performance for four different query types. The reported results are promising and offer useful insights on how different Elasticsearch configurations affect the retrieval effectiveness and efficiency.},
  isbn = {978-3-030-49460-5 978-3-030-49461-2},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/5I2LY2DZ/Book_2020_Keyword Search over RDF Using Document-Centric Information Retrieval Systems.pdf}
}

@misc{kalinowskiComparativeStudyStructural2020,
  title = {A {{Comparative Study}} on {{Structural}} and {{Semantic Properties}} of {{Sentence Embeddings}}},
  author = {Kalinowski, Alexander and An, Yuan},
  year = {2020},
  month = sep,
  number = {arXiv:2009.11226},
  eprint = {2009.11226},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2009.11226},
  urldate = {2024-09-29},
  abstract = {Sentence embeddings encode natural language sentences as low-dimensional dense vectors. A great deal of effort has been put into using sentence embeddings to improve several important natural language processing tasks. Relation extraction is such an NLP task that aims at identifying structured relations defined in a knowledge base from unstructured text. A promising and more efficient approach would be to embed both the text and structured knowledge in low-dimensional spaces and discover semantic alignments or mappings between them. Although a number of techniques have been proposed in the literature for embedding both sentences and knowledge graphs, little is known about the structural and semantic properties of these embedding spaces in terms of relation extraction. In this paper, we investigate the aforementioned properties by evaluating the extent to which sentences carrying similar senses are embedded in close proximity sub-spaces, and if we can exploit that structure to align sentences to a knowledge graph. We propose a set of experiments using a widely-used large-scale data set for relation extraction and focusing on a set of key sentence embedding methods. We additionally provide the code for reproducing these experiments at https://github.com/akalino/semanticstructural-sentences. These embedding methods cover a wide variety of techniques ranging from simple word embedding combination to transformer-based BERT-style model. Our experimental results show that different embedding spaces have different degrees of strength for the structural and semantic properties. These results provide useful information for developing embedding-based relation extraction methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/R7B4DIF2/Pre_2020_A Comparative Study on Structural and Semantic Properties of Sentence Embeddings.pdf}
}

@inproceedings{kalinskyExplorationKnowledgeGraphs20222022IEEE38thInt.Conf.DataEng.ICDE,
  title = {Exploration of {{Knowledge Graphs}} via {{Online Aggregation}}},
  booktitle = {2022 {{IEEE}} 38th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Kalinsky, Oren and Hogan, Aidan and Mishali, Oren and Etsion, Yoav and Kimelfeld, Benny},
  year = {2022},
  month = may,
  pages = {2695--2708},
  publisher = {IEEE},
  address = {Kuala Lumpur, Malaysia},
  doi = {10.1109/ICDE53745.2022.00247},
  urldate = {2025-01-23},
  abstract = {Exploration systems over large-scale RDF knowledge graphs often rely on aggregate count queries to indicate how many results the user can expect for the possible next steps of exploration. Such systems thus encounter a challenging computational problem: evaluating aggregate count queries efficiently enough to allow for interactive exploration. Given that precise results are not always necessary, a promising alternative is to apply online aggregation, where initially imprecise results converge towards more precise results over time. However, stateof-the-art online aggregation algorithms, such as Wander Join, fail to provide accurate results due to frequent rejected paths that slow convergence. We thus devise an algorithm for online aggregation that specializes in exploration queries on knowledge graphs; our proposal leverages the low dimension of RDF graphs, and the low selectivity of exploration queries, by augmenting random walks with exact partial computations using a worstcase optimal join algorithm. This approach reduces the number of rejected paths encountered while retaining a fast sample time. In an experimental study with random interactions exploring two large-scale knowledge graphs, our algorithm shows a clear reduction in error over time versus Wander Join.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66540-883-7},
  langid = {english},
  keywords = {Jab/ICDE,Read},
  annotation = {258 citations (Semantic Scholar/DOI) [2025-02-15]\\
2 citations (Semantic Scholar/DOI) [2025-01-23]\\
TLDR: An algorithm for online aggregation that specializes in exploration queries on knowledge graphs is devised, which leverages the low dimension of RDF graphs, and the low selectivity of exploration queries, by augmenting random walks with exact partial computations using a worst-case optimal join algorithm.},
  file = {/Users/janwardenga/Zotero/storage/JUYW2KSL/ICDE_2022_Exploration of Knowledge Graphs via Online Aggregation.pdf}
}

@misc{kandpalLargeLanguageModels2023,
  title = {Large {{Language Models Struggle}} to {{Learn Long-Tail Knowledge}}},
  author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  year = {2023},
  month = jul,
  number = {arXiv:2211.08411},
  eprint = {2211.08411},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.08411},
  urldate = {2025-03-03},
  abstract = {The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Read},
  annotation = {315 citations (Semantic Scholar/DOI) [2025-03-03]\\
TLDR: It is shown that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training, and retrieval-augmentation can reduce the dependence on relevant pre- training information, presenting a promising approach for capturing the long-tail.},
  file = {/Users/janwardenga/Zotero/storage/VCG3FXL5/Kandpal et al. - 2023 - Large Language Models Struggle to Learn Long-Tail Knowledge.pdf;/Users/janwardenga/Zotero/storage/9ABBRFFZ/2211.html}
}

@inproceedings{Kang2022KnowledgeConsistentDG,
  title = {Knowledge-Consistent Dialogue Generation with Knowledge Graphs},
  author = {Kang, Minki and Kwak, Jin Myung and Baek, Jinheon and Hwang, Sung Ju},
  year = {2022},
  url = {https://api.semanticscholar.org/CorpusID:252760721},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/NLAQYB36/Kang et al. - Knowledge-Consistent Dialogue Generation with Knowledge Graphs.pdf}
}

@incollection{kawamuraReportFirstKnowledge2020SemanticTechnology,
  title = {Report on the {{First Knowledge Graph Reasoning Challenge}} 2018: {{Toward}} the {{eXplainable AI System}}},
  shorttitle = {Report on the {{First Knowledge Graph Reasoning Challenge}} 2018},
  booktitle = {Semantic {{Technology}}},
  author = {Kawamura, Takahiro and Egami, Shusaku and Tamura, Koutarou and Hokazono, Yasunori and Ugai, Takanori and Koyanagi, Yusuke and Nishino, Fumihito and Okajima, Seiji and Murakami, Katsuhiko and Takamatsu, Kunihiko and Sugiura, Aoi and Shiramatsu, Shun and Zhang, Xiangyu and Kozaki, Kouji},
  editor = {Wang, Xin and Lisi, Francesca Alessandra and Xiao, Guohui and Botoeva, Elena},
  year = {2020},
  volume = {12032},
  pages = {18--34},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-41407-8_2},
  urldate = {2024-09-29},
  abstract = {A new challenge for knowledge graph reasoning started in 2018. Deep learning has promoted the application of artificial intelligence (AI) techniques to a wide variety of social problems. Accordingly, being able to explain the reason for an AI decision is becoming important to ensure the secure and safe use of AI techniques. Thus, we, the Special Interest Group on Semantic Web and Ontology of the Japanese Society for AI, organized a challenge calling for techniques that reason and/or estimate which characters are criminals while providing a reasonable explanation based on an open knowledge graph of a well-known Sherlock Holmes mystery story. This paper presents a summary report of the first challenge held in 2018, including the knowledge graph construction, the techniques proposed for reasoning and/or estimation, the evaluation metrics, and the results. The first prize went to an approach that formalized the problem as a constraint satisfaction problem and solved it using a lightweight formal method; the second prize went to an approach that used SPARQL and rules; the best resource prize went to a submission that constructed word embedding of characters from all sentences of Sherlock Holmes novels; and the best idea prize went to a discussion multi-agents model. We conclude this paper with the plans and issues for the next challenge in 2019.},
  isbn = {978-3-030-41406-1 978-3-030-41407-8},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/QLX6LKM4/Book_2020_Report on the First Knowledge Graph Reasoning Challenge 2018.pdf}
}

@article{kejriwalKnowledgeGraphsCOVID192020Harv.DataSci.Rev.,
  title = {Knowledge {{Graphs}} and {{COVID-19}}: {{Opportunities}}, {{Challenges}}, and {{Implementation}}},
  shorttitle = {Knowledge {{Graphs}} and {{COVID-19}}},
  author = {Kejriwal, Mayank},
  year = {2020},
  month = dec,
  journal = {Harvard Data Science Review},
  doi = {10.1162/99608f92.e45650b8},
  urldate = {2024-09-29},
  abstract = {The COVID-19 pandemic has been truly global and multidimensional in scope, with ramifications extending well beyond health. Yet, unlike previous crises, there is hope that timely release of relevant data sets, as well as advents in AI (artificial intelligence) technology, could lead to compressed timescales in finding a vaccine or cure. Despite the huge existing body of academic literature on the coronavirus family, searching through such a corpus, including new research that has emerged in the wake of the crisis, is a daunting task even for experts. Simple keyword search over such corpora is insufficient for experts who want answers to questions that require linking together multiple pieces of information across documents. In this article, we review an innovative AI technology called a knowledge graph (KG) that could be used to fulfill such complex information needs. We detail the potential for KGs to play an important role in the fight against COVID-19. We also cover challenges and ongoing collaborative implementations of COVID-19 KGs in industry and academia.},
  langid = {english},
  keywords = {Jab/HDSR,Unread},
  annotation = {25 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/Z75JUCAL/HDSR_2020_Knowledge Graphs and COVID-19.pdf}
}

@inproceedings{kimRetrievalAugmentedControllableReview2020Proc.28thInt.Conf.Comput.Linguist.,
  title = {Retrieval-{{Augmented Controllable Review Generation}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Kim, Jihyeok and Choi, Seungtaek and Amplayo, Reinald Kim and Hwang, Seung-won},
  year = {2020},
  pages = {2284--2295},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.207},
  urldate = {2024-09-29},
  abstract = {In this paper, we study review generation given a set of attribute identifiers which are user ID, product ID and rating. This is a difficult subtask of natural language generation since models are limited to the given identifiers, without any specific descriptive information regarding the inputs, when generating the text. The capacity of these models is thus confined and dependent to how well the models can capture vector representations of attributes. We thus propose to additionally leverage references, which are selected from a large pool of texts labeled with one of the attributes, as textual information that enriches inductive biases of given attributes. With these references, we can now pose the problem as an instance of text-to-text generation, which makes the task easier since texts that are syntactically, semantically similar to the output text are provided as inputs. Using this framework, we address issues such as selecting references from a large candidate set without textual context and improving the model complexity for generation. Our experiments show that our models improve over previous approaches on both automatic and human evaluation metrics.},
  langid = {english},
  keywords = {Jab/PICCL,Unread},
  annotation = {18 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/MDEIRNX6/PICCL_2020_Retrieval-Augmented Controllable Review Generation.pdf}
}

@article{kirkImprovingKnowledgeExtraction2024AAAI,
  title = {Improving {{Knowledge Extraction}} from {{LLMs}} for {{Task Learning}} through {{Agent Analysis}}},
  author = {Kirk, James R. and Wray, Robert E. and Lindes, Peter and Laird, John E.},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {18390--18398},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29799},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs) offer significant promise as a knowledge source for task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks. We describe a cognitive-agent approach, STARS, that extends and complements prompt engineering, mitigating its limitations and thus enabling an agent to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The STARS approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous agent, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how an agent, by retrieving and evaluating a breadth of responses from the LLM, can achieve 77 - 94\% task completion in one-shot learning without user oversight. The approach achieves 100\% task completion when human oversight (such as an indication of preference) is provided. Further, the type of oversight largely shifts from explicit, natural language instruction to simple confirmation/discomfirmation of high-quality responses that have been vetted by the agent before presentation to a user.},
  langid = {english},
  keywords = {Unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/5RMIB4QI/PACAI_2024_Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis.pdf}
}

@article{kovriguinaSPARQLGENOneShotPromptbased,
  title = {{{SPARQLGEN}}: {{One-Shot Prompt-based Approach}} for {{SPARQL Query Generation}}},
  author = {Kovriguina, Liubov and Teucher, Roman and Radyush, Daniil and Mouromtsev, Dmitry},
  abstract = {In this work, we present a one-shot generative approach (further referred to as SPARQLGEN) for generating SPARQL queries by augmenting Large Language Models (LLMs) with the relevant context within a single prompt. The prompt includes heterogeneous data sources: a question itself, an RDF subgraph required to answer the question, and an example of a correct SPARQL query for a different question. In the experiments, GPT-3, a popular pre-trained language model from OpenAI, was leveraged, but it is possible to extend the approach to any other generative LLM. We evaluate, how different types of context in the prompt influence the query generation performance on QALD-9, QALD-10 and Bestiary dataset (BESTIARY), which was created to test LLM performance on unseen data, and provide a detailed error analysis. One of the findings is that providing the model with the underlying KG and a random correct query improve the generation results. The approach shows strong results on QALD-9 dataset, but doesn't generalize on QALD-10 and BESTIARY, which can be caused by memorization problem.},
  langid = {english},
  keywords = {,No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/8DSTSXD4/Kovriguina et al. - SPARQLGEN One-Shot Prompt-based Approach for SPARQL Query Generation.pdf}
}

@misc{kumarFineTuningQuantizationLLMs2024,
  title = {Fine-{{Tuning}}, {{Quantization}}, and {{LLMs}}: {{Navigating Unintended Outcomes}}},
  shorttitle = {Fine-{{Tuning}}, {{Quantization}}, and {{LLMs}}},
  author = {Kumar, Divyanshu and Kumar, Anurakt and Agarwal, Sahil and Harshangi, Prashanth},
  year = {2024},
  month = sep,
  number = {arXiv:2404.04392},
  eprint = {2404.04392},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.04392},
  urldate = {2025-01-27},
  abstract = {Large Language Models (LLMs) have become very popular and have found use cases in many domains, such as chatbots, auto-task completion agents, and much more. However, LLMs are vulnerable to different types of attacks, such as jailbreaking, prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these foundational LLMs are subjected to fine-tuning or quantization for better performance and efficiency. We examine the impact of downstream tasks such as fine-tuning and quantization on LLM vulnerability. We test foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions. Our research shows that fine-tuning and quantization reduces jailbreak resistance significantly, leading to increased LLM vulnerabilities. Finally, we demonstrate the utility of external guardrails in reducing LLM vulnerabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]\\
5 citations (Semantic Scholar/arXiv) [2025-01-27]},
  file = {/Users/janwardenga/Zotero/storage/ILXH99ZF/Kumar et al. - 2024 - Fine-Tuning, Quantization, and LLMs Navigating Unintended Outcomes.pdf}
}

@misc{labrunaWhenRetrieveTeaching2024,
  title = {When to {{Retrieve}}: {{Teaching LLMs}} to {{Utilize Information Retrieval Effectively}}},
  shorttitle = {When to {{Retrieve}}},
  author = {Labruna, Tiziano and Campos, Jon Ander and Azkune, Gorka},
  year = {2024},
  month = may,
  number = {arXiv:2404.19705},
  eprint = {2404.19705},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.19705},
  urldate = {2024-09-29},
  abstract = {In this paper, we demonstrate how Large Language Models (LLMs) can effectively learn to use an off-the-shelf information retrieval (IR) system specifically when additional context is required to answer a given question. Given the performance of IR systems, the optimal strategy for question answering does not always entail external information retrieval; rather, it often involves leveraging the parametric memory of the LLM itself. Prior research has identified this phenomenon in the PopQA dataset, wherein the most popular questions are effectively addressed using the LLM's parametric memory, while less popular ones require IR system usage. Following this, we propose a tailored training approach for LLMs, leveraging existing open-domain question answering datasets. Here, LLMs are trained to generate a special token, {$\langle$}RET{$\rangle$}, when they do not know the answer to a question. Our evaluation of the Adaptive Retrieval LLM (ADAPT-LLM) on the PopQA dataset showcases improvements over the same LLM under three configurations: (i) retrieving information for all the questions, (ii) using always the parametric memory of the LLM, and (iii) using a popularity threshold to decide when to use a retriever. Through our analysis, we demonstrate that ADAPT-LLM is able to generate the {$\langle$}RET{$\rangle$} token when it determines that it does not know how to answer a question, indicating the need for IR, while it achieves notably high accuracy levels when it chooses to rely only on its parametric memory.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/HRF6B3H9/Pre_2024_When to Retrieve.pdf}
}

@article{lanComplexKnowledgeBase2023IEEETrans.Knowl.DataEng.,
  title = {Complex {{Knowledge Base Question Answering}}: {{A Survey}}},
  shorttitle = {Complex {{Knowledge Base Question Answering}}},
  author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2023},
  month = nov,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {11},
  pages = {11196--11215},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3223858},
  urldate = {2025-02-26},
  abstract = {Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performances on complex questions are still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances in KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models (PLMs) on complex KBQA. To help readers catch up with SOTA methods, we also provide a comprehensive evaluation and resource about complex KBQA task. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/43WHZ7A5/Lan et al. - 2023 - Complex Knowledge Base Question Answering A Survey.pdf}
}

@article{larsonGraphRAGUnlockingLLM,
  title = {{{GraphRAG}}: {{Unlocking LLM}} Discovery on Narrative Private Data},
  author = {Larson, Jonathan},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/2KZRZBZQ/Larson - GraphRAG Unlocking LLM discovery on narrative private data.pdf}
}

@article{lecueRoleKnowledgeGraphs2020SW,
  title = {On the Role of Knowledge Graphs in Explainable {{AI}}},
  author = {Lecue, Freddy},
  editor = {Hitzler, Pascal and Janowicz, Krzysztof},
  year = {2020},
  month = jan,
  journal = {Semantic Web},
  volume = {11},
  number = {1},
  pages = {41--51},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-190374},
  urldate = {2024-09-29},
  keywords = {Unread},
  annotation = {127 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search.},
  file = {/Users/janwardenga/Zotero/storage/EBN3QMPL/SW_2020_On the role of knowledge graphs in explainable AI.pdf}
}

@inproceedings{leePromptiverseScalableGeneration2022CHIConf.Hum.FactorsComput.Syst.,
  title = {Promptiverse: {{Scalable Generation}} of {{Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation}}},
  shorttitle = {Promptiverse},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lee, Yoonjoo and Chung, John Joon Young and Kim, Tae Soo and Song, Jean Y and Kim, Juho},
  year = {2022},
  month = apr,
  pages = {1--18},
  publisher = {ACM},
  address = {New Orleans LA USA},
  doi = {10.1145/3491102.3502087},
  urldate = {2024-09-29},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  keywords = {Jab/CCCHFCS,Unread},
  annotation = {22 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/4VMVCU7F/CCCHFCS_2022_Promptiverse.pdf}
}

@misc{lewisRetrievalAugmentedGenerationKnowledgeIntensive2021,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2024-09-30},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) --- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Unread},
  annotation = {4306 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/33S9JMUD/Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf}
}

@misc{liangKAGBoostingLLMs2024,
  title = {{{KAG}}: {{Boosting LLMs}} in {{Professional Domains}} via {{Knowledge Augmented Generation}}},
  shorttitle = {{{KAG}}},
  author = {Liang, Lei and Sun, Mengshu and Gui, Zhengke and Zhu, Zhongshu and Jiang, Zhouyu and Zhong, Ling and Qu, Yuan and Zhao, Peilong and Bo, Zhongpu and Yang, Jin and Xiong, Huaidong and Yuan, Lin and Xu, Jun and Wang, Zaoyang and Zhang, Zhiqiang and Zhang, Wen and Chen, Huajun and Chen, Wenguang and Zhou, Jun},
  year = {2024},
  month = sep,
  number = {arXiv:2409.13731},
  eprint = {2409.13731},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.13731},
  urldate = {2024-09-29},
  abstract = {The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-the-art methods, achieving a relative improvement of 19.6\% on hotpotQA and 33.5\% on 2wiki in terms of F1 score. We have successfully applied KAG to two professional knowledge Q\&A tasks of Ant Group, including E-Government Q\&A and E-Health Q\&A, achieving significant improvement in professionalism compared to RAG methods. Furthermore, we will soon natively support KAG on the opensource KG engine OpenSPG, allowing developers to more easily build rigorous knowledge decision-making or convenient information retrieval services. This will facilitate the localized development of KAG, enabling developers to build domain knowledge services with higher accuracy and efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6E7NQLBA/Pre_2024_KAG.pdf}
}

@misc{liAutomatedClinicalData2024,
  title = {Automated {{Clinical Data Extraction}} with {{Knowledge Conditioned LLMs}}},
  author = {Li, Diya and Kadav, Asim and Gao, Aijing and Li, Rui and Bourgon, Richard},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18027},
  eprint = {2406.18027},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.18027},
  urldate = {2024-09-29},
  abstract = {The extraction of lung lesion information from clinical and medical imaging reports is crucial for research on and clinical care of lung-related diseases. Large language models (LLMs) can be effective at interpreting unstructured text in reports, but they often hallucinate due to a lack of domain-specific knowledge, leading to reduced accuracy and posing challenges for use in clinical settings. To address this, we propose a novel framework that aligns generated internal knowledge with external knowledge through in-context learning (ICL). Our framework employs a retriever to identify relevant units of internal or external knowledge and a grader to evaluate the truthfulness and helpfulness of the retrieved internal-knowledge rules, to align and update the knowledge bases. Our knowledge-conditioned approach also improves the accuracy and reliability of LLM outputs by addressing the extraction task in two stages: (i) lung lesion finding detection and primary structured field parsing, followed by (ii) further parsing of lesion description text into additional structured fields. Experiments with expert-curated test datasets demonstrate that this ICL approach can increase the F1 score for key fields (lesion size, margin and solidity) by an average of 12.9\% over existing ICL methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/8M4H3I2C/Pre_2024_Automated Clinical Data Extraction with Knowledge Conditioned LLMs.pdf}
}

@misc{liAutomateKnowledgeConcept2024,
  title = {Automate {{Knowledge Concept Tagging}} on {{Math Questions}} with {{LLMs}}},
  author = {Li, Hang and Xu, Tianlong and Tang, Jiliang and Wen, Qingsong},
  year = {2024},
  month = mar,
  number = {arXiv:2403.17281},
  eprint = {2403.17281},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.17281},
  urldate = {2024-09-29},
  abstract = {Knowledge concept tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been conducted manually with help from pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications. Moreover, the zero/few-shot learning capability of LLMs makes them well-suited for application in educational scenarios, which often face challenges in collecting large-scale, expertise-annotated datasets. By conducting extensive experiments with a variety of representative LLMs, we demonstrate that LLMs are a promising tool for concept tagging in math questions. Furthermore, through case studies examining the results from different LLMs, we draw some empirical conclusions about the key factors for success in applying LLMs to the automatic concept tagging task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/J5RWESME/Pre_2024_Automate Knowledge Concept Tagging on Math Questions with LLMs.pdf}
}

@misc{liCanLLMAlready2023,
  title = {Can {{LLM Already Serve}} as {{A Database Interface}}? {{A BIg Bench}} for {{Large-Scale Database Grounded Text-to-SQLs}}},
  shorttitle = {Can {{LLM Already Serve}} as {{A Database Interface}}?},
  author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Cao, Rongyu and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Ma, Chenhao and Li, Guoliang and Chang, Kevin C. C. and Huang, Fei and Cheng, Reynold and Li, Yongbin},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.03111},
  urldate = {2024-09-29},
  abstract = {Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08\% in execution accuracy, which is still far from the human result of 92.96\%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),Read},
  annotation = {264 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: Bird is presented, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text- to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains and an efficiency analysis is provided to offer insights into generating text-To-efficient-SQLs that are beneficial to industries.},
  file = {/Users/janwardenga/Zotero/storage/NT3FDZ9B/Pre__Can LLM Already Serve as A Database Interface A BIg Bench for Large-Scale Database Grounded Text-to-SQLs.pdf}
}

@article{liddyEnhancedTextRetrieval1998Bul.Am.Soc.Info.Sci.Tech.,
  title = {Enhanced {{Text Retrieval Using Natural Language Processing}}},
  author = {Liddy, Elizabeth D.},
  year = {1998},
  month = apr,
  journal = {Bulletin of the American Society for Information Science and Technology},
  volume = {24},
  number = {4},
  pages = {14--16},
  issn = {0095-4403, 1550-8366},
  doi = {10.1002/bult.91},
  urldate = {2024-09-29},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  keywords = {Jab/BASIST,Unread},
  annotation = {90 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/NY6DSXYJ/BASIST_1998_Enhanced Text Retrieval Using Natural Language Processing.pdf}
}

@inproceedings{liFrameworkKnowledgeGraphEnhanced2024Find.Assoc.Comput.Linguist.EMNLP2024,
  title = {A {{Framework}} of {{Knowledge Graph-Enhanced Large Language Model Based}} on {{Question Decomposition}} and {{Atomic Retrieval}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Li, Yading and Song, Dandan and Zhou, Changzhi and Tian, Yuhang and Wang, Hao and Yang, Ziyi and Zhang, Shuhao},
  year = {2024},
  pages = {11472--11485},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.670},
  urldate = {2025-02-25},
  abstract = {Knowledge graphs (KGs) can provide explainable reasoning for large language models (LLMs), alleviating their hallucination problem. Knowledge graph question answering (KGQA) is a typical benchmark to evaluate the methods enhancing LLMs with KG. Previous methods on KG-enhanced LLM for KGQA either enhance LLMs with KG retrieval in a single round or perform multi-hop KG reasoning in multiple rounds with LLMs. Both of them conduct retrieving and reasoning based solely on the whole original question, without any processing to the question. To tackle this limitation, we propose a framework of KG-enhanced LLM based on question decomposition and atomic retrieval, called KELDaR. We introduce question decomposition tree as the framework for LLM reasoning. This approach extracts the implicit information of reasoning steps within complex questions, serving as a guide to facilitate atomic retrieval on KG targeting the atomic-level simple questions at leaves of the tree. Additionally, we design strategies for atomic retrieval, which extract and retrieve question-relevant KG subgraphs to assist the few-shot LLM in answering atomic-level questions. Experiments on KGQA datasets demonstrate that our framework outperforms existing reasoning-based baselines. And in a low-cost setting without additional training or fine-tuning, our framework achieves competitive or superior results compared to most existing training-based baselines.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/J65B4WRU/Li et al. - 2024 - A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Ato.pdf}
}

@misc{liKnowledgeTaggingSystem2024,
  title = {Knowledge {{Tagging System}} on {{Math Questions}} via {{LLMs}} with {{Flexible Demonstration Retriever}}},
  author = {Li, Hang and Xu, Tianlong and Tang, Jiliang and Wen, Qingsong},
  year = {2024},
  month = jun,
  number = {arXiv:2406.13885},
  eprint = {2406.13885},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.13885},
  urldate = {2024-09-29},
  abstract = {Knowledge tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations are always conducted by pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. With the recent emergence of advanced text encoding algorithms, such as pre-trained language models, many researchers have developed automatic knowledge tagging systems based on calculating the semantic similarity between the knowledge and question embeddings. In this paper, we explore automating the task using Large Language Models (LLMs), in response to the inability of prior encoding-based methods to deal with the hard cases which involve strong domain knowledge and complicated concept definitions. By showing the strong performance of zero- and few-shot results over math questions knowledge tagging tasks, we demonstrate LLMs' great potential in conquering the challenges faced by prior methods. Furthermore, by proposing a reinforcement learning-based demonstration retriever, we successfully exploit the great potential of different-sized LLMs in achieving better performance results while keeping the in-context demonstration usage efficiency high. Data and code is available in https://anonymous.4open.science/r/KnowTS-0563.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/939I5MEL/Pre_2024_Knowledge Tagging System on Math Questions via LLMs with Flexible Demonstration Retriever.pdf}
}

@misc{liLLMPQALLMenhancedPrediction2024,
  title = {{{LLM-PQA}}: {{LLM-enhanced Prediction Query Answering}}},
  shorttitle = {{{LLM-PQA}}},
  author = {Li, Ziyu and Zhao, Wenjie and Katsifodimos, Asterios and Hai, Rihan},
  year = {2024},
  month = sep,
  eprint = {2409.01140},
  primaryclass = {cs},
  doi = {10.1145/3627673.3679210},
  urldate = {2024-09-29},
  abstract = {The advent of Large Language Models (LLMs) provides an opportunity to change the way queries are processed, moving beyond the constraints of conventional SQL-based database systems. However, using an LLM to answer a prediction query is still challenging, since an external ML model has to be employed and inference has to be performed in order to provide an answer. This paper introduces LLM-PQA, a novel tool that addresses prediction queries formulated in natural language. LLM-PQA is the first to combine the capabilities of LLMs and retrieval-augmented mechanism for the needs of prediction queries by integrating data lakes and model zoos. This integration provides users with access to a vast spectrum of heterogeneous data and diverse ML models, facilitating dynamic prediction query answering. In addition, LLM-PQA can dynamically train models on demand, based on specific query requirements, ensuring reliable and relevant results even when no pre-trained model in a model zoo, available for the task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/SMA8WQHM/Pre_2024_LLM-PQA.pdf}
}

@misc{linVectorSearchOpenAI2023,
  title = {Vector {{Search}} with {{OpenAI Embeddings}}: {{Lucene Is All You Need}}},
  shorttitle = {Vector {{Search}} with {{OpenAI Embeddings}}},
  author = {Lin, Jimmy and Pradeep, Ronak and Teofili, Tommaso and Xian, Jasper},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14963},
  eprint = {2308.14963},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2308.14963},
  urldate = {2024-09-29},
  abstract = {We provide a reproducible, end-to-end demonstration of vector search with OpenAI embeddings using Lucene on the popular MS MARCO passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost--benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern ``AI stack'' for search, since such applications have already received substantial investments in existing, widely deployed infrastructure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {17 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/7LSWHHDL/Pre_2023_Vector Search with OpenAI Embeddings.pdf}
}

@misc{liRetrievalAugmentedGeneration2024,
  title = {Retrieval {{Augmented Generation}} or {{Long-Context LLMs}}? {{A Comprehensive Study}} and {{Hybrid Approach}}},
  shorttitle = {Retrieval {{Augmented Generation}} or {{Long-Context LLMs}}?},
  author = {Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  year = {2024},
  month = jul,
  number = {arXiv:2407.16833},
  eprint = {2407.16833},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.16833},
  urldate = {2024-09-29},
  abstract = {Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced sufficiently, LC consistently outperforms RAG in terms of average performance. However, RAG's significantly lower cost remains a distinct advantage. Based on this observation, we propose SELF-ROUTE, a simple yet effective method that routes queries to RAG or LC based on model self-reflection. SELFROUTE significantly reduces the computation cost while maintaining a comparable performance to LC. Our findings provide a guideline for long-context applications of LLMs using RAG and LC.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {17 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/FJLB4ALA/Pre_2024_Retrieval Augmented Generation or Long-Context LLMs.pdf}
}

@misc{liSurveyRetrievalAugmentedText2022,
  title = {A {{Survey}} on {{Retrieval-Augmented Text Generation}}},
  author = {Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
  year = {2022},
  month = feb,
  number = {arXiv:2202.01110},
  eprint = {2202.01110},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2202.01110},
  urldate = {2024-09-29},
  abstract = {Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This paper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some promising directions on top of recent methods to facilitate future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {155 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6T5BC4IU/Pre_2022_A Survey on Retrieval-Augmented Text Generation.pdf}
}

@inproceedings{liuInformationRetrievalMeets2024CompanionProc.ACMWebConf.2024,
  title = {Information {{Retrieval Meets Large Language Models}}},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Liu, Zheng and Zhou, Yujia and Zhu, Yutao and Lian, Jianxun and Li, Chaozhuo and Dou, Zhicheng and Lian, Defu and Nie, Jian-Yun},
  year = {2024},
  month = may,
  pages = {1586--1589},
  publisher = {ACM},
  address = {Singapore Singapore},
  doi = {10.1145/3589335.3641299},
  urldate = {2024-09-29},
  abstract = {The advent of large language models (LLMs) presents both opportunities and challenges for the information retrieval (IR) community. On one hand, LLMs will revolutionize how people access information, meanwhile the retrieval techniques can play a crucial role in addressing many inherent limitations of LLMs. On the other hand, there are open problems regarding the collaboration of retrieval and generation, the potential risks of misinformation, and the concerns about cost-effectiveness. To seize the critical moment for development, it calls for the joint effort from academia and industry on many key issues, including identification of new research problems, proposal of new techniques, and creation of new evaluation protocols. It has been one year since the launch of ChatGPT in November last year, and the entire community is currently undergoing a profound transformation in techniques. Therefore, this workshop will be a timely venue to exchange ideas and forge collaborations. The organizers, committee members, and invited speakers are composed of a diverse group of researchers coming from leading institutions in the world. This event will be made up of multiple sessions, including invited talks, paper presentations, hands-on tutorials, and panel discussions. All the materials collected for this workshop will be archived and shared publicly, which will present a long-term value to the community.},
  isbn = {9798400701726},
  langid = {english},
  keywords = {Jab/WWC,Unread},
  annotation = {5 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This workshop will be a timely venue to exchange ideas and forge collaborations on many key issues, including identification of new research problems, proposal of new techniques, and creation of new evaluation protocols.},
  file = {/Users/janwardenga/Zotero/storage/DBIMVR4X/WWC_2024_Information Retrieval Meets Large Language Models.pdf}
}

@misc{liuOptimizingLLMQueries2024,
  title = {Optimizing {{LLM Queries}} in {{Relational Workloads}}},
  author = {Liu, Shu and Biswal, Asim and Cheng, Audrey and Mo, Xiangxi and Cao, Shiyi and Gonzalez, Joseph E. and Stoica, Ion and Zaharia, Matei},
  year = {2024},
  month = mar,
  number = {arXiv:2403.05821},
  eprint = {2403.05821},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2403.05821},
  urldate = {2024-09-29},
  abstract = {Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking Large Language Models (LLMs) through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, LLM inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize LLM inference for analytical workloads that invoke LLMs within relational queries. We show that relational queries present novel opportunities for accelerating LLM inference, including reordering rows to maximize key-value (KV) cache reuse within the LLM inference engine, reordering columns within a row to further increase cache reuse, and deduplicating redundant inference requests. We implement these optimizations in Apache Spark, with vLLM as the model serving backend and achieve up to 4.4{\texttimes} improvement in end-to-end latency on a benchmark of diverse LLM-based queries on real datasets. To the best of our knowledge, this is the first work to explicitly address the problem of optimizing LLM invocations within SQL queries.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {12 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/JLKQTGDQ/Pre_2024_Optimizing LLM Queries in Relational Workloads.pdf}
}

@article{liuStagedMultiStrategyFramework2025IEEJTransactionsElecEngng,
  title = {Staged {{Multi}}-{{Strategy Framework With Open}}-{{Source Large Language Models}} for {{Natural Language}} to {{{\textsc{SQL}}}} {{Generation}}},
  shorttitle = {Staged {{Multi}}-{{Strategy Framework With Open}}-{{Source Large Language Models}} for {{Natural Language}} To},
  author = {Liu, Chuanlong and Liao, Wei and Xu, Zhen},
  year = {2025},
  month = jan,
  journal = {IEEJ Transactions on Electrical and Electronic Engineering},
  pages = {tee.24268},
  issn = {1931-4973, 1931-4981},
  doi = {10.1002/tee.24268},
  urldate = {2025-02-20},
  abstract = {In the field of natural language to SQL (NL2SQL), significant progress has been made with large pre-trained language models. However, these models still have deficiencies in terms of their ability to generalize, particularly in open-source Large Language Models (LLMs). Additionally, most research efforts tend to overlook the impact of key column information and data table content on the accuracy of queries during the SQL statement generation process. In this paper, we propose a staged, multi-strategy framework called Key Columns and Table Contents (KCTC). The framework is divided into two stages. Firstly, it uses fixed prompt content to extract SQL key column information from natural language questions, including selected columns and conditioned columns. It also formats the output of column information. Secondly, it combines variable prompt content to guide the model in generating SQL statements. It uses the content of the data table for constraints to reduce the impact of errors in condition values on SQL statements. We conducted experiments on the Chinese dataset TableQA using several open-source LLMs. The results demonstrate that our method significantly improved the execution accuracy of SQL statements, with an average increase of 60.29\% and reaching up to 91.22\% accuracy. This result validates the effectiveness of our approach. {\copyright} 2025 Institute of Electrical Engineers of Japan and Wiley Periodicals LLC.},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: A staged, multi-strategy framework called Key Columns and Table Contents (KCTC), which combines variable prompt content to guide the model in generating SQL statements and uses the content of the data table for constraints to reduce the impact of errors in condition values on SQL statements.}
}

@misc{longBailicaiDomainOptimizedRetrievalAugmented2024,
  title = {Bailicai: {{A Domain-Optimized Retrieval-Augmented Generation Framework}} for {{Medical Applications}}},
  shorttitle = {Bailicai},
  author = {Long, Cui and Liu, Yongbin and Ouyang, Chunping and Yu, Ying},
  year = {2024},
  month = jul,
  number = {arXiv:2407.21055},
  eprint = {2407.21055},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.21055},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have exhibited remarkable proficiency in natural language understanding, prompting extensive exploration of their potential applications across diverse domains. In the medical domain, open-source LLMs have demonstrated moderate efficacy following domainspecific fine-tuning; however, they remain substantially inferior to proprietary models such as GPT-4 and GPT-3.5. These opensource models encounter limitations in the comprehensiveness of domain-specific knowledge and exhibit a propensity for 'hallucinations' during text generation. To mitigate these issues, researchers have implemented the Retrieval-Augmented Generation (RAG) approach, which augments LLMs with background information from external knowledge bases while preserving the model's internal parameters. However, document noise can adversely affect performance, and the application of RAG in the medical field remains in its nascent stages. This study presents the Bailicai framework---a novel integration of retrieval-augmented generation with large language models optimized for the medical domain. The Bailicai framework augments the performance of LLMs in medicine through the implementation of four submodules. Experimental results demonstrate that the Bailicai approach surpasses existing medical domain LLMs across multiple medical benchmarks and exceeds the performance of GPT3.5. Furthermore, the Bailicai method effectively attenuates the prevalent issue of hallucinations in medical applications of LLMs and ameliorates the noise-related challenges associated with traditional RAG techniques when processing irrelevant or pseudo-relevant documents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CPHC6B6Q/Pre_2024_Bailicai.pdf}
}

@inproceedings{longwellTripleAugmentedGenerative2024Proc.2024Annu.Int.ACMSIGIRConf.Res.Dev.Inf.Retr.AsiaPac.Reg.,
  title = {Triple {{Augmented Generative Language Models}} for {{SPARQL Query Generation}} from {{Natural Language Questions}}},
  booktitle = {Proceedings of the 2024 {{Annual International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}} in the {{Asia Pacific Region}}},
  author = {Longwell, Jack and Ali Akbar Alavi, Mahdiyar and Zarrinkalam, Fattane and Ensan, Faezeh},
  year = {2024},
  month = dec,
  pages = {269--273},
  publisher = {ACM},
  address = {Tokyo Japan},
  doi = {10.1145/3673791.3698426},
  urldate = {2025-02-26},
  isbn = {9798400707247},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/8NAZAISW/Longwell et al. - 2024 - Triple Augmented Generative Language Models for SPARQL Query Generation from Natural Language Questi.pdf}
}

@misc{luzSemanticParsingNatural2018,
  title = {Semantic {{Parsing Natural Language}} into {{SPARQL}}: {{Improving Target Language Representation}} with {{Neural Attention}}},
  shorttitle = {Semantic {{Parsing Natural Language}} into {{SPARQL}}},
  author = {Luz, Fabiano Ferreira and Finger, Marcelo},
  year = {2018},
  month = mar,
  number = {arXiv:1803.04329},
  eprint = {1803.04329},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.04329},
  urldate = {2025-02-22},
  abstract = {Semantic parsing is the process of mapping a natural language sentence into a formal representation of its meaning. In this work we use the neural network approach to transform natural language sentence into a query to an ontology database in the SPARQL language. This method does not rely on handcraft-rules, high-quality lexicons, manually-built templates or other handmade complex structures. Our approach is based on vector space model and neural networks. The proposed model is based in two learning steps. The first step generates a vector representation for the sentence in natural language and SPARQL query. The second step uses this vector representation as input to a neural network (LSTM with attention mechanism) to generate a model able to encode natural language and decode SPARQL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Unread},
  file = {/Users/janwardenga/Zotero/storage/EUBV2J9J/Luz and Finger - 2018 - Semantic Parsing Natural Language into SPARQL Improving Target Language Representation with Neural.pdf;/Users/janwardenga/Zotero/storage/PF3VJGW9/1803.html}
}

@article{mahaboobSemanticInformationRetrieval2016IJCA,
  title = {Semantic {{Information Retrieval}}: {{An Ontology}} and {{RDF-based Model}}},
  shorttitle = {Semantic {{Information Retrieval}}},
  author = {Mahaboob, S. and Kanakam, Prathyusha and Suryanarayana, D. and Gunnam, Swathi and S., Sharmela},
  year = {2016},
  month = dec,
  journal = {International Journal of Computer Applications},
  volume = {156},
  number = {9},
  pages = {34--38},
  issn = {09758887},
  doi = {10.5120/ijca2016912575},
  urldate = {2024-09-29},
  abstract = {Retrieving the specific knowledge from the Web becomes a challenging task as it contributes enormous amounts of unorganized textual data. This paper focuses on lessening the time consumed by the user for searching the documents and providing the results as per user intention. This paper demonstrates a semantic query language SPARQL to extract the data from the student career knowledge base constructed using Resource Description Framework (RDF) that gives relevant information to the user. This paper provides the requested information by understanding user's query intention with the created career ontology using RDF and SPARQL in a semantic manner.},
  langid = {english},
  keywords = {Jab/IJCA,Unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-20]\\
1 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/5Y7J8BF5/IJCA_2016_Semantic Information Retrieval.pdf}
}

@inproceedings{mai2018combining,
  title = {Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines.},
  booktitle = {Semdeep/{{NLIWoD}}@ {{ISWC}}},
  author = {Mai, Gengchen and Janowicz, Krzysztof and Yan, Bo},
  year = {2018},
  pages = {77--88},
  keywords = {,No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/LDBFF6KJ/Pre__Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines.pdf}
}

@article{maillotIndeGxModelFramework2023JournalofWebSemantics,
  title = {{{IndeGx}}: {{A}} Model and a Framework for Indexing {{RDF}} Knowledge Graphs with {{SPARQL-based}} Test Suits},
  shorttitle = {{{IndeGx}}},
  author = {Maillot, Pierre and Corby, Olivier and Faron, Catherine and Gandon, Fabien and Michel, Franck},
  year = {2023},
  month = apr,
  journal = {Journal of Web Semantics},
  volume = {76},
  pages = {100775},
  issn = {15708268},
  doi = {10.1016/j.websem.2023.100775},
  urldate = {2024-09-26},
  abstract = {In recent years, a large number of RDF datasets have been built and published on the Web in fields as diverse as linguistics or life sciences, as well as general datasets such as DBpedia or Wikidata. The joint exploitation of these datasets requires specific knowledge about their content, access points, and commonalities. However, not all datasets contain a self-description, and not all access points can handle the complex queries used to generate such a description. In this article, we provide a standard-based approach to generate the description of a dataset. The generated descriptions as well as the process of their computation are expressed using standard vocabularies and languages. We implemented our approach into a framework, called IndeGx, where each indexing feature and its computation is collaboratively and declaratively defined in a GitHub repository. We have experimented IndeGx on a set of 339 RDF datasets with endpoints listed in public catalogs, over 8 months. The results show that we can collect, as much as possible, important characteristics of the datasets depending on their availability and capacities. The resulting index captures the commonalities, variety and disparity in the offered content and services and it provides an important support to any application designed to query RDF datasets.},
  langid = {english},
  keywords = {Jab/JWS,Unread},
  annotation = {11 citations (Semantic Scholar/DOI) [2025-02-15]\\
126 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/W9X9EVFS/JWS_2023_IndeGx.pdf}
}

@article{maModelingQueryingTemporal2023JIntellInfSyst,
  title = {Modeling and Querying Temporal {{RDF}} Knowledge Graphs with Relational Databases},
  author = {Ma, Ruizhe and Han, Xiao and Yan, Li and Khan, Nasrullah and Ma, Zongmin},
  year = {2023},
  month = oct,
  journal = {Journal of Intelligent Information Systems},
  volume = {61},
  number = {2},
  pages = {569--609},
  issn = {0925-9902, 1573-7675},
  doi = {10.1007/s10844-023-00780-6},
  urldate = {2024-09-26},
  abstract = {RDF (Resource Description Framework), a standard resource description model, is popularized and applied in many application scenarios for its explicit representation of semantics. To represent and process time-aware semantics with RDF, the temporal RDF model is proposed and applied in temporal knowledge graphs. The requirement for efficiently handling diverse temporal RDF data has become increasingly important with the rapid development and popularity of RDF. In this paper, we propose a novel temporal RDF model and effectively tackle the management of temporal RDF data in relational databases. In particular, we propose a temporal RDF model called tRDF to represent both temporal entities and relationships and further propose a temporal query language for the tRDF model. To manage temporal RDF data in an effective manner, we propose to store temporal RDF data with relational databases that follow the SQL:2011 standard and support temporal data manipulation. To query the tRDF data stored in relational databases with the tRDF query language, we implement the transformation from this query language to SQL. The experimental results show the feasibility and effectiveness of the proposed tRDF model as well as its storage and query methods.},
  langid = {english},
  keywords = {Jab/JIIS,Unread},
  annotation = {6 citations (Semantic Scholar/DOI) [2025-02-15]\\
10 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/U365RV5D/JIIS_2023_Modeling and querying temporal RDF knowledge graphs with relational databases.pdf}
}

@misc{manakulSelfCheckGPTZeroResourceBlackBox2023,
  title = {{{SelfCheckGPT}}: {{Zero-Resource Black-Box Hallucination Detection}} for {{Generative Large Language Models}}},
  shorttitle = {{{SelfCheckGPT}}},
  author = {Manakul, Potsawee and Liusie, Adian and Gales, Mark J. F.},
  year = {2023},
  month = oct,
  number = {arXiv:2303.08896},
  eprint = {2303.08896},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08896},
  urldate = {2024-12-20},
  abstract = {Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,Read},
  annotation = {315 citations (Semantic Scholar/DOI) [2025-02-15]\\
299 citations (Semantic Scholar/DOI) [2024-12-20]\\
TLDR: This work proposes "SelfCheckGPT", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality.},
  file = {/Users/janwardenga/Zotero/storage/LBPUDTNZ/Manakul et al. - 2023 - SelfCheckGPT Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.pdf}
}

@misc{manathungaRetrievalAugmentedGeneration2023,
  title = {Retrieval {{Augmented Generation}} and {{Representative Vector Summarization}} for Large Unstructured Textual Data in {{Medical Education}}},
  author = {Manathunga, S. S. and Illangasekara, Y. A.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.00479},
  eprint = {2308.00479},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2308.00479},
  urldate = {2024-09-29},
  abstract = {Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {H.3.1,J.3,Unread},
  annotation = {9 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/XWHALI24/Pre_2023_Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education.pdf}
}

@misc{maoGenerationAugmentedRetrievalOpendomain2021,
  title = {Generation-{{Augmented Retrieval}} for {{Open-domain Question Answering}}},
  author = {Mao, Yuning and He, Pengcheng and Liu, Xiaodong and Shen, Yelong and Gao, Jianfeng and Han, Jiawei and Chen, Weizhu},
  year = {2021},
  month = aug,
  number = {arXiv:2009.08553},
  eprint = {2009.08553},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2009.08553},
  urldate = {2024-09-29},
  abstract = {We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {209 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/WHDFFHC7/Pre_2021_Generation-Augmented Retrieval for Open-domain Question Answering.pdf}
}

@article{markQueryingKnowledgeGraphs,
  title = {Querying {{Knowledge Graphs}} to {{Increase}} the {{Accuracy}} of {{Large Language Model-based Systems}} for {{Question Answering}}},
  author = {Mark, David Paul},
  langid = {english},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/26K6JWJA/Mark - Querying Knowledge Graphs to Increase the Accuracy of Large Language Model-based Systems for Questio.pdf}
}

@incollection{martinKnowledgeRepresentationSharing2003WebIntelligence,
  title = {Knowledge {{Representation}}, {{Sharing}}, and {{Retrieval}} on the {{Web}}},
  booktitle = {Web {{Intelligence}}},
  author = {Martin, Philippe},
  editor = {Zhong, Ning and Liu, Jiming and Yao, Yiyu},
  year = {2003},
  pages = {243--276},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-05320-1_12},
  urldate = {2024-09-29},
  abstract = {By ``knowledge retrieval'', we refer to the automatic retrieval of statements permitting a tool to make logical inferences and answer queries precisely and correctly, as opposed to retrieving documents or statements ``related to'' the queries. Given the ambiguity of natural language and our current inability to make computers ``understand'' it, the knowledge has to be manually encoded and structured using a formal graphic/textual language and ontologies (structured catalogs of categories and associated constraints of use).},
  isbn = {978-3-642-07936-8 978-3-662-05320-1},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/IPX3JWFV/Book_2003_Knowledge Representation, Sharing, and Retrieval on the Web.pdf}
}

@book{martraLargeLanguageModels2024,
  title = {Large {{Language Models Projects}}: {{Apply}} and {{Implement Strategies}} for {{Large Language Models}}},
  shorttitle = {Large {{Language Models Projects}}},
  author = {Martra, Pere},
  year = {2024},
  publisher = {Apress},
  address = {Berkeley, CA},
  doi = {10.1007/979-8-8688-0515-8},
  urldate = {2024-09-29},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {9798868805141 9798868805158},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/9H9BJA24/Pre_2024_Large Language Models Projects.pdf}
}

@incollection{marxExploringTermNetworks2016MetadataandSemanticsResearch,
  title = {Exploring {{Term Networks}} for {{Semantic Search}} over {{RDF Knowledge Graphs}}},
  booktitle = {Metadata and {{Semantics Research}}},
  author = {Marx, Edgard and H{\"o}ffner, Konrad and Shekarpour, Saeedeh and Ngomo, Axel-Cyrille Ngonga and Lehmann, Jens and Auer, S{\"o}ren},
  editor = {Garoufallou, Emmanouel and Subirats Coll, Imma and Stellato, Armando and Greenberg, Jane},
  year = {2016},
  volume = {672},
  pages = {249--261},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-49157-8_22},
  urldate = {2024-09-29},
  abstract = {Information retrieval approaches are considered as a key technology to empower lay users to access the Web of Data. A large number of related approaches such as Question Answering and Semantic Search have been developed to address this problem. While Question Answering promises more accurate results by returning a specific answer, Semantic Search engines are designed to retrieve the best top-K ranked resources. In this work, we propose *path, a Semantic Search approach that explores term networks for querying RDF knowledge graphs. The adequacy of the approach is evaluated employing benchmark datasets against stateof-the-art Question Answering as well as Semantic Search systems. The results show that *path achieves better F1-score than the currently best performing Semantic Search system.},
  isbn = {978-3-319-49156-1 978-3-319-49157-8},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/C9BSKZIJ/Book_2016_Exploring Term Networks for Semantic Search over RDF Knowledge Graphs.pdf}
}

@article{maStoringMassiveResource2016TheKnowledgeEngineeringReview,
  title = {Storing Massive {{Resource Description Framework}} ({{RDF}}) Data: A Survey},
  shorttitle = {Storing Massive {{Resource Description Framework}} ({{RDF}}) Data},
  author = {Ma, Zongmin and Capretz, Miriam A. M. and Yan, Li},
  year = {2016},
  month = sep,
  journal = {The Knowledge Engineering Review},
  volume = {31},
  number = {4},
  pages = {391--413},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888916000217},
  urldate = {2024-09-29},
  abstract = {The Resource Description Framework (RDF) is a flexible model for representing information about resources on the Web. As a W3C (World Wide Web Consortium) Recommendation, RDF has rapidly gained popularity. With the widespread acceptance of RDF on the Web and in the enter prise, a huge amount of RDF data is being proliferated and becoming available. Efficient and scalable management of RDF data is therefore of increasing importance. RDF data management has attracted attention in the database and Semantic Web communities. Much work has been devoted to proposing different solutions to store RDF data efficiently. This paper focusses on using relational databases and NoSQL (for `not only SQL (Structured Query Language)') databases to store massive RDF data. A full up-to-date overview of the current state of the art in RDF data storage is provided in the paper.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  keywords = {Unread},
  annotation = {45 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/MHS48WJH/KER_2016_Storing massive Resource Description Framework (RDF) data.pdf}
}

@article{matsumotoKRAGENKnowledgeGraphenhanced2024Bioinformatics,
  title = {{{KRAGEN}}: A Knowledge Graph-Enhanced {{RAG}} Framework for Biomedical Problem Solving Using Large Language Models},
  shorttitle = {{{KRAGEN}}},
  author = {Matsumoto, Nicholas and Moran, Jay and Choi, Hyunjun and Hernandez, Miguel E and Venkatesan, Mythreye and Wang, Paul and Moore, Jason H},
  editor = {Kendziorski, Christina},
  year = {2024},
  month = jun,
  journal = {Bioinformatics},
  volume = {40},
  number = {6},
  pages = {btae353},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btae353},
  urldate = {2024-11-14},
  abstract = {Motivation: Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Jab/B,Unread},
  annotation = {11 citations (Semantic Scholar/DOI) [2025-02-15]\\
6 citations (Semantic Scholar/DOI) [2024-11-14]\\
TLDR: KRAGEN is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language to solve complex problems with natural language.},
  file = {/Users/janwardenga/Zotero/storage/JU2CDQ6N/B_2024_KRAGEN.pdf}
}

@article{mcglothlinMaterializingPersistingInferred2010AAAI,
  title = {Materializing and {{Persisting Inferred}} and {{Uncertain Knowledge}} in {{RDF Datasets}}},
  author = {McGlothlin, James and Khan, Latifur},
  year = {2010},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {24},
  number = {1},
  pages = {1405--1412},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v24i1.7522},
  urldate = {2024-09-29},
  abstract = {As the semantic web grows in popularity and enters the mainstream of computer technology, RDF (Resource Description Framework) datasets are becoming larger and more complex. Advanced semantic web ontologies, especially in medicine and science, are developing. As more complex ontologies are developed, there is a growing need for efficient queries that handle inference. In areas such as research, it is vital to be able to perform queries that retrieve not just facts but also inferred knowledge and uncertain information. OWL (Web Ontology Language) defines rules that govern provable inference in semantic web datasets. In this paper, we detail a database schema using bit vectors that is designed specifically for RDF datasets. We introduce a framework for materializing and storing inferred triples. Our bit vector schema enables storage of inferred knowledge without a query performance penalty. Inference queries are simplified and performance is improved. Our evaluation results demonstrate that our inference solution is more scalable and efficient than the current state-of-the-art. There are also standards being developed for representing probabilistic reasoning within OWL ontologies. We specify a framework for materializing uncertain information and probabilities using these ontologies. We define a multiple vector schema for representing probabilities and classifying uncertain knowledge using thresholds. This solution increases the breadth of information that can be efficiently retrieved.},
  langid = {english},
  keywords = {Unread},
  annotation = {11 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper details a database schema using bit vectors that is designed specifically for RDF datasets and introduces a framework for materializing and storing inferred triples, and defines a multiple vector schema for representing probabilities and classifying uncertain knowledge using thresholds.},
  file = {/Users/janwardenga/Zotero/storage/7ER5R9RM/PACAI_2010_Materializing and Persisting Inferred and Uncertain Knowledge in RDF Datasets.pdf}
}

@article{mcglothlinRDFJoinScalableData,
  title = {{{RDFJoin}}: {{A Scalable Data Model}} for {{Persistence}} and {{Efficient Querying}} of {{RDF Datasets}}},
  author = {McGlothlin, James P and Khan, Latifur R},
  abstract = {Recent research has emphasized the design of database schemata and data models constructed to allow scalable and efficient querying of large quantities of semantic web data. The objective is to create a solution that is viable for very large RDF datasets, thus allowing RDF to be used as a solution for development of global databases. The bottleneck in the querying of large RDF datasets is performing joins and unions. Because a RDF dataset is a collection of simple three column tuples, most queries involve many such joins. This paper introduces RDFJoin, a new data model specifically designed to eliminate or greatly reduce the cost of these joins. RDFJoin utilizes bit vectors to efficiently store an entire collection into a single column of a tuple. These bit vectors also allow the use of high speed bit masking operations to join these collections. Using this approach, we create tables that can be accessed and queried more efficiently. RDFJoin provides tables that implement persistent sextuple indexing. Additionally, we introduce join tables that store the results of join executions. We assert that our novel solution reduces the overhead of RDF queries and dramatically improves performance. RDFJoin is a persistent solution using relational database storage. Our experiments demonstrate that RDFJoin consistently outperforms current state-of-the-art technology for querying stored data and even compares favorably to main memory solutions. RDFJoin is truly a scalable and persistent data model for RDF data storage that improves the performance of queries.},
  langid = {english},
  keywords = {No DOI found,Not citable but relevant,Read},
  file = {/Users/janwardenga/Zotero/storage/6RVJT4JU/Pre__RDFJoin A Scalable Data Model for Persistence and Efficient Querying of RDF Datasets.pdf}
}

@inproceedings{mcglothlinRDFKBEfficientSupport2009Proc.2009Int.DatabaseEng.Appl.Symp.-IDEAS09,
  title = {{{RDFKB}}: Efficient Support for {{RDF}} Inference Queries and Knowledge Management},
  shorttitle = {{{RDFKB}}},
  booktitle = {Proceedings of the 2009 {{International Database Engineering}} \& {{Applications Symposium}} on - {{IDEAS}} '09},
  author = {McGlothlin, James P. and Khan, Latifur R.},
  year = {2009},
  pages = {259},
  publisher = {ACM Press},
  address = {Cetraro - Calabria, Italy},
  doi = {10.1145/1620432.1620460},
  urldate = {2024-09-29},
  abstract = {RDFKB (Resource Description Framework Knowledge Base) is a relational database system for RDF datasets which supports inference and knowledge management. Significant research has addressed improving the performance of queries against RDF datasets. Generally, this research has not addressed queries against inferred knowledge. Solutions which do support inference queries have done so as part of query processing. Ontologies define the rules that govern inference for RDF datasets. These inference rules can be applied to RDF datasets to derive additional facts through methods such as subsumption, symmetry and transitive closure. We propose a framework that supports inference at data storage time rather than as part of query processing. The dataset is increased to include all knowledge whether explicitly specified or derived through inference with a negligible overhead. Queries against inferred data are simplified, and performance is increased.},
  isbn = {978-1-60558-402-7},
  langid = {english},
  keywords = {,Jab/IDE&AS,Unread},
  annotation = {33 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work proposes a framework that supports inference at data storage time rather than as part of query processing, and increases the dataset to include all knowledge whether explicitly specified or derived through inference with a negligible overhead.},
  file = {/Users/janwardenga/Zotero/storage/XFFDINRK/IDE&AS_2009_RDFKB.pdf}
}

@article{meijerEmpoweringNaturalProduct2024Nat.Prod.Rep.,
  title = {Empowering Natural Product Science with {{AI}}: Leveraging Multimodal Data and Knowledge Graphs},
  shorttitle = {Empowering Natural Product Science with {{AI}}},
  author = {Meijer, David and Beniddir, Mehdi A. and Coley, Connor W. and Mejri, Yassine M. and {\"O}zt{\"u}rk, Meltem and Van Der Hooft, Justin J. J. and Medema, Marnix H. and Skiredj, Adam},
  year = {2024},
  journal = {Natural Product Reports},
  pages = {10.1039.D4NP00008K},
  issn = {0265-0568, 1460-4752},
  doi = {10.1039/D4NP00008K},
  urldate = {2024-09-29},
  abstract = {This viewpoint article promotes the ongoing efforts to organise natural product science within knowledge graphs, a promising approach for structuring training data for AI models capable of achieving human-level natural product anticipation.           ,              Artificial intelligence (AI) is accelerating how we conduct science, from folding proteins with AlphaFold and summarizing literature findings with large language models, to annotating genomes and prioritizing newly generated molecules for screening using specialized software. However, the application of AI to emulate human cognition in natural product research and its subsequent impact has so far been limited. One reason for this limited impact is that available natural product data is multimodal, unbalanced, unstandardized, and scattered across many data repositories. This makes natural product data challenging to use with existing deep learning architectures that consume fairly standardized, often non-relational, data. It also prevents models from learning overarching patterns in natural product science. In this Viewpoint, we address this challenge and support ongoing initiatives aimed at democratizing natural product data by collating our collective knowledge into a knowledge graph. By doing so, we believe there will be an opportunity to use such a knowledge graph to develop AI models that can truly mimic natural product scientists' decision-making.},
  langid = {english},
  keywords = {Jab/NPR,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/HJZ9ITN8/NPR_2024_Empowering natural product science with AI.pdf}
}

@misc{mielkeWordsCharactersBrief2021,
  title = {Between Words and Characters: {{A Brief History}} of {{Open-Vocabulary Modeling}} and {{Tokenization}} in {{NLP}}},
  shorttitle = {Between Words and Characters},
  author = {Mielke, Sabrina J. and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gall{\'e}, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y. and Sagot, Beno{\^i}t and Tan, Samson},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10508},
  eprint = {2112.10508},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10508},
  urldate = {2025-03-04},
  abstract = {What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subword-based approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Unread},
  annotation = {124 citations (Semantic Scholar/arXiv) [2025-03-04]},
  file = {/Users/janwardenga/Zotero/storage/6IMZPPDB/Mielke et al. - 2021 - Between words and characters A Brief History of Open-Vocabulary Modeling and Tokenization in NLP.pdf;/Users/janwardenga/Zotero/storage/5WIPC4Y9/2112.html}
}

@inproceedings{mihalcea2006corpus,
  title = {Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity},
  booktitle = {Aaai},
  author = {Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo and others},
  year = {2006},
  volume = {6},
  pages = {775--780},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/UJS4WYLM/Pre__Corpus-based and Knowledge-based Measures of Text Semantic Similarity.pdf}
}

@inproceedings{mihindukulasooriyaRDFShapeInduction2018Proc.33rdAnnu.ACMSymp.Appl.Comput.,
  title = {{{RDF}} Shape Induction Using Knowledge Base Profiling},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Mihindukulasooriya, Nandana and Rashid, Mohammad Rifat Ahmmad and Rizzo, Giuseppe and {Garc{\'i}a-Castro}, Ra{\'u}l and Corcho, Oscar and Torchiano, Marco},
  year = {2018},
  month = apr,
  pages = {1952--1959},
  publisher = {ACM},
  address = {Pau France},
  doi = {10.1145/3167132.3167341},
  urldate = {2025-02-02},
  isbn = {978-1-4503-5191-1},
  langid = {english},
  keywords = {Unread},
  annotation = {29 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper presents a data driven approach for inducing integrity constraints for RDF data using data profiling, based on machine learning techniques to automatically generate RDF shapes using profiled RDFData as features.}
}

@inproceedings{mihindukulasooriyaRDFShapeInduction2018Proc.33rdAnnu.ACMSymp.Appl.Comput.a,
  title = {{{RDF}} Shape Induction Using Knowledge Base Profiling},
  booktitle = {Proceedings of the 33rd {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Mihindukulasooriya, Nandana and Rashid, Mohammad Rifat Ahmmad and Rizzo, Giuseppe and {Garc{\'i}a-Castro}, Ra{\'u}l and Corcho, Oscar and Torchiano, Marco},
  year = {2018},
  month = apr,
  pages = {1952--1959},
  publisher = {ACM},
  address = {Pau France},
  doi = {10.1145/3167132.3167341},
  urldate = {2025-02-17},
  isbn = {978-1-4503-5191-1},
  langid = {english},
  keywords = {Unread},
  annotation = {TLDR: This paper presents a data driven approach for inducing integrity constraints for RDF data using data profiling, based on machine learning techniques to automatically generate RDF shapes using profiled RDFData as features.}
}

@inproceedings{milenova2005svm,
  title = {{{SVM}} in Oracle Database 10g: Removing the Barriers to Widespread Adoption of Support Vector Machines},
  booktitle = {Proceedings of the 31st International Conference on {{Very}} Large Data Bases},
  author = {Milenova, Boriana L and Yarmus, Joseph S and Campos, Marcos M},
  year = {2005},
  pages = {1152--1163},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/2UXF9NS4/Pre__SVM in Oracle Database 10g Removing the Barriers to Widespread Adoption of Support Vector Machines.pdf}
}

@misc{minamiSkipVectorsRDF2022,
  title = {Skip {{Vectors}} for {{RDF Data}}: {{Extraction Based}} on the {{Complexity}} of {{Feature Patterns}}},
  shorttitle = {Skip {{Vectors}} for {{RDF Data}}},
  author = {Minami, Yota and Kaneiwa, Ken},
  year = {2022},
  month = mar,
  number = {arXiv:2201.01996},
  eprint = {2201.01996},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2201.01996},
  urldate = {2024-09-29},
  abstract = {The Resource Description Framework (RDF) is a framework for describing metadata, such as attributes and relationships of resources on the Web. Machine learning tasks for RDF graphs adopt three methods: (i) support vector machines (SVMs) with RDF graph kernels, (ii) RDF graph embeddings, and (iii) relational graph convolutional networks. In this paper, we propose a novel feature vector (called a Skip vector) that represents some features of each resource in an RDF graph by extracting various combinations of neighboring edges and nodes. In order to make the Skip vector low-dimensional, we select important features for classification tasks based on the information gain ratio of each feature. The classification tasks can be performed by applying the low-dimensional Skip vector of each resource to conventional machine learning algorithms, such as SVMs, the k-nearest neighbors method, neural networks, random forests, and AdaBoost. In our evaluation experiments with RDF data, such as Wikidata, DBpedia, and YAGO, we compare our method with RDF graph kernels in an SVM. We also compare our method with the two approaches: RDF graph embeddings such as RDF2Vec and relational graph convolutional networks on the AIFB, MUTAG, BGS, and AM benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/8GPKSTKK/Pre_2022_Skip Vectors for RDF Data.pdf}
}

@inproceedings{moEfficientDecisionmakingSMEs2023Low-CostDigit.Solut.Ind.Autom.LoDiSA2023,
  title = {Efficient Decision-Making in {{SMEs}}: Leveraging Knowledge Graphs with {{Neo4j}} and {{AI}} Vision},
  shorttitle = {Efficient Decision-Making in {{SMEs}}},
  booktitle = {Low-{{Cost Digital Solutions}} for {{Industrial Automation}} ({{LoDiSA}} 2023)},
  author = {Mo, F. and Rehman, H. U. and Elshafei, B. and Chaplin, J. C. and Sanderson, D. and {Mart{\'i}nez-Arellano}, G. and Ratchev, S.},
  year = {2023},
  pages = {69--75},
  publisher = {{Institution of Engineering and Technology}},
  address = {Cambridge, UK},
  doi = {10.1049/icp.2023.1736},
  urldate = {2024-09-29},
  abstract = {In the evolving digital landscape, Small and Medium-sized Enterprises (SMEs) grapple with the intricate task of managing vast manufacturing data while operating within budgetary constraints. Addressing this dichotomy, our research introduces an innovative and cost-conscious solution that marries the capabilities of Neo4j's knowledge graph with an AI-enhanced vision system. This integrated system adeptly captures realtime manufacturing data, including product images, configuration details, and specific parameters relevant to the leak testing process. This data is subsequently structured within a comprehensive knowledge graph, enabling SMEs to derive actionable insights and optimize their manufacturing decisions. By harnessing the affordability and scalability of Neo4j's cloud service, the approach we propose stands as a beacon for SMEs, positioning them for enhanced precision in leak testing, operational efficiency, and sustained growth in the digitized economy.},
  isbn = {978-1-83953-932-9},
  langid = {english},
  keywords = {Jab/LoDiSA 2023,Unread},
  file = {/Users/janwardenga/Zotero/storage/34VC6IGL/LoDiSA 2023_2023_Efficient decision-making in SMEs.pdf}
}

@article{moffettGlobalPublicDatabase2009PLoSNeglTropDis,
  title = {A {{Global Public Database}} of {{Disease Vector}} and {{Reservoir Distributions}}},
  author = {Moffett, Alexander and Strutz, Stavana and Guda, Nelson and Gonz{\'a}lez, Camila and Ferro, Maria Cristina and {S{\'a}nchez-Cordero}, V{\'i}ctor and Sarkar, Sahotra},
  editor = {Utzinger, Juerg},
  year = {2009},
  month = mar,
  journal = {PLoS Neglected Tropical Diseases},
  volume = {3},
  number = {3},
  pages = {e378},
  issn = {1935-2735},
  doi = {10.1371/journal.pntd.0000378},
  urldate = {2024-09-29},
  langid = {english},
  keywords = {Jab/PNTD,Unread},
  file = {/Users/janwardenga/Zotero/storage/99FKNCE2/PNTD_2009_A Global Public Database of Disease Vector and Reservoir Distributions.pdf}
}

@article{mohoneyHighThroughputVectorSimilarity2023Proc.ACMManag.Data,
  title = {High-{{Throughput Vector Similarity Search}} in {{Knowledge Graphs}}},
  author = {Mohoney, Jason and Pacaci, Anil and Chowdhury, Shihabur Rahman and Mousavi, Ali and Ilyas, Ihab F. and Minhas, Umar Farooq and Pound, Jeffrey and Rekatsinas, Theodoros},
  year = {2023},
  month = jun,
  journal = {Proceedings of the ACM on Management of Data},
  volume = {1},
  number = {2},
  pages = {1--25},
  issn = {2836-6573},
  doi = {10.1145/3589777},
  urldate = {2024-09-29},
  abstract = {There is an increasing adoption of machine learning for encoding data into vectors to serve online recommendation and search use cases. As a result, recent data management systems propose augmenting query processing with online vector similarity search. In this work, we explore vector similarity search in the context of Knowledge Graphs (KGs). Motivated by the tasks of finding related KG queries and entities for past KG query workloads, we focus on hybrid vector similarity search (hybrid queries for short) where part of the query corresponds to vector similarity search and part of the query corresponds to predicates over relational attributes associated with the underlying data vectors. For example, given past KG queries for a song entity, we want to construct new queries for new song entities whose vector representations are close to the vector representation of the entity in the past KG query. But entities in a KG also have non-vector attributes such as a song associated with an artist, a genre, and a release date. Therefore, suggested entities must also satisfy query predicates over non-vector attributes beyond a vector-based similarity predicate. While these tasks are central to KGs, our contributions are generally applicable to hybrid queries. In contrast to prior works that optimize online queries, we focus on enabling efficient batch processing of past hybrid query workloads. We present our system, HQI, for high-throughput batch processing of hybrid queries. We introduce a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describe a multi-query optimization technique to reduce the overhead of vector similarity computations. We evaluate our methods on industrial workloads and demonstrate that HQI yields a 31{\texttimes} improvement in throughput for finding related KG queries compared to existing hybrid query processing approaches.},
  langid = {english},
  keywords = {Jab/PMD,Unread},
  annotation = {TLDR: This work focuses on enabling efficient batch processing of past hybrid query workloads and introduces a workload-aware vector data partitioning scheme to tailor the vector index layout to the given workload and describes a multi-query optimization technique to reduce the overhead of vector similarity computations.},
  file = {/Users/janwardenga/Zotero/storage/3NQ6VR3J/PMD_2023_High-Throughput Vector Similarity Search in Knowledge Graphs.pdf}
}

@article{mollardImprovingRetrievalAugmented2024,
  title = {Improving {{Retrieval Augmented Generation}}},
  author = {Mollard, Eric and Patel, Anant and Pham, Lynn and Trachtenberg, Reuben},
  year = {2024},
  langid = {english},
  keywords = {,No DOI found,Not citable but relevant,Unread},
  file = {/Users/janwardenga/Zotero/storage/Z6ITYLNA/Mollard et al. - Improving Retrieval Augmented Generation.pdf}
}

@misc{mondalEndtoEndNLPKnowledge2021,
  title = {End-to-{{End NLP Knowledge Graph Construction}}},
  author = {Mondal, Ishani and Hou, Yufang and Jochim, Charles},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01167},
  eprint = {2106.01167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.01167},
  urldate = {2024-11-28},
  abstract = {This paper studies the end-to-end construction of an NLP Knowledge Graph (KG) from scientific papers. We focus on extracting four types of relations: evaluatedOn between tasks and datasets, evaluatedBy between tasks and evaluation metrics, as well as coreferent and related relations between the same type of entities. For instance, F1-score is coreferent with F-measure. We introduce novel methods for each of these relation types and apply our final framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a large-scale KG, which can facilitate automatically constructing scientific leaderboards for the NLP community. The results of our experiments indicate that the resulting KG contains high-quality information.},
  archiveprefix = {arXiv},
  keywords = {,Computer Science - Computation and Language,Jab/Pre,Unread},
  annotation = {31 citations (Semantic Scholar/arXiv) [2025-02-15]\\
31 citations (Semantic Scholar/arXiv) [2024-11-28]},
  file = {/Users/janwardenga/Zotero/storage/NMF4L6WN/Pre_2021_End-to-End NLP Knowledge Graph Construction.pdf;/Users/janwardenga/Zotero/storage/56DAVK4W/2106.html}
}

@misc{mountantonakisUsingMultipleRDF2023,
  title = {Using {{Multiple RDF Knowledge Graphs}} for {{Enriching ChatGPT Responses}}},
  author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05774},
  eprint = {2304.05774},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2304.05774},
  urldate = {2024-09-26},
  abstract = {There is a recent trend for using the novel Artificial Intelligence ChatGPT chatbox, which provides detailed responses and articulate answers across many domains of knowledge. However, in many cases it returns plausible-sounding but incorrect or inaccurate responses, whereas it does not provide evidence. Therefore, any user has to further search for checking the accuracy of the answer or/and for finding more information about the entities of the response. At the same time there is a high proliferation of RDF Knowledge Graphs (KGs) over any real domain, that offer high quality structured data. For enabling the combination of ChatGPT and RDF KGs, we present a research prototype, called GPT{$\bullet$}LODS, which is able to enrich any ChatGPT response with more information from hundreds of RDF KGs. In particular, it identifies and annotates each entity of the response with statistics and hyperlinks to LODsyndesis KG (which contains integrated data from 400 RDF KGs and over 412 million entities). In this way, it is feasible to enrich the content of entities and to perform fact checking and validation for the facts of the response at real time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {12 citations (Semantic Scholar/arXiv) [2025-02-15]\\
7 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/5EFTU7QE/Pre_2023_Using Multiple RDF Knowledge Graphs for Enriching ChatGPT Responses.pdf}
}

@misc{nafarLearningVsRetrieval2024,
  title = {Learning vs {{Retrieval}}: {{The Role}} of {{In-Context Examples}} in {{Regression}} with {{LLMs}}},
  shorttitle = {Learning vs {{Retrieval}}},
  author = {Nafar, Aliakbar and Venable, Kristen Brent and Kordjamshidi, Parisa},
  year = {2024},
  month = sep,
  number = {arXiv:2409.04318},
  eprint = {2409.04318},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.04318},
  urldate = {2024-09-29},
  abstract = {Generative Large Language Models (LLMs) are capable of being in-context learners. However, the underlying mechanism of in-context learning (ICL) is still a major research question, and experimental research results about how models exploit ICL are not always consistent. In this work, we propose a framework for evaluating in-context learning mechanisms, which we claim are a combination of retrieving internal knowledge and learning from in-context examples by focusing on regression tasks. First, we show that LLMs can perform regression on real-world datasets and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples. We argue that this process lies on a spectrum between these two extremes. We provide an in-depth analysis of the degrees to which these mechanisms are triggered depending on various factors, such as prior knowledge about the tasks and the type and richness of the information provided by the in-context examples. We employ three LLMs and utilize multiple datasets to corroborate the robustness of our findings. Our results shed light on how to engineer prompts to leverage meta-learning from in-context examples and foster knowledge retrieval depending on the problem being addressed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/AW7UBZKJ/Pre_2024_Learning vs Retrieval.pdf}
}

@article{najaUsingKnowledgeGraphs2022IEEEAccess,
  title = {Using {{Knowledge Graphs}} to {{Unlock Practical Collection}}, {{Integration}}, and {{Audit}} of {{AI Accountability Information}}},
  author = {Naja, Iman and Markovic, Milan and Edwards, Peter and Pang, Wei and Cottrill, Caitlin and Williams, Rebecca},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {74383--74411},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3188967},
  urldate = {2024-09-29},
  abstract = {To enhance trustworthiness of AI systems, a number of solutions have been proposed to document how such systems are built and used. A key facet of realizing trust in AI is how to make such systems accountable - a challenging task, not least due to the lack of an agreed definition of accountability and differing perspectives on what information should be recorded and how it should be used (e.g., to inform audit). Information originates across the life cycle stages of an AI system and from a variety of sources (individuals, organizations, systems), raising numerous challenges around collection, management, and audit. In our previous work, we argued that semantic Knowledge Graphs (KGs) are ideally suited to address those challenges and we presented an approach utilizing KGs to aid in the tasks of modelling, recording, viewing, and auditing accountability information related to the design stage of AI system development. Moreover, as KGs store data in a structured format understandable by both humans and machines, we argued that this approach provides new opportunities for building intelligent applications that facilitate and automate such tasks. In this paper, we expand our earlier work by reporting additional detailed requirements for knowledge representation and capture in the context of AI accountability; these extend the scope of our work beyond the design stage, to also include system implementation. Furthermore, we present the RAInS ontology which has been extended to satisfy these requirements. We evaluate our approach against three popular baseline frameworks, namely, Datasheets, Model Cards, and FactSheets, by comparing the range of information that can be captured by our KGs against these three frameworks. We demonstrate that our approach subsumes and extends the capabilities of the baseline frameworks and discuss how KGs can be used to integrate and enhance accountability information collection processes.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  keywords = {Jab/A,Unread},
  annotation = {11 citations (Semantic Scholar/DOI) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/LJLUPWJD/A_2022_Using Knowledge Graphs to Unlock Practical Collection, Integration, and Audit of AI Accountability Information.pdf}
}

@misc{nanEvaluatingIntegrationReasoning2023,
  title = {On {{Evaluating}} the {{Integration}} of {{Reasoning}} and {{Action}} in {{LLM Agents}} with {{Database Question Answering}}},
  author = {Nan, Linyong and Zhang, Ellen and Zou, Weijin and Zhao, Yilun and Zhou, Wenfei and Cohan, Arman},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09721},
  eprint = {2311.09721},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2311.09721},
  urldate = {2024-09-29},
  abstract = {This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/F9T6RLIT/Pre_2023_On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering.pdf}
}

@misc{naveedComprehensiveOverviewLarge2024,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  year = {2024},
  month = oct,
  number = {arXiv:2307.06435},
  eprint = {2307.06435},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.06435},
  urldate = {2025-03-03},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Unread},
  annotation = {370 citations (Semantic Scholar/DOI) [2025-03-03]\\
TLDR: A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.},
  file = {/Users/janwardenga/Zotero/storage/54X57C7Y/Naveed et al. - 2024 - A Comprehensive Overview of Large Language Models.pdf;/Users/janwardenga/Zotero/storage/F28754WS/2307.html}
}

@misc{neelakantanCompositionalVectorSpace2015,
  title = {Compositional {{Vector Space Models}} for {{Knowledge Base Completion}}},
  author = {Neelakantan, Arvind and Roth, Benjamin and McCallum, Andrew},
  year = {2015},
  month = may,
  number = {arXiv:1504.06662},
  eprint = {1504.06662},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1504.06662},
  urldate = {2024-09-29},
  abstract = {Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z) {$\rightarrow$} containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11\%, and a method leveraging pre-trained embeddings by 7\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {275 citations (Semantic Scholar/arXiv) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/M74EWFW6/Pre_2015_Compositional Vector Space Models for Knowledge Base Completion.pdf}
}

@article{nguyenKnowledgeGraphFusion2020InformationFusion,
  title = {Knowledge Graph Fusion for Smart Systems: {{A Survey}}},
  shorttitle = {Knowledge Graph Fusion for Smart Systems},
  author = {Nguyen, Hoang Long and Vu, Dang Thinh and Jung, Jason J.},
  year = {2020},
  month = sep,
  journal = {Information Fusion},
  volume = {61},
  pages = {56--70},
  issn = {15662535},
  doi = {10.1016/j.inffus.2020.03.014},
  urldate = {2024-09-29},
  abstract = {The emergence of various disruptive technologies such as big data, Internet of Things, and artificial intelligence have instigated our society to generate enormous volumes of data. The effective, efficient, and transparent capture and fusion of knowledge from a massive amount data is becoming an increasingly popular and crucial topic. In this study, we aim to provide a broad, complete, and systematic overview of the definitions and challenges of the knowledge graph fusion, which represents a holistic approach for integrating, enhancing, and unifying knowledge graphs. Further, advanced techniques for handling knowledge graph fusion along with the pragmatic smart systems leveraging it are discussed as a part of multiple perspectives. We believe that this survey study can be used as a potential reference for system practitioners and researchers in surpassing current obstacles as well as shaping their future direction.},
  langid = {english},
  keywords = {Jab/IF,Unread},
  annotation = {69 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: A broad, complete, and systematic overview of the definitions and challenges of the knowledge graph fusion, which represents a holistic approach for integrating, enhancing, and unifying knowledge graphs is provided.},
  file = {/Users/janwardenga/Zotero/storage/5T4H7T5D/IF_2020_Knowledge graph fusion for smart systems.pdf}
}

@article{nickelHolographicEmbeddingsKnowledge2016AAAI,
  title = {Holographic {{Embeddings}} of {{Knowledge Graphs}}},
  author = {Nickel, Maximilian and Rosasco, Lorenzo and Poggio, Tomaso},
  year = {2016},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v30i1.10314},
  urldate = {2024-09-29},
  abstract = {Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HOLE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator, HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. Experimentally, we show that holographic embeddings are able to outperform state-ofthe-art methods for link prediction on knowledge graphs and relational learning benchmark datasets.},
  langid = {english},
  keywords = {Unread},
  annotation = {1158 citations (Semantic Scholar/DOI) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/WHG4PXF2/PACAI_2016_Holographic Embeddings of Knowledge Graphs.pdf}
}

@misc{niWhenLLMsNeed2024,
  title = {When {{Do LLMs Need Retrieval Augmentation}}? {{Mitigating LLMs}}' {{Overconfidence Helps Retrieval Augmentation}}},
  shorttitle = {When {{Do LLMs Need Retrieval Augmentation}}?},
  author = {Ni, Shiyu and Bi, Keping and Guo, Jiafeng and Cheng, Xueqi},
  year = {2024},
  month = jun,
  number = {arXiv:2402.11457},
  eprint = {2402.11457},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.11457},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls. The code can be found at https://github.com/ ShiyuNee/When-to-Retrieve.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {23 citations (Semantic Scholar/arXiv) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/JPBDUBWI/Pre_2024_When Do LLMs Need Retrieval Augmentation.pdf}
}

@article{noyIndustryscaleKnowledgeGraphs2019Queue,
  title = {Industry-Scale {{Knowledge Graphs}}: {{Lessons}} and {{Challenges}}: {{Five}} Diverse Technology Companies Show How It's Done},
  shorttitle = {Industry-Scale {{Knowledge Graphs}}},
  author = {Noy, Natasha and Gao, Yuqing and Jain, Anshu and Narayanan, Anant and Patterson, Alan and Taylor, Jamie},
  year = {2019},
  month = apr,
  journal = {Queue},
  volume = {17},
  number = {2},
  pages = {48--75},
  issn = {1542-7730, 1542-7749},
  doi = {10.1145/3329781.3332266},
  urldate = {2025-02-15},
  abstract = {This article looks at the knowledge graphs of five diverse tech companies, comparing the similarities and differences in their respective experiences of building and using the graphs, and discussing the challenges that all knowledge-driven enterprises face today. The collection of knowledge graphs discussed here covers the breadth of applications, from search, to product descriptions, to social networks.},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/9ZZTR4EB/Noy et al. - 2019 - Industry-scale Knowledge Graphs Lessons and Challenges Five diverse technology companies show how.pdf}
}

@inproceedings{o-etal-2018-word,
  title = {Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-Based Graph},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {O, Dongsuk and Kwon, Sunjae and Kim, Kyungsun and Ko, Youngjoong},
  editor = {Bender, Emily M. and Derczynski, Leon and Isabelle, Pierre},
  year = {2018},
  month = aug,
  pages = {2704--2714},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA},
  url = {https://aclanthology.org/C18-1229},
  abstract = {Word sense disambiguation (WSD) is the task to determine the word sense according to its context. Many existing WSD studies have been using an external knowledge-based unsupervised approach because it has fewer word set constraints than supervised approaches requiring training data. In this paper, we propose a new WSD method to generate the context of an ambiguous word by using similarities between an ambiguous word and words in the input document. In addition, to leverage our WSD method, we further propose a new word similarity calculation method based on the semantic network structure of BabelNet. We evaluate the proposed methods on the SemEval-13 and SemEval-15 for English WSD dataset. Experimental results demonstrate that the proposed WSD method significantly improves the baseline WSD method. Furthermore, our WSD system outperforms the state-of-the-art WSD systems in the Semeval-13 dataset. Finally, it has higher performance than the state-of-the-art unsupervised knowledge-based WSD system in the average performance of both datasets.},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/A6VNL7SV/Pre__Word Sense Disambiguation Based on Word Similarity Calculation Using Word Vector Representation from a Knowledge-based Graph.pdf}
}

@article{okorieNigeriaAnophelesVector2011PLoSONE,
  title = {Nigeria {{Anopheles Vector Database}}: {{An Overview}} of 100 {{Years}}' {{Research}}},
  shorttitle = {Nigeria {{Anopheles Vector Database}}},
  author = {Okorie, Patricia Nkem and McKenzie, F. Ellis and Ademowo, Olusegun George and Bockarie, Moses and {Kelly-Hope}, Louise},
  editor = {Mores, Christopher N.},
  year = {2011},
  month = dec,
  journal = {PLoS ONE},
  volume = {6},
  number = {12},
  pages = {e28347},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0028347},
  urldate = {2024-09-29},
  abstract = {Anopheles mosquitoes are important vectors of malaria and lymphatic filariasis (LF), which are major public health diseases in Nigeria. Malaria is caused by infection with a protozoan parasite of the genus Plasmodium and LF by the parasitic worm Wuchereria bancrofti. Updating our knowledge of the Anopheles species is vital in planning and implementing evidence based vector control programs. To present a comprehensive report on the spatial distribution and composition of these vectors, all published data available were collated into a database. Details recorded for each source were the locality, latitude/longitude, time/period of study, species, abundance, sampling/collection methods, morphological and molecular species identification methods, insecticide resistance status, including evidence of the kdr allele, and P. falciparum sporozoite rate and W. bancrofti microfilaria prevalence. This collation resulted in a total of 110 publications, encompassing 484,747 Anopheles mosquitoes in 632 spatially unique descriptions at 142 georeferenced locations being identified across Nigeria from 1900 to 2010. Overall, the highest number of vector species reported included An. gambiae complex (65.2\%), An. funestus complex (17.3\%), An. gambiae s.s. (6.5\%). An. arabiensis (5.0\%) and An. funestus s.s. (2.5\%), with the molecular forms An. gambiae M and S identified at 120 locations. A variety of sampling/collection and species identification methods were used with an increase in molecular techniques in recent decades. Insecticide resistance to pyrethroids and organochlorines was found in the main Anopheles species across 45 locations. Presence of P. falciparum and W. bancrofti varied between species with the highest sporozoite rates found in An. gambiae s.s, An. funestus s.s. and An. moucheti, and the highest microfilaria prevalence in An. gambiae s.l., An. arabiensis, and An. gambiae s.s. This comprehensive georeferenced database provides an essential baseline on Anopheles vectors and will be an important resource for malaria and LF vector control programmes in Nigeria.},
  langid = {english},
  keywords = {Jab/PO,Unread},
  annotation = {66 citations (Semantic Scholar/DOI) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/R4N4CI8G/PO_2011_Nigeria Anopheles Vector Database.pdf}
}

@article{omranSHACLLearningKnowledge2020ISWCDemos/Industry,
  title = {Towards {{SHACL Learning}} from {{Knowledge Graphs}}},
  author = {Omran, Pouya Ghiasnezhad and Taylor, Kerry and Mendez, Sergio Rodriguez and Haller, Armin},
  year = {2020},
  abstract = {Knowledge Graphs (KGs) are typically large data-first knowledge bases with weak inference rules and weakly-constraining data schemes. The SHACL Shapes Constraint Language is a W3C recommendation for the expression of shapes as constraints on graph data. SHACL shapes serve to validate a KG and can give informative insight into the structure of data. Here, we introduce Inverse Open Path (IOP) rules, a logical formalism which acts as a building block for a restricted fragment of SHACL that can be used for schema-driven structural knowledge graph validation and completion. We define quality measures for IOP rules and propose a novel method to learn them, SHACLearner. SHACLearner adapts a state-of-the-art embedding-based open path rule learner (oprl) by modifying the efficient matrix-based evaluation module. We demonstrate SHACLearner performance on real-world massive KGs, YAGO2s (4M facts), DBpedia 3.8 (11M facts), and Wikidata (8M facts), finding that it can efficiently learn hundreds of high-quality rules.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/GZTC46L5/Omran et al. - Towards SHACL Learning from Knowledge Graphs.pdf}
}

@article{ooiNeurDBAIpoweredAutonomous2024Sci.ChinaInf.Sci.,
  title = {{{NeurDB}}: {{An AI-powered Autonomous Data System}}},
  shorttitle = {{{NeurDB}}},
  author = {Ooi, Beng Chin and Cai, Shaofeng and Chen, Gang and Shen, Yanyan and Tan, Kian-Lee and Wu, Yuncheng and Xiao, Xiaokui and Xing, Naili and Yue, Cong and Zeng, Lingze and Zhang, Meihui and Zhao, Zhanhao},
  year = {2024},
  month = oct,
  journal = {Science China Information Sciences},
  volume = {67},
  number = {10},
  eprint = {2405.03924},
  primaryclass = {cs},
  pages = {200901},
  issn = {1674-733X, 1869-1919},
  doi = {10.1007/s11432-024-4125-9},
  urldate = {2024-09-29},
  abstract = {In the wake of rapid advancements in artificial intelligence (AI), we stand on the brink of a transformative leap in data systems. The imminent fusion of AI and DB (AI{\texttimes}DB) promises a new generation of data systems, which will relieve the burden on end-users across all industry sectors by featuring AIenhanced functionalities, such as personalized and automated in-database AI-powered analytics, self-driving capabilities for improved system performance, etc. In this paper, we explore the evolution of data systems with a focus on deepening the fusion of AI and DB. We present NeurDB, an AI-powered autonomous data system designed to fully embrace AI design in each major system component and provide in-database AIpowered analytics. We outline the conceptual and architectural overview of NeurDB, discuss its design choices and key components, and report its current development and future plan.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Jab/SCIS,Unread},
  file = {/Users/janwardenga/Zotero/storage/8XKGNLCP/SCIS_2024_NeurDB.pdf}
}

@misc{panEndtoEndTableQuestion2022,
  title = {End-to-{{End Table Question Answering}} via {{Retrieval-Augmented Generation}}},
  author = {Pan, Feifei and Canim, Mustafa and Glass, Michael and Gliozzo, Alfio and Hendler, James},
  year = {2022},
  month = mar,
  number = {arXiv:2203.16714},
  eprint = {2203.16714},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2203.16714},
  urldate = {2024-09-29},
  abstract = {Most existing end-to-end Table Question Answering (Table QA) models consist of a twostage framework with a retriever to select relevant table candidates from a corpus and a reader to locate the correct answers from table candidates. Even though the accuracy of the reader models is significantly improved with the recent transformer-based approaches, the overall performance of such frameworks still suffers from the poor accuracy of using traditional information retrieval techniques as retrievers. To alleviate this problem, we introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence model to generate answer tokens. Given any natural language question, T-RAG utilizes a unified pipeline to automatically search through a table corpus to directly locate the correct answer from table cell. We apply T-RAG on recent open-domain Table QA benchmarks and demonstrate that the finetuned T-RAG model is able to achieve stateof-the-art performance in both the end-to-end Table QA and the table retrieval tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {13 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/IGB6GZIP/Pre_2022_End-to-End Table Question Answering via Retrieval-Augmented Generation.pdf}
}

@misc{panSurveyVectorDatabase2023,
  title = {Survey of {{Vector Database Management Systems}}},
  author = {Pan, James Jie and Wang, Jianguo and Li, Guoliang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14021},
  eprint = {2310.14021},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.14021},
  urldate = {2024-09-29},
  abstract = {There are now over 20 commercial vector database management systems (VDBMSs), all produced within the past five years. But embedding-based retrieval has been studied for over ten years, and similarity search a staggering half century and more. Driving this shift from algorithms to systems are new data intensive applications, notably large language models, that demand vast stores of unstructured data coupled with reliable, secure, fast, and scalable query processing capability. A variety of new data management techniques now exist for addressing these needs, however there is no comprehensive survey to thoroughly review these techniques and systems. We start by identifying five main obstacles to vector data management, namely vagueness of semantic similarity, large size of vectors, high cost of similarity comparison, lack of natural partitioning that can be used for indexing, and difficulty of efficiently answering hybrid queries that require both attributes and vectors. Overcoming these obstacles has led to new approaches to query processing, storage and indexing, and query optimization and execution. For query processing, a variety of similarity scores and query types are now well understood; for storage and indexing, techniques include vector compression, namely quantization, and partitioning based on randomization, learning partitioning, and navigable partitioning; for query optimization and execution, we describe new operators for hybrid queries, as well as techniques for plan enumeration, plan selection, and hardware accelerated execution. These techniques lead to a variety of VDBMSs across a spectrum of design and runtime characteristics, including native systems specialized for vectors and extended systems that incorporate vector capabilities into existing systems. We then discuss benchmarks, and finally we outline research challenges and point the direction for future work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {21 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/4VCJ28NN/Pre_2023_Survey of Vector Database Management Systems.pdf}
}

@article{panUnifyingLargeLanguage2024IEEETrans.Knowl.DataEng.,
  title = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}: {{A Roadmap}}},
  shorttitle = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}},
  author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  year = {2024},
  month = jul,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {36},
  number = {7},
  eprint = {2306.08302},
  primaryclass = {cs},
  pages = {3580--3599},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2024.3352100},
  urldate = {2024-11-14},
  abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Unread},
  annotation = {567 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A forward-looking roadmap for the unification of LLMs and KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge.},
  file = {/Users/janwardenga/Zotero/storage/FMPEM95I/Pan et al. - 2024 - Unifying Large Language Models and Knowledge Graphs A Roadmap.pdf}
}

@article{papadakiBriefSurveyMethods2023Analytics,
  title = {A {{Brief Survey}} of {{Methods}} for {{Analytics}} over {{RDF Knowledge Graphs}}},
  author = {Papadaki, Maria-Evangelia and Tzitzikas, Yannis and Mountantonakis, Michalis},
  year = {2023},
  month = jan,
  journal = {Analytics},
  volume = {2},
  number = {1},
  pages = {55--74},
  issn = {2813-2203},
  doi = {10.3390/analytics2010004},
  urldate = {2024-09-26},
  abstract = {There are several Knowledge Graphs expressed in RDF (Resource Description Framework) that aggregate/integrate data from various sources for providing unified access services and enabling insightful analytics. We observe this trend in almost every domain of our life. However, the provision of effective, efficient, and user-friendly analytic services and systems is quite challenging. In this paper we survey the approaches, systems and tools that enable the formulation of analytic queries over KGs expressed in RDF. We identify the main challenges, we distinguish two main categories of analytic queries (domain specific and quality-related), and five kinds of approaches for analytics over RDF. Then, we describe in brief the works of each category and related aspects, like efficiency and visualization. We hope this collection to be useful for researchers and engineers for advancing the capabilities and user-friendliness of methods for analytics over knowledge graphs.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Jab/A,Unread},
  annotation = {7 citations (Semantic Scholar/DOI) [2025-02-15]\\
2 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper surveys the approaches, systems and tools that enable the formulation of analytic queries over KGs expressed in RDF, and distinguishes two main categories of analytic querying (domain specific and quality-related), and five kinds of approaches for analytics over RDF.},
  file = {/Users/janwardenga/Zotero/storage/HLD3W58W/A_2023_A Brief Survey of Methods for Analytics over RDF Knowledge Graphs.pdf}
}

@misc{papadakiRDFAnalyticsInteractiveAnalytics2023,
  title = {{{RDF-Analytics}}: {{Interactive Analytics}} over {{RDF Knowledge Graphs}}},
  shorttitle = {{{RDF-Analytics}}},
  author = {Papadaki, Maria-Evaggelia and Tzitzikas, Yannis},
  year = {2023},
  publisher = {OpenProceedings.org},
  doi = {10.48786/EDBT.2023.70},
  urldate = {2024-09-26},
  abstract = {The formulation of structured queries in knowledge graphs is a challenging task that presupposes familiarity with the syntax of the query language and the contents of the knowledge graph. To alleviate this difficulty in this paper we introduce RDF-ANALYTICS, a novel system that enables plain users to formulate analytic queries over complex, i.e. not necessarily star-schema based, RDF knowledge graphs. To come up with an intuitive interface, we leverage the familiarity of users with Faceted Search (FS) systems, i.e. we extend FS with actions that enable users to formulate analytic queries, too. Distinctive characteristics of the approach is the ability to include arbitrarily long paths in the analytic query (accompanied with count information), interactive formulation of HAVING restrictions, the support of both Faceted Search (i.e. the locating of the desired resources in a faceted search manner) and analytic queries, and the ability to formulate nested analytic queries. Finally, we present the results of a preliminary task-based evaluation with users, which are very promising.},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]\\
100 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/KZVU56Z7/Pre_2023_RDF-Analytics.pdf}
}

@misc{peffersDesignScienceResearch2020,
  title = {Design {{Science Research Process}}: {{A Model}} for {{Producing}} and {{Presenting Information Systems Research}}},
  shorttitle = {Design {{Science Research Process}}},
  author = {Peffers, Ken and Tuunanen, Tuure and Gengler, Charles E. and Rossi, Matti and Hui, Wendy and Virtanen, Ville and Bragge, Johanna},
  year = {2020},
  month = jun,
  number = {arXiv:2006.02763},
  eprint = {2006.02763},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.02763},
  urldate = {2024-12-26},
  abstract = {The authors design and demonstrate a process for carrying out design science (DS) research in information systems and demonstrate use of the process to conduct research in two case studies. Several IS researchers have pioneered the acceptance of DS research in IS, but in the last 15 years little DS research has been done within the discipline. The lack of a generally accepted process for DS research in IS may have contributed to this problem. We sought to design a design science research process (DSRP) model that would meet three objectives: it would be consistent with prior literature, it would provide a nominal process model for doing DS research, and it would provide a mental model for presenting and appreciating DS research in IS. The process includes six steps: problem identification and motivation, objectives for a solution, design and development, evaluation, and communication. We demonstrated the process by using it in this study and by presenting two case studies, one in IS planning to develop application ideas for mobile financial services and another in requirements engineering to specify feature requirements for a self service advertising design and sales system intended for wide audience end users. The process effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering,Unread},
  annotation = {228 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/7LCVZA5A/Peffers et al. - 2020 - Design Science Research Process A Model for Producing and Presenting Information Systems Research.pdf;/Users/janwardenga/Zotero/storage/YVGN5KGW/2006.html}
}

@article{peffersDesignScienceResearchSyst.Res.,
  title = {The Design Science Research Process: {{A}} Model for Producing and Presenting Information Systems Research},
  author = {Peffers, Ken and Tuunanen, Tuure and Gengler, Charles E and Rossi, Matti and Hui, Wendy},
  journal = {SYSTEMS RESEARCH},
  doi = {https://arxiv.org/abs/2006.02763},
  abstract = {The authors design and demonstrate a process for carrying out design science (DS) research in information systems and demonstrate use of the process to conduct research in two case studies. Several IS researchers have pioneered the acceptance of DS research in IS, but in the last 15 years little DS research has been done within the discipline. The lack of a generally accepted process for DS research in IS may have contributed to this problem. We sought to design a design science research process (DSRP) model that would meet three objectives: it would be consistent with prior literature, it would provide a nominal process model for doing DS research, and it would provide a mental model for presenting and appreciating DS research in IS. The process includes six steps: problem identification and motivation, objectives for a solution, design and development, evaluation, and communication. We demonstrated the process by using it in this study and by presenting two case studies, one in IS planning to develop application ideas for mobile financial services and another in requirements engineering to specify feature requirements for a self service advertising design and sales system intended for wide audience end users. The process effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline.},
  langid = {english},
  keywords = {,No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/84LE5K35/Peffers et al. - THE DESIGN SCIENCE RESEARCH PROCESS A MODEL FOR PRODUCING AND PRESENTING INFORMATION SYSTEMS RESEAR.pdf}
}

@misc{pengGraphRetrievalAugmentedGeneration2024,
  title = {Graph {{Retrieval-Augmented Generation}}: {{A Survey}}},
  shorttitle = {Graph {{Retrieval-Augmented Generation}}},
  author = {Peng, Boci and Zhu, Yun and Liu, Yongchao and Bo, Xiaohe and Shi, Haizhou and Hong, Chuntao and Zhang, Yan and Tang, Siliang},
  year = {2024},
  month = sep,
  number = {arXiv:2408.08921},
  eprint = {2408.08921},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.08921},
  urldate = {2024-09-29},
  abstract = {Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at {\textbackslash}url\{https://github.com/pengboci/GraphRAG-Survey\}.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Jab/Pre,Unread},
  annotation = {20 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/W54WRD5I/Pre_2024_Graph Retrieval-Augmented Generation.pdf}
}

@article{pengKnowledgeGraphsOpportunities2023ArtifIntellRev,
  title = {Knowledge {{Graphs}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Knowledge {{Graphs}}},
  author = {Peng, Ciyuan and Xia, Feng and Naseriparsa, Mehdi and Osborne, Francesco},
  year = {2023},
  month = nov,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {11},
  pages = {13071--13102},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-023-10465-9},
  urldate = {2024-09-29},
  abstract = {With the explosive growth of artificial intelligence (AI) and big data, it has become vitally important to organize and represent the enormous volume of knowledge appropriately. As graph data, knowledge graphs accumulate and convey knowledge of the real world. It has been well-recognized that knowledge graphs effectively represent complex information; hence, they rapidly gain the attention of academia and industry in recent years. Thus to develop a deeper understanding of knowledge graphs, this paper presents a systematic overview of this field. Specifically, we focus on the opportunities and challenges of knowledge graphs. We first review the opportunities of knowledge graphs in terms of two aspects: (1) AI systems built upon knowledge graphs; (2) potential application fields of knowledge graphs. Then, we thoroughly discuss severe technical challenges in this field, such as knowledge graph embeddings, knowledge acquisition, knowledge graph completion, knowledge fusion, and knowledge reasoning. We expect that this survey will shed new light on future research and the development of knowledge graphs.},
  langid = {english},
  keywords = {Jab/AIR,Unread},
  annotation = {176 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper thoroughly discusses severe technical challenges in this field, such as knowledge graph embeddings, knowledge acquisition, knowledge graph completion, knowledge fusion, and knowledge reasoning, and expects that this survey will shed new light on future research and the development of knowledge graphs.},
  file = {/Users/janwardenga/Zotero/storage/QWX7N4JR/AIR_2023_Knowledge Graphs.pdf}
}

@misc{perevalovQALD9plusMultilingualDataset2022,
  title = {{{QALD-9-plus}}: {{A Multilingual Dataset}} for {{Question Answering}} over {{DBpedia}} and {{Wikidata Translated}} by {{Native Speakers}}},
  shorttitle = {{{QALD-9-plus}}},
  author = {Perevalov, Aleksandr and Diefenbach, Dennis and Usbeck, Ricardo and Both, Andreas},
  year = {2022},
  month = feb,
  number = {arXiv:2202.00120},
  eprint = {2202.00120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.00120},
  urldate = {2025-02-19},
  abstract = {The ability to have the same experience for different user groups (i.e., accessibility) is one of the most important characteristics of Web-based systems. The same is true for Knowledge Graph Question Answering (KGQA) systems that provide the access to Semantic Web data via natural language interface. While following our research agenda on the multilingual aspect of accessibility of KGQA systems, we identified several ongoing challenges. One of them is the lack of multilingual KGQA benchmarks. In this work, we extend one of the most popular KGQA benchmarks -- QALD-9 by introducing highquality questions' translations to 8 languages provided by native speakers, and transferring the SPARQL queries of QALD-9 from DBpedia to Wikidata, s.t., the usability and relevance of the dataset is strongly increased. Five of the languages --Armenian, Ukrainian, Lithuanian, Bashkir and Belarusian -- to our best knowledge were never considered in KGQA research community before. The latter two of the languages are considered as ``endangered'' by UNESCO. We call the extended dataset QALD-9-plus and made it available online1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Jab/Pre,Unread},
  annotation = {44 citations (Semantic Scholar/arXiv) [2025-02-19]},
  file = {/Users/janwardenga/Zotero/storage/SG68VZ4I/Pre_2022_QALD-9-plus.pdf}
}

@incollection{perevalovUnderstandingSPARQLQueries2025TheSemanticWeb-ISWC2024,
  title = {Understanding {{SPARQL Queries}}: {{Are We Already There}}? {{Multilingual Natural Language Generation Based}} on {{SPARQL Queries}} and {{Large Language Models}}},
  shorttitle = {Understanding {{SPARQL Queries}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2024},
  author = {Perevalov, Aleksandr and Gashkov, Aleksandr and Eltsova, Maria and Both, Andreas},
  editor = {Demartini, Gianluca and Hose, Katja and Acosta, Maribel and Palmonari, Matteo and Cheng, Gong and {Skaf-Molli}, Hala and Ferranti, Nicolas and Hern{\'a}ndez, Daniel and Hogan, Aidan},
  year = {2025},
  volume = {15232},
  pages = {173--191},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-77850-6_10},
  urldate = {2025-02-20},
  isbn = {978-3-031-77849-0 978-3-031-77850-6},
  langid = {english},
  keywords = {Unread}
}

@article{pirkVoodooVectorAlgebra2016Proc.VLDBEndow.,
  title = {Voodoo - a Vector Algebra for Portable Database Performance on Modern Hardware},
  author = {Pirk, Holger and Moll, Oscar and Zaharia, Matei and Madden, Sam},
  year = {2016},
  month = oct,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {14},
  pages = {1707--1718},
  issn = {2150-8097},
  doi = {10.14778/3007328.3007336},
  urldate = {2024-09-29},
  abstract = {In-memory databases require careful tuning and many engineering tricks to achieve good performance. Such database performance engineering is hard: a plethora of data and hardware-dependent optimization techniques form a design space that is difficult to navigate for a skilled engineer -- even more so for a query compiler. To facilitate performanceoriented design exploration and query plan compilation, we present Voodoo, a declarative intermediate algebra that abstracts the detailed architectural properties of the hardware, such as multi- or many-core architectures, caches and SIMD registers, without losing the ability to generate highly tuned code. Because it consists of a collection of declarative, vector-oriented operations, Voodoo is easier to reason about and tune than low-level C and related hardware-focused extensions (Intrinsics, OpenCL, CUDA, etc.). This enables our Voodoo compiler to produce (OpenCL) code that rivals and even outperforms the fastest state-of-the-art in memory databases for both GPUs and CPUs. In addition, Voodoo makes it possible to express techniques as diverse as cacheconscious processing, predication and vectorization (again on both GPUs and CPUs) with just a few lines of code. Central to our approach is a novel idea we termed control vectors, which allows a code generating frontend to expose parallelism to the Voodoo compiler in a abstract manner, enabling portable performance across hardware platforms.},
  langid = {english},
  keywords = {Unread},
  annotation = {115 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CGWISEYM/PVE_2016_Voodoo - a vector algebra for portable database performance on modern hardware.pdf}
}

@book{porterHandbookKnowledgeRepresentation2008,
  title = {Handbook of Knowledge Representation},
  author = {Porter, Bruce and Lifschitz, Vladimir and Van Harmelen, Frank},
  year = {2008},
  series = {Foundations of Artificial Intelligence},
  edition = {1st ed},
  publisher = {Elsevier},
  address = {Amsterdam Boston},
  abstract = {Knowledge representation, which lies at the core of artificial intelligence, is concerned with encoding knowledge on computers to enable systems to reason automatically. The aims are to help readers make their computer smarter, handle qualitative and uncertain information, and improve computational tractability},
  isbn = {978-0-444-52211-5},
  langid = {english},
  lccn = {006.332},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/4HJNHAY3/Pre_2008_Handbook of knowledge representation.pdf}
}

@incollection{potockiOntoQuadNativeHighSpeed2013KnowledgeEngineeringandtheSemanticWeb,
  title = {{{OntoQuad}}: {{Native High-Speed RDF DBMS}} for {{Semantic Web}}},
  shorttitle = {{{OntoQuad}}},
  booktitle = {Knowledge {{Engineering}} and the {{Semantic Web}}},
  author = {Potocki, Alexander and Polukhin, Anton and Drobyazko, Grigory and Hladky, Daniel and Klintsov, Victor and Unbehauen, J{\"o}rg},
  editor = {Klinov, Pavel and Mouromtsev, Dmitry},
  year = {2013},
  volume = {394},
  pages = {117--131},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-41360-5_10},
  urldate = {2024-09-29},
  abstract = {In the last years native RDF stores made enormous progress in closing the performance gap compared to RDBMS. This albeit smaller gap, however, still prevents adoption of RDF stores in scenarios with high requirements on responsiveness. We try to bridge the gap and present a native RDF store ``OntoQuad'' and its fundamental design principles. Basing on previous researches, we develop a vector database schema for quadruples, its realization on index data structures, and ways to efficiently implement the joining of two and more data sets simultaneously. We also offer approaches to optimizing the SPARQL query execution plan which is based on its heuristic transformations. The query performance efficiency is checked and proved on BSBM tests. The study results can be taken into consideration during the development of RDF DBMS's suitable for storing large volumes of Semantic Web data, as well as for the creation of large-scale repositories of semantic data.},
  isbn = {978-3-642-41359-9 978-3-642-41360-5},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/JUU82UFV/Book_2013_OntoQuad.pdf}
}

@inproceedings{prakashIntegratingLLMsDatabase2024Proc.3rdInt.WorkshopDataSyst.Educ.Bridg.Educ.Pract.Educ.Res.,
  title = {Integrating {{LLMs}} into {{Database Systems Education}}},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Data Systems Education}}: {{Bridging}} Education Practice with Education Research},
  author = {Prakash, Kishore and Rao, Shashwat and Hamza, Rayan and Lukich, Jack and Chaudhari, Vatsal and Nandi, Arnab},
  year = {2024},
  month = jun,
  pages = {33--39},
  publisher = {ACM},
  address = {Santiago AA Chile},
  doi = {10.1145/3663649.3664371},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have sparked a drastic improvement in the ways computers can understand, process, and generate language. As LLM-based offerings become mainstream, we explore the incorporation of such LLMs into introductory or undergraduate database systems education. Students and instructors are both faced with the calculator dilemma: while the use of LLM-based tools may ``solve'' tasks such as assignments and exams, do they impede or accelerate the learning itself? We review deficiencies of using existing off-the-shelf tools for learning, and further articulate the differentiated needs of database systems students as opposed to trained data practitioners. Building on our exploration, we outline a vision that integrates LLMs into database education in a principled manner, keeping pedagogical best practices in mind. If implemented correctly, we posit that LLMs can drastically amplify the impact of existing instruction, minimizing costs and barriers towards learning database systems fundamentals.},
  isbn = {9798400706783},
  langid = {english},
  keywords = {Jab/SICMD,Unread},
  annotation = {5 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/ARQ8AT2N/SICMD_2024_Integrating LLMs into Database Systems Education.pdf}
}

@article{pramanikUniqornUnifiedQuestion2024JournalofWebSemantics,
  title = {Uniqorn: {{Unified}} Question Answering over {{RDF}} Knowledge Graphs and Natural Language Text},
  shorttitle = {Uniqorn},
  author = {Pramanik, Soumajit and Alabi, Jesujoba and Roy, Rishiraj Saha and Weikum, Gerhard},
  year = {2024},
  month = dec,
  journal = {Journal of Web Semantics},
  volume = {83},
  pages = {100833},
  issn = {15708268},
  doi = {10.1016/j.websem.2024.100833},
  urldate = {2024-09-26},
  abstract = {Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving questionrelevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for heterogeneous QA -- in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.},
  langid = {english},
  keywords = {Jab/JWS,Unread},
  annotation = {31 citations (Semantic Scholar/DOI) [2025-02-15]\\
84 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/G554XF8X/JWS_2024_Uniqorn.pdf}
}

@inproceedings{predaActiveKnowledgeDynamically2010Proc.2010ACMSIGMODInt.Conf.Manag.Data,
  title = {Active Knowledge: Dynamically Enriching {{RDF}} Knowledge Bases by Web Services},
  shorttitle = {Active Knowledge},
  booktitle = {Proceedings of the 2010 {{ACM SIGMOD International Conference}} on {{Management}} of Data},
  author = {Preda, Nicoleta and Kasneci, Gjergji and Suchanek, Fabian M. and Neumann, Thomas and Yuan, Wenjun and Weikum, Gerhard},
  year = {2010},
  month = jun,
  pages = {399--410},
  publisher = {ACM},
  address = {Indianapolis Indiana USA},
  doi = {10.1145/1807167.1807212},
  urldate = {2024-09-29},
  abstract = {The proliferation of knowledge-sharing communities and the advances in information extraction have enabled the construction of large knowledge bases using the RDF data model to represent entities and relationships. However, as the Web and its latently embedded facts evolve, a knowledge base can never be complete and up-to-date. On the other hand, a rapidly increasing suite of Web services provide access to timely and high-quality information, but this is encapsulated by the service interface. We propose to leverage the information that could be dynamically obtained from Web services in order to enrich RDF knowledge bases on the fly whenever the knowledge base does not suffice to answer a user query.},
  isbn = {978-1-4503-0032-2},
  langid = {english},
  keywords = {,Jab/SICMD,Unread},
  annotation = {51 citations (Semantic Scholar/DOI) [2025-02-20]\\
51 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work develops a sound framework for appropriately generating queries to encapsulated Web services and efficient algorithms for query execution and result integration and demonstrates the viability and efficiency of the approach in experiments based on real-life data provided by popular Web services.},
  file = {/Users/janwardenga/Zotero/storage/3VLAHRJY/SICMD_2010_Active knowledge.pdf}
}

@inproceedings{priebeSearchEngineRDF2004Proc.15thInt.WorkshopDatabaseExpertSyst.Appl.2004,
  title = {A Search Engine for {{RDF}} Metadata},
  booktitle = {Proceedings. 15th {{International Workshop}} on {{Database}} and {{Expert Systems Applications}}, 2004.},
  author = {Priebe, T. and Schlager, C. and Pernul, G.},
  year = {2004},
  pages = {168--172},
  publisher = {IEEE},
  address = {Zaragoza, Spain},
  doi = {10.1109/DEXA.2004.1333468},
  urldate = {2024-09-29},
  abstract = {This paper presents a search engine for RDF metadata. Existing search facilities in this environment only support exact queries. We argue that a fuzzy approach as known from classic (full-text) Information Retrieval, providing fuzzy result sets ranked by relevance, is also desirable for metadata-based searches. Consequently, we develop an Information Retrieval model based on the similarity of RDF descriptions. The role of an ontology and implicit information that can be inferred from it will be discussed and the implementation within a knowledge portal prototype will be presented.},
  isbn = {978-0-7695-2195-4},
  langid = {english},
  keywords = {,Read},
  annotation = {28 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: It is argued that a fuzzy approach as known from classic (full-text) Information Retrieval, providing fuzzy result sets ranked by relevance, is also desirable for metadata-based searches and developed an information retrieval model based on the similarity of RDF descriptions.},
  file = {/Users/janwardenga/Zotero/storage/NFMQUVRI/PIWDESA_2004_A search engine for RDF metadata.pdf}
}

@misc{prockoGraphRetrievalAugmentedGeneration2024,
  title = {Graph {{Retrieval-Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Graph {{Retrieval-Augmented Generation}} for {{Large Language Models}}},
  author = {Procko, Tyler and Ochoa, Omar},
  year = {2024},
  doi = {10.2139/ssrn.4895062},
  urldate = {2024-09-30},
  abstract = {Large Language Models (LLMs) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without re-training, are 1) prompt engineering and 2) RetrievalAugmented Generation (RAG). RAG is a form of prompt engineering, insofar as relevant lexical snippets retrieved from RAG corpora are vectorized and aggregated with prompts. However, RAG documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pre-training a LLM on massive and general corpora is to engender a generally applicable model, RAG is not: it is a means of LLM optimization, and as such, RAG document selection must be precise, not general. For expert tasks, it is imperative that a RAG corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs (KGs) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a better understanding of this novel research area for future work.},
  langid = {english},
  keywords = {,Jab/Pre,Read},
  file = {/Users/janwardenga/Zotero/storage/K46ALMYN/Pre_2024_Graph Retrieval-Augmented Generation for Large Language Models.pdf}
}

@inproceedings{prudhommeauxShapeExpressionsRDF2014Proc.10thInt.Conf.SemanticSyst.,
  title = {Shape Expressions: An {{RDF}} Validation and Transformation Language},
  shorttitle = {Shape Expressions},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Semantic Systems}}},
  author = {Prud'hommeaux, Eric and Labra Gayo, Jose Emilio and Solbrig, Harold},
  year = {2014},
  month = sep,
  pages = {32--40},
  publisher = {ACM},
  address = {Leipzig Germany},
  doi = {10.1145/2660517.2660523},
  urldate = {2025-01-27},
  abstract = {RDF is a graph based data model which is widely used for semantic web and linked data applications. In this paper we describe a Shape Expression definition language which enables RDF validation through the declaration of constraints on the RDF model. Shape Expressions can be used to validate RDF data, communicate expected graph patterns for interfaces and generate user interface forms. In this paper we describe the syntax and the formal semantics of Shape Expressions using inference rules. Shape Expressions can be seen as domain specific language to define Shapes of RDF graphs based on regular expressions.},
  isbn = {978-1-4503-2927-9},
  langid = {english},
  keywords = {Unread},
  annotation = {159 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A Shape Expression definition language which enables RDF validation through the declaration of constraints on the RDF model and two extensions are implemented that leverage the predictability of the graph traversal and create ordered, closed content, XML/Json documents.},
  file = {/Users/janwardenga/Zotero/storage/E5Y8LFAH/Prud'hommeaux et al. - 2014 - Shape expressions an RDF validation and transformation language.pdf}
}

@article{pujionoIMPLEMENTINGRETRIEVALAUGMENTEDGENERATION2024jitk,
  title = {{{IMPLEMENTING RETRIEVAL-AUGMENTED GENERATION AND VECTOR DATABASES FOR CHATBOTS IN PUBLIC SERVICES AGENCIES CONTEXT}}},
  author = {Pujiono, Ibnu and Agtyaputra, Irfan Murtadho and Ruldeviyani, Yova},
  year = {2024},
  month = aug,
  journal = {JITK (Jurnal Ilmu Pengetahuan dan Teknologi Komputer)},
  volume = {10},
  number = {1},
  pages = {216--223},
  issn = {2527-4864, 2685-8223},
  doi = {10.33480/jitk.v10i1.5572},
  urldate = {2024-09-29},
  abstract = {Rapid developments in information technology, such as chatbots and generative artificial intelligence, have drastically lowered the cost of providing services to the society. This study aims to measure performance of developed chatbot using retrieval augmented generation and vector database. This research compares the performance of existing Large Language Modelling (LLM) in answering questions related to regulations concerning public service agencies.. Using a vector database, questions are assessed and answered by the LLM model, considering cosine similarity scores. The best-performing model, gpt-4, is selected for the deployment process which have average cosine similarity score 0,404. The use of LLM for chatbot creation at the prototyping stage can provide a good response to the question asked related to public service agencies with retrieval augmented generation (RAG) process through regulation-based document extraction.},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0},
  langid = {english},
  keywords = {Jab/J(IPTK,Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CCWJ8PJG/J(IPTK_2024_IMPLEMENTING RETRIEVAL-AUGMENTED GENERATION AND VECTOR DATABASES FOR CHATBOTS IN PUBLIC SERVICES AGENCIES CONTEXT.pdf}
}

@article{purohitKnowledgeGraphsEmpower2020IEEEInternetComput.,
  title = {Knowledge {{Graphs}} to {{Empower Humanity-Inspired AI Systems}}},
  author = {Purohit, Hemant and Shalin, Valerie L. and Sheth, Amit P.},
  year = {2020},
  month = jul,
  journal = {IEEE Internet Computing},
  volume = {24},
  number = {4},
  pages = {48--54},
  issn = {1089-7801, 1941-0131},
  doi = {10.1109/MIC.2020.3013683},
  urldate = {2024-09-29},
  abstract = {We present a theoretically-motivated design perspective, challenges, and applications of nextgeneration artificial intelligence (AI) systems. We envision systems with greater capabilities for meaningful human interaction, including socially-adaptive behavior that incorporates personalization and sensitivity to social context and intentionality. Personalized knowledge graphs (KGs) combining generic, common-sense and domain-specific knowledge with both socio-cultural values and norms and individual cognitive models provide a foundation for building humanity-inspired AI systems.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Jab/IC,Unread},
  annotation = {16 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/Z37VDFAV/IC_2020_Knowledge Graphs to Empower Humanity-Inspired AI Systems.pdf}
}

@article{qiEnhancingSPARQLQuery2024AppliedSciences,
  title = {Enhancing {{SPARQL Query Generation}} for {{Knowledge Base Question Answering Systems}} by {{Learning}} to {{Correct Triplets}}},
  author = {Qi, Jiexing and Su, Chang and Guo, Zhixin and Wu, Lyuwen and Shen, Zanwei and Fu, Luoyi and Wang, Xinbing and Zhou, Chenghu},
  year = {2024},
  month = feb,
  journal = {Applied Sciences},
  volume = {14},
  number = {4},
  pages = {1521},
  issn = {2076-3417},
  doi = {10.3390/app14041521},
  urldate = {2025-02-21},
  abstract = {Generating SPARQL queries from natural language questions is challenging in Knowledge Base Question Answering (KBQA) systems. The current state-of-the-art models heavily rely on fine-tuning pretrained models such as T5. However, these methods still encounter critical issues such as triple-flip errors (e.g., (subject, relation, object) is predicted as (object, relation, subject)). To address this limitation, we introduce TSET (Triplet Structure Enhanced T5), a model with a novel pretraining stage positioned between the initial T5 pretraining and the fine-tuning for the Text-to-SPARQL task. In this intermediary stage, we introduce a new objective called Triplet Structure Correction (TSC) to train the model on a SPARQL corpus derived from Wikidata. This objective aims to deepen the model's understanding of the order of triplets. After this specialized pretraining, the model undergoes fine-tuning for SPARQL query generation, augmenting its query-generation capabilities. We also propose a method named ``semantic transformation'' to fortify the model's grasp of SPARQL syntax and semantics without compromising the pre-trained weights of T5. Experimental results demonstrate that our proposed TSET outperforms existing methods on three well-established KBQA datasets: LC-QuAD 2.0, QALD-9 plus, and QALD-10, establishing a new state-of-the-art performance (95.0\% F1 and 93.1\% QM on LC-QuAD 2.0, 75.85\% F1 and 61.76\% QM on QALD-9 plus, 51.37\% F1 and 40.05\% QM on QALD-10).},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-21]\\
TLDR: This work introduces TSET (Triplet Structure Enhanced T5), a model with a novel pretraining stage positioned between the initial T5 pretraining and the fine-tuning for the Text-to-SPARQL task, and proposes a method named ``semantic transformation'' to fortify the model's grasp of SPARQL syntax and semantics without compromising the pre-trained weights of T5.},
  file = {/Users/janwardenga/Zotero/storage/UY98QDMB/Qi et al. - 2024 - Enhancing SPARQL Query Generation for Knowledge Base Question Answering Systems by Learning to Corre.pdf}
}

@article{rabbaniExtractionValidatingShapes2023Proc.VLDBEndow.,
  title = {Extraction of {{Validating Shapes}} from {{Very Large Knowledge Graphs}}},
  author = {Rabbani, Kashif and Lissandrini, Matteo and Hose, Katja},
  year = {2023},
  month = jan,
  journal = {Proceedings of the VLDB Endowment},
  volume = {16},
  number = {5},
  pages = {1023--1032},
  issn = {2150-8097},
  doi = {10.14778/3579075.3579078},
  urldate = {2025-01-23},
  abstract = {Knowledge Graphs (KGs) represent heterogeneous domain knowledge on the Web and within organizations. There exist shapes constraint languages to define validating shapes to ensure the quality of the data in KGs. Existing techniques to extract validating shapes often fail to extract complete shapes, are not scalable, and are prone to produce spurious shapes. To address these shortcomings, we propose the Quality Shapes Extraction (QSE) approach to extract validating shapes in very large graphs, for which we devise both an exact and an approximate solution. QSE provides information about the reliability of shape constraints by computing their confidence and support within a KG and in doing so allows to identify shapes that are most informative and less likely to be affected by incomplete or incorrect data. To the best of our knowledge, QSE is the first approach to extract a complete set of validating shapes from WikiData. Moreover, QSE provides a 12x reduction in extraction time compared to existing approaches, while managing to filter out up to 93\% of the invalid and spurious shapes, resulting in a reduction of up to 2 orders of magnitude in the number of constraints presented to the user, e.g., from 11,916 to 809 on DBpedia.},
  langid = {english},
  keywords = {Read},
  annotation = {1411 citations (Semantic Scholar/DOI) [2025-02-15]\\
2 citations (Semantic Scholar/DOI) [2025-01-23]\\
TLDR: The Quality Shapes Extraction approach to extract validating shapes in very large graphs provides information about the reliability of shape constraints by computing their confidence and support within a KG and in doing so allows to identify shapes that are most informative and less likely to be affected by incomplete or incorrect data.},
  file = {/Users/janwardenga/Zotero/storage/R27NC2G7/Rabbani et al. - 2023 - Extraction of Validating Shapes from Very Large Knowledge Graphs.pdf}
}

@misc{rabbaniOptimizingSPARQLQueries2021,
  title = {Optimizing {{SPARQL Queries}} Using {{Shape Statistics}}},
  author = {Rabbani, Kashif and Lissandrini, Matteo and Hose, Katja},
  year = {2021},
  publisher = {OpenProceedings.org},
  doi = {10.5441/002/EDBT.2021.59},
  urldate = {2025-02-02},
  abstract = {With the growing popularity of storing data in native RDF, we witness more and more diverse use cases with complex SPARQL queries. As a consequence, query optimization -- and in particular cardinality estimation and join ordering -- becomes even more crucial. Classical methods exploit global statistics covering the entire RDF graph as a whole, which naturally fails to correctly capture correlations that are very common in RDF datasets, which then leads to erroneous cardinality estimations and suboptimal query execution plans. The alternative of trying to capture correlations in a  ne-granular manner, on the other hand, results in very costly preprocessing steps to create these statistics. Hence, in this paper we propose shapes statistics, which extend the recent SHACL standard with statistic information to capture the correlation between classes and properties. Our extensive experiments on synthetic and real data show that shapes statistics can be generated and managed with only little overhead without disadvantages in query runtime while leading to noticeable improvements in cardinality estimation.},
  langid = {english},
  keywords = {Database Technology,Unread},
  file = {/Users/janwardenga/Zotero/storage/LIRRPP9N/Rabbani et al. - 2021 - Optimizing SPARQL Queries using Shape Statistics.pdf}
}

@inproceedings{rabbaniSHACLShExWild2022CompanionProc.WebConf.2022,
  title = {{{SHACL}} and {{ShEx}} in the {{Wild}}: {{A Community Survey}} on {{Validating Shapes Generation}} and {{Adoption}}},
  shorttitle = {{{SHACL}} and {{ShEx}} in the {{Wild}}},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2022},
  author = {Rabbani, Kashif and Lissandrini, Matteo and Hose, Katja},
  year = {2022},
  month = apr,
  pages = {260--263},
  publisher = {ACM},
  address = {Virtual Event, Lyon France},
  doi = {10.1145/3487553.3524253},
  urldate = {2025-01-27},
  abstract = {Knowledge Graphs (KGs) are widely used to represent heterogeneous domain knowledge on the Web and within organizations. Various methods exist to manage KGs and ensure the quality of their data. Among these, the Shapes Constraint Language (SHACL) and the Shapes Expression Language (ShEx) are the two state-ofthe-art languages to define validating shapes for KGs. Since the usage of these constraint languages has recently increased, new needs arose. One such need is to enable the efficient generation of these shapes. Yet, since these languages are relatively new, we witness a lack of understanding of how they are effectively employed for existing KGs. Therefore, in this work, we answer How validating shapes are being generated and adopted? Our contribution is threefold. First, we conducted a community survey to analyze the needs of users (both from industry and academia) generating validating shapes. Then, we cross-referenced our results with an extensive survey of the existing tools and their features. Finally, we investigated how existing automatic shape extraction approaches work in practice on real, large KGs. Our analysis shows the need for developing semi-automatic methods that can help users generate shapes from large KGs.},
  isbn = {978-1-4503-9130-6},
  langid = {english},
  keywords = {Jab/WWC,Unread},
  annotation = {20 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A community survey is conducted to analyze the needs of users generating validating shapes and shows the need for developing semi-automatic methods that can help users generate shapes from large KGs.},
  file = {/Users/janwardenga/Zotero/storage/7JNLFV7S/WWC_2022_SHACL and ShEx in the Wild.pdf}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/QMIAFTPE/Radford et al. - Language Models are Unsupervised Multitask Learners.pdf}
}

@article{rahmSurveyApproachesAutomatic2001VLDBJ.,
  title = {A Survey of Approaches to Automatic Schema Matching},
  author = {Rahm, Erhard and Bernstein, Philip A.},
  year = {2001},
  month = dec,
  journal = {The VLDB Journal},
  volume = {10},
  number = {4},
  pages = {334--350},
  issn = {10668888},
  doi = {10.1007/s007780100057},
  urldate = {2025-02-22},
  abstract = {Schema matching is a basic problem in many database application domains, such as data integration, Ebusiness, data warehousing, and semanticquery processing.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {,Read},
  annotation = {3859 citations (Semantic Scholar/DOI) [2025-02-22]\\
TLDR: A taxonomy is presented that distinguishes between schema-level and instance-level, element- level and structure- level, and language-based and constraint-based matchers and is intended to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.},
  file = {/Users/janwardenga/Zotero/storage/RMBZYBL8/Rahm and Bernstein - 2001 - A survey of approaches to automatic schema matching.pdf}
}

@article{rajabiKnowledgegraphbasedExplainableAI2024JournalofInformationScience,
  title = {Knowledge-Graph-Based Explainable {{AI}}: {{A}} Systematic Review},
  shorttitle = {Knowledge-Graph-Based Explainable {{AI}}},
  author = {Rajabi, Enayat and Etminani, Kobra},
  year = {2024},
  month = aug,
  journal = {Journal of Information Science},
  volume = {50},
  number = {4},
  pages = {1019--1029},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/01655515221112844},
  urldate = {2024-09-29},
  abstract = {In recent years, knowledge graphs (KGs) have been widely applied in various domains for different purposes. The semantic model of KGs can represent knowledge through a hierarchical structure based on classes of entities, their properties, and their relationships. The construction of large KGs can enable the integration of heterogeneous information sources and help Artificial Intelligence (AI) systems be more explainable and interpretable. This systematic review examines a selection of recent publications to understand how KGs are currently being used in eXplainable AI systems. To achieve this goal, we design a framework and divide the use of KGs into four categories: extracting features, extracting relationships, constructing KGs, and KG reasoning. We also identify where KGs are mostly used in eXplainable AI systems (pre-model, in-model, and post-model) according to the aforementioned categories. Based on our analysis, KGs have been mainly used in pre-model XAI for feature and relation extraction. They were also utilised for inference and reasoning in post-model XAI. We found several studies that leveraged KGs to explain the XAI models in the healthcare domain.},
  langid = {english},
  keywords = {Jab/JIS,Unread},
  annotation = {37 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/P42T9UB7/JIS_2024_Knowledge-graph-based explainable AI.pdf}
}

@article{rajabiKnowledgeGraphsExplainable2022Information,
  title = {Knowledge {{Graphs}} and {{Explainable AI}} in {{Healthcare}}},
  author = {Rajabi, Enayat and Kafaie, Somayeh},
  year = {2022},
  month = sep,
  journal = {Information},
  volume = {13},
  number = {10},
  pages = {459},
  issn = {2078-2489},
  doi = {10.3390/info13100459},
  urldate = {2024-09-29},
  abstract = {Building trust and transparency in healthcare can be achieved using eXplainable Artificial Intelligence (XAI), as it facilitates the decision-making process for healthcare professionals. Knowledge graphs can be used in XAI for explainability by structuring information, extracting features and relations, and performing reasoning. This paper highlights the role of knowledge graphs in XAI models in healthcare, considering a state-of-the-art review. Based on our review, knowledge graphs have been used for explainability to detect healthcare misinformation, adverse drug reactions, drug-drug interactions and to reduce the knowledge gap between healthcare experts and AI-based models. We also discuss how to leverage knowledge graphs in pre-model, in-model, and post-model XAI models in healthcare to make them more explainable.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {19 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/KUL483HC/I_2022_Knowledge Graphs and Explainable AI in Healthcare.pdf}
}

@misc{ranganFinetuningEnhancedRAG2024,
  title = {A {{Fine-tuning Enhanced RAG System}} with {{Quantized Influence Measure}} as {{AI Judge}}},
  author = {Rangan, Keshav and Yin, Yiqiao},
  year = {2024},
  month = feb,
  number = {arXiv:2402.17081},
  eprint = {2402.17081},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.17081},
  urldate = {2024-09-29},
  abstract = {This study presents an innovative enhancement to retrieval-augmented generation (RAG) systems by seamlessly integrating fine-tuned large language models (LLMs) with vector databases. This integration capitalizes on the combined strengths of structured data retrieval and the nuanced comprehension provided by advanced LLMs. Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model's continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a Quantized Influence Measure (QIM) as an innovative "AI Judge" mechanism to enhance the precision of result selection, further refining the system's accuracy. Accompanied by an executive diagram and a detailed algorithm for fine-tuning QLoRA, our work provides a comprehensive framework for implementing these advancements within chatbot technologies. This research contributes significant insights into LLM optimization for specific uses and heralds new directions for further development in retrieval-augmented models. Through extensive experimentation and analysis, our findings lay a robust foundation for future advancements in chatbot technology and retrieval systems, marking a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems. We make the dataset, the model, and the app publicly available for the literature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/ABEFXBU2/Pre_2024_A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge.pdf}
}

@misc{renQuery2boxReasoningKnowledge2020,
  title = {Query2box: {{Reasoning}} over {{Knowledge Graphs}} in {{Vector Space}} Using {{Box Embeddings}}},
  shorttitle = {Query2box},
  author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  year = {2020},
  month = feb,
  number = {arXiv:2002.05969},
  eprint = {2002.05969},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2002.05969},
  urldate = {2024-09-29},
  abstract = {Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ({$\wedge$}) and existential quantifiers ({$\exists$}). Handling queries with logical disjunctions ({$\vee$}) remains an open problem. Here we propose QUERY2BOX, an embedding-based framework for reasoning over arbitrary queries with {$\wedge$}, {$\vee$}, and {$\exists$} operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, QUERY2BOX is capable of handling arbitrary logical queries with {$\wedge$}, {$\vee$}, {$\exists$} in a scalable manner. We demonstrate the effectiveness of QUERY2BOX on three large KGs and show that QUERY2BOX achieves up to 25\% relative improvement over the state of the art.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {281 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/49BR3KDS/Pre_2020_Query2box.pdf}
}

@inproceedings{renSMOREKnowledgeGraph2022Proc.28thACMSIGKDDConf.Knowl.Discov.DataMin.,
  title = {{{SMORE}}: {{Knowledge Graph Completion}} and {{Multi-hop Reasoning}} in {{Massive Knowledge Graphs}}},
  shorttitle = {{{SMORE}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
  year = {2022},
  month = aug,
  pages = {1472--1482},
  publisher = {ACM},
  address = {Washington DC USA},
  doi = {10.1145/3534678.3539405},
  urldate = {2024-09-29},
  isbn = {978-1-4503-9385-0},
  langid = {english},
  keywords = {Jab/KSCKDDM,Unread},
  annotation = {38 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/TZ4C72PN/KSCKDDM_2022_SMORE.pdf}
}

@incollection{ristoskiRDF2VecRDFGraph2016TheSemanticWeb-ISWC2016,
  title = {{{RDF2Vec}}: {{RDF Graph Embeddings}} for {{Data Mining}}},
  shorttitle = {{{RDF2Vec}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2016},
  author = {Ristoski, Petar and Paulheim, Heiko},
  editor = {Groth, Paul and Simperl, Elena and Gray, Alasdair and Sabou, Marta and Kr{\"o}tzsch, Markus and Lecue, Freddy and Fl{\"o}ck, Fabian and Gil, Yolanda},
  year = {2016},
  volume = {9981},
  pages = {498--514},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46523-4_30},
  urldate = {2024-09-29},
  abstract = {Linked Open Data has been recognized as a valuable source for background information in data mining. However, most data mining tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph substructures, harvested by Weisfeiler-Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. Our evaluation shows that such vector representations outperform existing techniques for the propositionalization of RDF graphs on a variety of different predictive machine learning tasks, and that feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.},
  isbn = {978-3-319-46522-7 978-3-319-46523-4},
  langid = {english},
  keywords = {,Jab/Book,Read},
  file = {/Users/janwardenga/Zotero/storage/2RJL4FWU/Book_2016_RDF2Vec.pdf}
}

@article{ristoskiRDF2VecRDFGraph2019SW,
  title = {{{RDF2Vec}}: {{RDF}} Graph Embeddings and Their Applications},
  shorttitle = {{{RDF2Vec}}},
  author = {Ristoski, Petar and Rosati, Jessica and Di Noia, Tommaso and De Leone, Renato and Paulheim, Heiko},
  editor = {Lecue, Freddy},
  year = {2019},
  month = may,
  journal = {Semantic Web},
  volume = {10},
  number = {4},
  pages = {721--752},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-180317},
  urldate = {2024-09-29},
  abstract = {Linked Open Data has been recognized as a valuable source for background information in many data mining and information retrieval tasks. However, most of the existing tools require features in propositional form, i.e., a vector of nominal or numerical features associated with an instance, while Linked Open Data sources are graphs by nature. In this paper, we present RDF2Vec, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs. We generate sequences by leveraging local information from graph sub-structures, harvested by Weisfeiler--Lehman Subtree RDF Graph Kernels and graph walks, and learn latent numerical representations of entities in RDF graphs. We evaluate our approach on three different tasks: (i) standard machine learning tasks, (ii) entity and document modeling, and (iii) content-based recommender systems. The evaluation shows that the proposed entity embeddings outperform existing techniques, and that pre-computed feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.},
  langid = {english},
  keywords = {Unread},
  annotation = {178 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: RDF2Vec is presented, an approach that uses language modeling approaches for unsupervised feature extraction from sequences of words, and adapts them to RDF graphs that shows that the proposed entity embeddings outperform existing techniques, and that pre-computed feature vector representations of general knowledge graphs such as DBpedia and Wikidata can be easily reused for different tasks.},
  file = {/Users/janwardenga/Zotero/storage/8475UFBG/SW_2019_RDF2Vec.pdf}
}

@misc{rokonRepo2VecComprehensiveEmbedding2021,
  title = {{{Repo2Vec}}: {{A Comprehensive Embedding Approach}} for {{Determining Repository Similarity}}},
  shorttitle = {{{Repo2Vec}}},
  author = {Rokon, Md Omar Faruk and Yan, Pei and Islam, Risul and Faloutsos, Michalis},
  year = {2021},
  month = jul,
  number = {arXiv:2107.05112},
  eprint = {2107.05112},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2107.05112},
  urldate = {2024-09-29},
  abstract = {How can we identify similar repositories and clusters among a large online archive, such as GitHub? Determining repository similarity is an essential building block in studying the dynamics and the evolution of such software ecosystems. The key challenge is to determine the right representation for the diverse repository features in a way that: (a) it captures all aspects of the available information, and (b) it is readily usable by ML algorithms. We propose Repo2Vec, a comprehensive embedding approach to represent a repository as a distributed vector by combining features from three types of information sources. As our key novelty, we consider three types of information: (a) metadata, (b) the structure of the repository, and (c) the source code. We also introduce a series of embedding approaches to represent and combine these information types into a single embedding. We evaluate our method with two real datasets from GitHub for a combined 1013 repositories. First, we show that our method outperforms previous methods in terms of precision (93\% vs 78\%), with nearly twice as many Strongly Similar repositories and 30\% fewer False Positives. Second, we show how Repo2Vec provides a solid basis for: (a) distinguishing between malware and benign repositories, and (b) identifying a meaningful hierarchical clustering. For example, we achieve 98\% precision, and 96\% recall in distinguishing malware and benign repositories. Overall, our work is a fundamental building block for enabling many repository analysis functions such as repository categorization by target platform or intention, detecting code-reuse and clones, and identifying lineage and evolution.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {15 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/62SNWV59/Pre_2021_Repo2Vec.pdf}
}

@article{ronySGPTGenerativeApproach2022IEEEAccess,
  title = {{{SGPT}}: {{A Generative Approach}} for {{SPARQL Query Generation From Natural Language Questions}}},
  shorttitle = {{{SGPT}}},
  author = {Rony, Md Rashad Al Hasan and Kumar, Uttam and Teucher, Roman and Kovriguina, Liubov and Lehmann, Jens},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {70712--70723},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3188714},
  urldate = {2025-01-26},
  abstract = {SPARQL query generation from natural language questions is complex because it requires an understanding of both the question and underlying knowledge graph (KG) patterns. Most SPARQL query generation approaches are template-based, tailored to a specific knowledge graph and require pipelines with multiple steps, including entity and relation linking. Template-based approaches are also difficult to adapt for new KGs and require manual efforts from domain experts to construct query templates. To overcome this hurdle, we propose a new approach, dubbed SGPT, that combines the benefits of end-to-end and modular systems and leverages recent advances in large-scale language models. Specifically, we devise a novel embedding technique that can encode linguistic features from the question which enables the system to learn complex question patterns. In addition, we propose training techniques that allow the system to implicitly employ the graph-specific information (i.e., entities and relations) into the language model's parameters and generate SPARQL queries accurately. Finally, we introduce a strategy to adapt standard automatic metrics for evaluating SPARQL query generation. A comprehensive evaluation demonstrates the effectiveness of SGPT over state-of-the-art methods across several benchmark datasets.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  keywords = {Jab/A,Read},
  annotation = {22 citations (Semantic Scholar/DOI) [2025-02-20]\\
7 citations (Semantic Scholar/DOI) [2025-02-15]\\
22 citations (Semantic Scholar/DOI) [2025-01-26]\\
TLDR: A novel embedding technique is devised that can encode linguistic features from the question which enables the system to learn complex question patterns and is demonstrated to be effective over state-of-the-art methods across several benchmark datasets.},
  file = {/Users/janwardenga/Zotero/storage/42HRWBGP/A_2022_SGPT.pdf}
}

@inproceedings{rosati2016rdf,
  title = {{{RDF}} Graph Embeddings for Content-Based Recommender Systems},
  booktitle = {{{CEUR}} Workshop Proceedings},
  author = {Rosati, Jessica and Ristoski, Petar and Di Noia, Tommaso and de Leone, Renato and Paulheim, Heiko},
  year = {2016},
  volume = {1673},
  pages = {23--30},
  publisher = {RWTH Aachen},
  keywords = {,No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/PPXQMQMU/Pre__RDF Graph Embeddings for Content-based Recommender Systems.pdf}
}

@misc{ruRAGCheckerFinegrainedFramework2024,
  title = {{{RAGChecker}}: {{A Fine-grained Framework}} for {{Diagnosing Retrieval-Augmented Generation}}},
  shorttitle = {{{RAGChecker}}},
  author = {Ru, Dongyu and Qiu, Lin and Hu, Xiangkun and Zhang, Tianhang and Shi, Peng and Chang, Shuaichen and Jiayang, Cheng and Wang, Cunxiang and Sun, Shichao and Li, Huanyu and Zhang, Zizhao and Wang, Binjie and Jiang, Jiarong and He, Tong and Wang, Zhiguo and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
  year = {2024},
  month = aug,
  number = {arXiv:2408.08067},
  eprint = {2408.08067},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.08067},
  urldate = {2024-09-29},
  abstract = {Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGCHECKER, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGCHECKER has significantly better correlations with human judgments than other evaluation metrics. Using RAGCHECKER, we evaluate 8 RAG systems and conduct an indepth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGCHECKER can guide researchers and practitioners in developing more effective RAG systems3.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/32QUTXWX/Pre_2024_RAGChecker.pdf}
}

@incollection{russell-roseRoleNaturalLanguage2009InformationRetrieval,
  title = {The {{Role}} of {{Natural Language Processing}} in {{Information Retrieval}}: {{Searching}} for {{Meaning}} and {{Structure}}},
  shorttitle = {The {{Role}} of {{Natural Language Processing}} in {{Information Retrieval}}},
  booktitle = {Information {{Retrieval}}},
  author = {Russell-Rose, Tony and Stevenson, Mark},
  editor = {G{\"o}ker, Ay{\c s}e and Davies, John},
  year = {2009},
  month = oct,
  edition = {1},
  pages = {215--231},
  publisher = {Wiley},
  doi = {10.1002/9780470033647.ch10},
  urldate = {2024-09-29},
  isbn = {978-0-470-02762-2 978-0-470-03364-7},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/XZXNRW9M/Book_2009_The Role of Natural Language Processing in Information Retrieval.pdf}
}

@misc{saeedNotAllEmbeddings2018,
  title = {Not All {{Embeddings}} Are Created {{Equal}}: {{Extracting Entity-specific Substructures}} for {{RDF Graph Embedding}}},
  shorttitle = {Not All {{Embeddings}} Are Created {{Equal}}},
  author = {Saeed, Muhammad Rizwan and Chelmis, Charalampos and Prasanna, Viktor K.},
  year = {2018},
  month = apr,
  number = {arXiv:1804.05184},
  eprint = {1804.05184},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1804.05184},
  urldate = {2024-09-29},
  abstract = {Knowledge Graphs (KGs) are becoming essential to information systems that require access to structured data. Several approaches have been recently proposed, for obtaining vector representations of KGs suitable for Machine Learning tasks, based on identifying and extracting relevant graph substructures using uniform and biased random walks. However, such approaches lead to representations comprising mostly ``popular'', instead of ``relevant'', entities in the KG. In KGs, in which different types of entities often exist (such as in Linked Open Data), a given target entity may have its own distinct set of most ``relevant'' nodes and edges. We propose specificity as an accurate measure of identifying most relevant, entity-specific, nodes and edges. We develop a scalable method based on bidirectional random walks to compute specificity. Our experimental evaluation results show that specificity-based biased random walks extract more ``meaningful'' (in terms of size and relevance) RDF substructures compared to the state-of-the-art and, the graph embedding learned from the extracted substructures, outperform existing techniques in the task of entity recommendation in DBpedia.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {8 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/97FN7JHQ/Pre_2018_Not all Embeddings are created Equal.pdf}
}

@inproceedings{salemiEvaluatingRetrievalQuality2024Proc.47thInt.ACMSIGIRConf.Res.Dev.Inf.Retr.,
  title = {Evaluating {{Retrieval Quality}} in {{Retrieval-Augmented Generation}}},
  booktitle = {Proceedings of the 47th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Salemi, Alireza and Zamani, Hamed},
  year = {2024},
  month = jul,
  pages = {2395--2400},
  publisher = {ACM},
  address = {Washington DC USA},
  doi = {10.1145/3626772.3657957},
  urldate = {2024-09-29},
  abstract = {Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's {$T$} correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.},
  isbn = {9798400704314},
  langid = {english},
  keywords = {Jab/SISCRDIR,Unread},
  file = {/Users/janwardenga/Zotero/storage/J8SHEGMX/SISCRDIR_2024_Evaluating Retrieval Quality in Retrieval-Augmented Generation.pdf}
}

@misc{sannidhiRetrievalAugmentedGenerationMeets2024,
  title = {Retrieval-{{Augmented Generation Meets Data-Driven Tabula Rasa Approach}} for {{Temporal Knowledge Graph Forecasting}}},
  author = {Sannidhi, Geethan and Sakhinana, Sagar Srinivas and Runkana, Venkataramana},
  year = {2024},
  month = aug,
  number = {arXiv:2408.13273},
  eprint = {2408.13273},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.13273},
  urldate = {2024-09-30},
  abstract = {Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google Gemini face challenges such as inaccurate factual recall, hallucinations, biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting. To address these issues, we introduce sLA-tKGF (small-scale language assistant for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG) aided, custom-trained small-scale language models through a tabula rasa approach from scratch for effective tKG forecasting. Our framework constructs knowledge-infused prompts with relevant historical data from tKGs, web search results, and PLLMs-generated textual descriptions to understand historical entity relationships prior to the target time. It leverages these external knowledge-infused prompts for deeper understanding and reasoning of context-specific semantic and temporal information to zero-shot prompt small-scale language models for more accurate predictions of future events within tKGs. It reduces hallucinations and mitigates distributional shift challenges through comprehending changing trends over time. As a result, it enables more accurate and contextually grounded forecasts of future events while minimizing computational demands. Rigorous empirical studies demonstrate our framework's robustness, scalability, and state-of-the-art (SOTA) performance on benchmark datasets with interpretable and trustworthy tKG forecasting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Unread},
  annotation = {27 citations (Semantic Scholar/arXiv) [2025-02-15]\\
0 citations (Semantic Scholar/arXiv) [2024-09-30]},
  file = {/Users/janwardenga/Zotero/storage/WM4Y6EDH/Sannidhi et al. - 2024 - Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph F.pdf}
}

@misc{sarkarNavigatingKnowledgeSea2024,
  title = {Navigating the {{Knowledge Sea}}: {{Planet-scale}} Answer Retrieval Using {{LLMs}}},
  shorttitle = {Navigating the {{Knowledge Sea}}},
  author = {Sarkar, Dipankar},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05318},
  eprint = {2402.05318},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.05318},
  urldate = {2024-09-29},
  abstract = {Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones that have shaped this journey and the potential future directions in this rapidly changing field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/ZTQVSGPB/Pre_2024_Navigating the Knowledge Sea.pdf}
}

@incollection{sarkerWikipediaKnowledgeGraph2020KnowledgeGraphsandSemanticWeb,
  title = {Wikipedia {{Knowledge Graph}} for {{Explainable AI}}},
  booktitle = {Knowledge {{Graphs}} and {{Semantic Web}}},
  author = {Sarker, Md Kamruzzaman and Schwartz, Joshua and Hitzler, Pascal and Zhou, Lu and Nadella, Srikanth and Minnery, Brandon and Juvina, Ion and Raymer, Michael L. and Aue, William R.},
  editor = {{Villaz{\'o}n-Terrazas}, Boris and {Ortiz-Rodr{\'i}guez}, Fernando and Tiwari, Sanju M. and Shandilya, Shishir K.},
  year = {2020},
  volume = {1232},
  pages = {72--87},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-65384-2_6},
  urldate = {2024-09-29},
  abstract = {Explainable artificial intelligence (XAI) requires domain information to explain a system's decisions, for which structured forms of domain information like Knowledge Graphs (KGs) or ontologies are best suited. As such, readily available KGs are important to accelerate progress in XAI. To facilitate the advancement of XAI, we present the cycle-free Wikipedia Knowledge Graph (WKG) based on information from English Wikipedia. Each Wikipedia article title, its corresponding category, and the category hierarchy are transformed into different entities in the knowledge graph. Along with cycle-free version we also provide the original knowledge graph as it is. We evaluate whether the WKG is helpful to improve XAI compared with existing KGs, finding that WKG is better suited than the current state of the art. We also compare the cycle-free WKG with the Suggested Upper Merged Ontology (SUMO) and DBpedia schema KGs, finding minimal to no information loss.},
  isbn = {978-3-030-65383-5 978-3-030-65384-2},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/CNHT6W94/Book_2020_Wikipedia Knowledge Graph for Explainable AI.pdf}
}

@misc{sarmahHybridRAGIntegratingKnowledge2024,
  title = {{{HybridRAG}}: {{Integrating Knowledge Graphs}} and {{Vector Retrieval Augmented Generation}} for {{Efficient Information Extraction}}},
  shorttitle = {{{HybridRAG}}},
  author = {Sarmah, Bhaskarjit and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano and Mehta, Dhagash},
  year = {2024},
  month = aug,
  number = {arXiv:2408.04948},
  eprint = {2408.04948},
  primaryclass = {cs, q-fin, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.04948},
  urldate = {2024-09-30},
  abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q\&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q\&A format, and hence provide a natural set of pairs of ground-truth Q\&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Computation and Language,Computer Science - Machine Learning,Jab/Pre,Quantitative Finance - Statistical Finance,Read,Statistics - Applications,Statistics - Machine Learning},
  annotation = {13 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/IT4WGQA4/Pre_2024_HybridRAG.pdf}
}

@misc{sarthiRAPTORRecursiveAbstractive2024,
  title = {{{RAPTOR}}: {{Recursive Abstractive Processing}} for {{Tree-Organized Retrieval}}},
  shorttitle = {{{RAPTOR}}},
  author = {Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.18059},
  eprint = {2401.18059},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.18059},
  urldate = {2024-12-20},
  abstract = {Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\% in absolute accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Read},
  annotation = {78 citations (Semantic Scholar/DOI) [2025-02-15]\\
70 citations (Semantic Scholar/DOI) [2024-12-20]\\
TLDR: This work introduces the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up, and retrieves from this tree, integrating information across lengthy documents at different levels of abstraction.},
  file = {/Users/janwardenga/Zotero/storage/TT4Y3Z55/Sarthi et al. - 2024 - RAPTOR Recursive Abstractive Processing for Tree-Organized Retrieval.pdf}
}

@incollection{schmidUsingKnowledgeGraphs2019TheSemanticWeb:ESWC2019SatelliteEvents,
  title = {Using {{Knowledge Graphs}} to {{Search}} an {{Enterprise Data Lake}}},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2019 {{Satellite Events}}},
  author = {Schmid, Stefan and Henson, Cory and Tran, Tuan},
  editor = {Hitzler, Pascal and Kirrane, Sabrina and Hartig, Olaf and De Boer, Victor and Vidal, Maria-Esther and Maleshkova, Maria and Schlobach, Stefan and Hammar, Karl and Lasierra, Nelia and Stadtm{\"u}ller, Steffen and Hose, Katja and Verborgh, Ruben},
  year = {2019},
  volume = {11762},
  pages = {262--266},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32327-1_46},
  urldate = {2025-01-27},
  abstract = {This paper summarizes our research \& development activities in building a semantic data management platform for large enterprise data lakes, with a focus on the automotive domain. We demonstrate the use of ontology models to systematically represent, link, and search large amounts of automotive data. Such search capability is an important enabler for Hadoop-based big data analytics and machine learning. These findings are being transferred to a productive system in order to foster the advanced engineering and AI at Bosch Chassis Systems Control (CC), especially in the automated driving area.},
  isbn = {978-3-030-32326-4 978-3-030-32327-1},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/LKUC9RNW/Schmid et al. - 2019 - Using Knowledge Graphs to Search an Enterprise Data Lake.pdf}
}

@misc{schneiderCourseModularizationApplied1973,
  title = {Course Modularization Applied: {{The}} Interface System and Its Implications for Sequence Control and Data Analysis.: (436252004-001)},
  shorttitle = {Course Modularization Applied},
  author = {Schneider, E. W.},
  year = {1973},
  publisher = {American Psychological Association},
  doi = {10.1037/e436252004-001},
  urldate = {2025-02-15},
  langid = {english},
  keywords = {Jab/Pre,Unread},
  annotation = {38 citations (Semantic Scholar/DOI) [2025-02-15]\\
7 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The Interface System is a comprehensive method for developing and managing computer-assisted instructional courses or computer-managed instructional courses composed of sets of instructional modules that combines a standari general structure for all modules.},
  file = {/Users/janwardenga/Zotero/storage/E3HZ9DKM/Pre_1973_Course modularization applied.pdf}
}

@misc{sequedaBenchmarkUnderstandRole2023,
  title = {A {{Benchmark}} to {{Understand}} the {{Role}} of {{Knowledge Graphs}} on {{Large Language Model}}'s {{Accuracy}} for {{Question Answering}} on {{Enterprise SQL Databases}}},
  author = {Sequeda, Juan and Allemang, Dean and Jacob, Bryon},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07509},
  eprint = {2311.07509},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07509},
  urldate = {2025-01-26},
  abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16\%. Notably, this accuracy increases to 54\% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Unread},
  annotation = {23 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/MQFXPHEU/Sequeda et al. - 2023 - A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Questi.pdf}
}

@misc{shaoVortexRippletEmpirical2024,
  title = {Vortex under {{Ripplet}}: {{An Empirical Study}} of {{RAG-enabled Applications}}},
  shorttitle = {Vortex under {{Ripplet}}},
  author = {Shao, Yuchen and Huang, Yuheng and Shen, Jiawei and Ma, Lei and Su, Ting and Wan, Chengcheng},
  year = {2024},
  month = jul,
  number = {arXiv:2407.05138},
  eprint = {2407.05138},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.05138},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs) enhanced by retrieval-augmented generation (RAG) provide effective solutions in various application scenarios. However, developers face challenges in integrating RAG-enhanced LLMs into software systems, due to lack of interface specification, requirements from software context, and complicated system management. In this paper, we manually studied 100 open-source applications that incorporate RAG-enhanced LLMs, and their issue reports. We have found that more than 98\% of applications contain multiple integration defects that harm software functionality, efficiency, and security. We have also generalized 19 defect patterns and proposed guidelines to tackle them. We hope this work could aid LLM-enabled software development and motivate future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/D8QK76WY/Pre_2024_Vortex under Ripplet.pdf}
}

@article{ShapeMapStructureLanguage,
  title = {{{ShapeMap Structure}} and {{Language}}},
  abstract = {The Shape Expressions (ShEx) language describes RDF nodes and graph structures. A node constraint describes an RDF node (IRI, blank node or literal) and a shape describes the triples involving nodes in an RDF graph. The ShapeMap language associates RDF nodes with ShEx shapes. These associations can be used to state candidate shape maps as an input to the validation process. They can be the output of a validation process, where the ShEx engine reports the conformance of RDF nodes with respect to ShEx shapes.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/FLTJD9MR/ShapeMap Structure and Language.pdf}
}

@article{shaRetrievalAugmentedKnowledgeGraph2023Mathematics,
  title = {Retrieval-{{Augmented Knowledge Graph Reasoning}} for {{Commonsense Question Answering}}},
  author = {Sha, Yuchen and Feng, Yujian and He, Miao and Liu, Shangdong and Ji, Yimu},
  year = {2023},
  month = jul,
  journal = {Mathematics},
  volume = {11},
  number = {15},
  pages = {3269},
  issn = {2227-7390},
  doi = {10.3390/math11153269},
  urldate = {2024-09-30},
  abstract = {Existing knowledge graph (KG) models for commonsense question answering present two challenges: (i) existing methods retrieve entities related to questions from the knowledge graph, which may extract noise and irrelevant nodes, and (ii) there is a lack of interaction representation between questions and graph entities. However, current methods mainly focus on retrieving relevant entities with some noisy and irrelevant nodes. In this paper, we propose a novel retrieval-augmented knowledge graph (RAKG) model, which solves the above issues using two key innovations. First, we leverage the density matrix to make the model reason along the corrected knowledge path and extract an enhanced subgraph of the knowledge graph. Second, we fuse representations of questions and graph entities through a bidirectional attention strategy, in which two representations fuse and update using a graph convolutional network (GCN). To evaluate the performance of our method, we conducted experiments on two widely used benchmark datasets: CommonsenseQA and OpenBookQA. The case study gives insight into the finding that the augmented subgraph provides reasoning along the corrected knowledge path for question answering.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {6 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: A novel retrieval-augmented knowledge graph (RAKG) model is proposed, which solves the above issues using two key innovations using the density matrix to make the model reason along the corrected knowledge path and extract an enhanced subgraph of the knowledge graph.},
  file = {/Users/janwardenga/Zotero/storage/45PA6PLJ/Sha et al. - 2023 - Retrieval-Augmented Knowledge Graph Reasoning for Commonsense Question Answering.pdf}
}

@misc{sharmaReducingHallucinationsLanguage2025,
  title = {Reducing {{Hallucinations}} in {{Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval}}},
  author = {Sharma, Aditya and Lara, Luis and Zouaq, Amal and Pal, Christopher J.},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13369},
  eprint = {2502.13369},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13369},
  urldate = {2025-02-20},
  abstract = {The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-20]\\
0 citations (Semantic Scholar/arXiv) [2025-02-20]},
  file = {/Users/janwardenga/Zotero/storage/5WQ7DHTF/Sharma et al. - 2025 - Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory.pdf;/Users/janwardenga/Zotero/storage/S3XJEHR9/2502.html}
}

@inproceedings{shenEnhancingRAGRetrieval202420245thInt.Conf.Electron.Commun.Artif.Intell.ICECAI,
  title = {Enhancing the {{RAG Retrieval Engine Through Multi-Encoder Fusion}}},
  booktitle = {2024 5th {{International Conference}} on {{Electronic Communication}} and {{Artificial Intelligence}} ({{ICECAI}})},
  author = {Shen, Jie and He, Huaiyuan and Shen, Wenzhuo and Shen, Tiyan},
  year = {2024},
  month = may,
  pages = {227--230},
  publisher = {IEEE},
  address = {Shenzhen, China},
  doi = {10.1109/ICECAI62591.2024.10674962},
  urldate = {2024-09-29},
  abstract = {With the rapid advancement of large language model technology, Retriever-Augmented Generation (RAG), which is based on vector databases, has demonstrated extensive potential for application. This paper focuses on the text retrieval phase of RAG, enhancing text recall by introducing multiple text encoders. Consequently, this paper designs a neural network model and loss function for multi-encoder fusion and determines the optimal weight allocation for multi-encoder fusion through neural network training. Experimental results demonstrate that the predicted fusion weights from the neural network trained in this paper can be closely aligned with the optimal fusion weights. By using the predicted BCE and BGE encoding model weights, an improvement of 6\% in Mean Reciprocal Rank (MRR) is achieved.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350386943},
  langid = {english},
  keywords = {Jab/ICECAI,Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/W3CJ79CM/ICECAI_2024_Enhancing the RAG Retrieval Engine Through Multi-Encoder Fusion.pdf}
}

@article{shethKnowledgeGraphsKnowledge2019IEEEInternetComput.,
  title = {Knowledge {{Graphs}} and {{Knowledge Networks}}: {{The Story}} in {{Brief}}},
  shorttitle = {Knowledge {{Graphs}} and {{Knowledge Networks}}},
  author = {Sheth, Amit and Padhee, Swati and Gyrard, Amelie},
  year = {2019},
  month = jul,
  journal = {IEEE Internet Computing},
  volume = {23},
  number = {4},
  pages = {67--75},
  issn = {1089-7801, 1941-0131},
  doi = {10.1109/MIC.2019.2928449},
  urldate = {2024-09-29},
  abstract = {Knowledge Graphs (KGs) represent real-world noisy raw information in a structured form, capturing relationships between entities. However, for dynamic real-world applications such as social networks, recommender systems, computational biology, relational knowledge representation has emerged as a challenging research problem where there is a need to represent the changing nodes, attributes, and edges over time. The evolution of search engine responses to user queries in the last few years is partly because of the role of KGs such as Google KG. KGs are significantly contributing to various AI applications from link prediction, entity relations prediction, node classification to recommendation and question answering systems. This article is an attempt to summarize the journey of KG for AI.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/USG.html},
  langid = {english},
  keywords = {Jab/IC,Unread},
  annotation = {49 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/LV3AAX33/IC_2019_Knowledge Graphs and Knowledge Networks.pdf}
}

@inproceedings{shimorinaKnowledgeExtractionTexts2022Proc.2022Conf.NorthAm.ChapterAssoc.Comput.Linguist.Hum.Lang.Technol.Ind.Track,
  title = {Knowledge {{Extraction From Texts Based}} on {{Wikidata}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Industry Track}}},
  author = {Shimorina, Anastasia and Heinecke, Johannes and Herledan, Fr{\'e}d{\'e}ric},
  year = {2022},
  pages = {297--304},
  publisher = {Association for Computational Linguistics},
  address = {Hybrid: Seattle, Washington + Online},
  doi = {10.18653/v1/2022.naacl-industry.33},
  urldate = {2025-02-27},
  abstract = {This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models1.},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-27]\\
TLDR: A system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema is presented, which can be further used for constructing an entreprise-specific knowledge base.},
  file = {/Users/janwardenga/Zotero/storage/8BFY9C9N/Shimorina et al. - 2022 - Knowledge Extraction From Texts Based on Wikidata.pdf}
}

@inproceedings{shiTopkRelevantSemantic2016Proc.2016Int.Conf.Manag.Data,
  title = {Top-k {{Relevant Semantic Place Retrieval}} on {{Spatial RDF Data}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Shi, Jieming and Wu, Dingming and Mamoulis, Nikos},
  year = {2016},
  month = jun,
  pages = {1977--1990},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2882903.2882941},
  urldate = {2024-09-29},
  abstract = {RDF data are traditionally accessed using structured query languages, such as SPARQL. However, this requires users to understand the language as well as the RDF schema. Keyword search on RDF data aims at relieving the user from these requirements; the user only inputs a set of keywords and the goal is to find small RDF subgraphs which contain all keywords. At the same time, popular RDF knowledge bases also include spatial semantics, which opens the road to location-based search operations. In this work, we propose and study a novel location-based keyword search query on RDF data. The objective of top-k relevant semantic places (kSP) retrieval is to find RDF subgraphs which contain the query keywords and are rooted at spatial entities close to the query location. The novelty of kSP queries is that they are location-aware and that they do not rely on the use of structured query languages. We design a basic method for the processing of kSP queries. To further accelerate kSP retrieval, two pruning approaches and a data preprocessing technique are proposed. Extensive empirical studies on two real datasets demonstrate the superior and robust performance of our proposals compared to the basic method.},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {,Jab/ICMD,Unread},
  annotation = {29 citations (Semantic Scholar/DOI) [2025-02-20]\\
28 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work proposes and study a novel location-based keyword search query on RDF data and designs a basic method for the processing of kSP queries, and proposes two pruning approaches and a data preprocessing technique to accelerate kSP retrieval.},
  file = {/Users/janwardenga/Zotero/storage/9SBHM5K7/ICMD_2016_Top-k Relevant Semantic Place Retrieval on Spatial RDF Data.pdf}
}

@article{siddharthEnhancingPatentRetrieval2022JournalofEngineeringDesign,
  title = {Enhancing Patent Retrieval Using Text and Knowledge Graph Embeddings: A Technical Note},
  shorttitle = {Enhancing Patent Retrieval Using Text and Knowledge Graph Embeddings},
  author = {Siddharth, L. and Li, Guangtong and Luo, Jianxi},
  year = {2022},
  month = sep,
  journal = {Journal of Engineering Design},
  volume = {33},
  number = {8-9},
  pages = {670--683},
  issn = {0954-4828, 1466-1837},
  doi = {10.1080/09544828.2022.2144714},
  urldate = {2024-09-29},
  abstract = {Patent retrieval influences several applications within engineering design research, education, and practice as well as applications that concern innovation, intellectual property, and knowledge management etc. In this article, we propose a method to retrieve patents relevant to an initial set of patents, by synthesizing state-ofthe-art techniques among natural language processing and knowledge graph embedding. Our method involves a patent embedding that captures text, citation, and inventor information, which individually represent different facets of knowledge communicated through a patent document. We obtain text embeddings using Sentence-BERT applied to titles and abstracts. We obtain citation and inventor embeddings through TransE that is trained using the corresponding knowledge graphs. We identify using a classification task that the concatenation of text, citation, and inventor embeddings offers a plausible representation of a patent. While the proposed patent embedding could be used to associate a pair of patents, we observe using a recall task that multiple initial patents could be associated with a target patent using mean cosine similarity, which could then be utilized to rank all target patents and retrieve the most relevant ones. We apply the proposed patent retrieval method to a set of patents corresponding to a product family and an inventor's portfolio.},
  langid = {english},
  keywords = {Jab/JED,Unread},
  annotation = {13 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/2DU8MESJ/JED_2022_Enhancing patent retrieval using text and knowledge graph embeddings.pdf}
}

@article{siddharthRetrievalAugmentedGeneration2024Knowledge-BasedSystems,
  title = {Retrieval Augmented Generation Using Engineering Design Knowledge},
  author = {Siddharth, L. and Luo, Jianxi},
  year = {2024},
  month = nov,
  journal = {Knowledge-Based Systems},
  volume = {303},
  pages = {112410},
  issn = {09507051},
  doi = {10.1016/j.knosys.2024.112410},
  urldate = {2024-09-30},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-15]}
}

@electronic{singhal2012introducing,
  title = {Introducing the {{Knowledge Graph}}: Things, Not Strings},
  author = {Singhal, Amit},
  year = {2012},
  url = {https://www.blog.google/products/search/introducing-knowledge-graph-things-not/},
  added-at = {2022-11-26T03:41:36.000+0100},
  interhash = {091175fd22ba89545528f5182c81eb88},
  intrahash = {3de8f9e39e3b0c5df9709b2548fbd189},
  keywords = {Graph Knowledge,Unread},
  timestamp = {2022-11-26T03:41:36.000+0100}
}

@article{singhalIntroducingKnowledgeGraph2012,
  title = {Introducing the {{Knowledge Graph}}: Things, Not Strings},
  author = {Singhal, Amit},
  year = {2012},
  url = {https://blog.google/products/search/introducing-knowledge-graph-things-not/},
  langid = {english},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/63SHDDKV/Singhal - Introducing the Knowledge Graph things, not strings.pdf}
}

@inproceedings{singlaExperimentalStudyBig20212021IEEE37thInt.Conf.DataEng.ICDE,
  title = {Experimental {{Study}} of {{Big Raster}} and {{Vector Database Systems}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Singla, Samriddhi and Eldawy, Ahmed and Diao, Tina and Mukhopadhyay, Ayan and Scudiero, Elia},
  year = {2021},
  month = apr,
  pages = {2243--2248},
  publisher = {IEEE},
  address = {Chania, Greece},
  doi = {10.1109/ICDE51399.2021.00231},
  urldate = {2024-09-29},
  abstract = {Spatial data is traditionally represented using two data models, raster and vector. Raster data refers to satellite imagery while vector data includes GPS data, Tweets, and regional boundaries. While there are many real-world applications that need to process both raster and vector data concurrently, state-of-the-art systems are limited to processing one of these two representations while converting the other one which limits their scalability. This paper draws the attention of the research community to the research problems that emerge from the concurrent processing of raster and vector data. It describes three real-world applications and explains their computation and access patterns for raster and vector data. Additionally, it runs an extensive experimental evaluation using state-of-the-art big spatial data systems with raster data of up-to a trillion pixels, and vector data with up-to hundreds of millions of edges. The results show that while most systems can analyze raster and vector concurrently, but they have limited scalability for largescale data.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72819-184-3},
  langid = {english},
  keywords = {Unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper describes three real-world applications and explains their computation and access patterns for raster and vector data and runs an extensive experimental evaluation using state-of-the-art big spatial data systems with raster data of up-to a trillion pixels, andvector data with up- to hundreds of millions of edges.},
  file = {/Users/janwardenga/Zotero/storage/2C5BVHEE/ICDE_2021_Experimental Study of Big Raster and Vector Database Systems.pdf}
}

@article{sinoaraKnowledgeenhancedDocumentEmbeddings2019Knowledge-BasedSystems,
  title = {Knowledge-Enhanced Document Embeddings for Text Classification},
  author = {Sinoara, Roberta A. and {Camacho-Collados}, Jose and Rossi, Rafael G. and Navigli, Roberto and Rezende, Solange O.},
  year = {2019},
  month = jan,
  journal = {Knowledge-Based Systems},
  volume = {163},
  pages = {955--971},
  issn = {09507051},
  doi = {10.1016/j.knosys.2018.10.026},
  urldate = {2024-09-29},
  abstract = {Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of wordand word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.},
  langid = {english},
  keywords = {Jab/KS,Unread},
  annotation = {112 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/3NCSPPEE/KS_2019_Knowledge-enhanced document embeddings for text classification.pdf}
}

@article{smeatonProgressApplicationNatural1992TheComputerJournal,
  title = {Progress in the {{Application}} of {{Natural Language Processing}} to {{Information Retrieval Tasks}}},
  author = {Smeaton, A. F.},
  year = {1992},
  month = jun,
  journal = {The Computer Journal},
  volume = {35},
  number = {3},
  pages = {268--278},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/35.3.268},
  urldate = {2024-09-29},
  langid = {english},
  keywords = {Jab/CJ,Read},
  annotation = {150 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper will examine and review recent progress in using the lexical, syntactic, semantic and discourse levels of the language analysis for tasks like automatic and semi-automatic indexing of text, text retrieval, text abstracting and summarisation, thesaurus generation from text corpus and conceptual information retrieval.},
  file = {/Users/janwardenga/Zotero/storage/GSMFR2ML/CJ_1992_Progress in the Application of Natural Language Processing to Information Retrieval Tasks.pdf}
}

@article{songSemanticQueryGraph2019ClusterComput,
  title = {Semantic Query Graph Based {{SPARQL}} Generation from Natural Language Questions},
  author = {Song, Shengli and Huang, Wen and Sun, Yulong},
  year = {2019},
  month = jan,
  journal = {Cluster Computing},
  volume = {22},
  number = {S1},
  pages = {847--858},
  issn = {1386-7857, 1573-7543},
  doi = {10.1007/s10586-017-1332-3},
  urldate = {2025-02-20},
  abstract = {In order to precisely represent natural language questions (NLQs) in question answering system (QAS) and provide a more naturally interactive mode, we require SPARQL, a formalized query patterns, instead of search expression to express the user's semantic query intention. However, how to generate and evaluate SPARQL query from NLQ is a mostly open research question. In this paper, we propose a framework that can help users translating NLQ into well-formed queries for knowledge based systems. We define a new graph structure, semantic query graph, and vocabulary to match all kinds of complex and compound questions without using domain ontology. Through query expansion and semantic query graph generation, the framework resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Extensive experiments over NLQ in real RDF QASs verify the feasibility and efficiency of semantic query graph and our proposed framework with average F-measure of 0.825.},
  langid = {english},
  keywords = {Jab/CC,To read,Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-20]\\
0 citations (Semantic Scholar/DOI) [2025-02-20]\\
TLDR: A new graph structure, semantic query graph, and vocabulary to match all kinds of complex and compound questions without using domain ontology is defined and the framework resembles subgraphs of the knowledge base and can be directly mapped to a logical form.},
  file = {/Users/janwardenga/Zotero/storage/WBT9RNFR/CC_2019_Semantic query graph based SPARQL generation from natural language questions.pdf}
}

@incollection{steenwinckelWalkExtractionStrategies2021DatabaseandExpertSystemsApplications-DEXA2021Workshops,
  title = {Walk {{Extraction Strategies}} for {{Node Embeddings}} with {{RDF2Vec}} in {{Knowledge Graphs}}},
  booktitle = {Database and {{Expert Systems Applications}} - {{DEXA}} 2021 {{Workshops}}},
  author = {Steenwinckel, Bram and Vandewiele, Gilles and Bonte, Pieter and Weyns, Michael and Paulheim, Heiko and Ristoski, Petar and De Turck, Filip and Ongenae, Femke},
  editor = {Kotsis, Gabriele and Tjoa, A Min and Khalil, Ismail and Moser, Bernhard and Mashkoor, Atif and Sametinger, Johannes and Fensel, Anna and {Martinez-Gil}, Jorge and Fischer, Lukas and Czech, Gerald and Sobieczky, Florian and Khan, Sohail},
  year = {2021},
  volume = {1479},
  pages = {70--80},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-87101-7_8},
  urldate = {2024-09-29},
  abstract = {As Knowledge Graphs are symbolic constructs, specialized techniques have to be applied in order to make them compatible with data mining techniques. RDF2Vec is an unsupervised technique that can create task-agnostic numerical representations of the nodes in a KG by extending successful language modeling techniques. The original work proposed the Weisfeiler-Lehman kernel to improve the quality of the representations. However, in this work, we show that the Weisfeiler-Lehman kernel does little to improve walk embeddings in the context of a single Knowledge Graph. As an alternative, we examined five alternative strategies to extract information complementary to basic random walks and compare them on several benchmark datasets to show that research within this field is still relevant for node classification tasks.},
  isbn = {978-3-030-87100-0 978-3-030-87101-7},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/Z82PU3N6/Book_2021_Walk Extraction Strategies for Node Embeddings with RDF2Vec in Knowledge Graphs.pdf}
}

@article{strzalkowskiNaturalLanguageInformation1995InformationProcessing&Management,
  title = {Natural Language Information Retrieval},
  author = {Strzalkowski, Tomek},
  year = {1995},
  month = may,
  journal = {Information Processing \& Management},
  volume = {31},
  number = {3},
  pages = {397--417},
  issn = {03064573},
  doi = {10.1016/0306-4573(94)00055-8},
  urldate = {2024-09-29},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Read},
  file = {/Users/janwardenga/Zotero/storage/KGT2NTPY/Strzalkowski - 1995 - Natural language information retrieval.pdf;/Users/janwardenga/Zotero/storage/ZKAM4MLP/Pre__Natural Language Processing in Information Retrieval.pdf}
}

@misc{suKnowledgeGraphBased2024,
  title = {Knowledge {{Graph Based Agent}} for {{Complex}}, {{Knowledge-Intensive QA}} in {{Medicine}}},
  author = {Su, Xiaorui and Wang, Yibo and Gao, Shanghua and Liu, Xiaolong and Giunchiglia, Valentina and Clevert, Djork-Arn{\'e} and Zitnik, Marinka},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04660},
  eprint = {2410.04660},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2410.04660},
  urldate = {2024-11-05},
  abstract = {Biomedical knowledge is uniquely complex and structured, requiring distinct reasoning strategies compared to other scientific disciplines like physics or chemistry. Biomedical scientists do not rely on a single approach to reasoning; instead, they use various strategies, including rule-based, prototype-based, and casebased reasoning. This diversity calls for flexible approaches that accommodate multiple reasoning strategies while leveraging in-domain knowledge. We introduce KGAREVION, a knowledge graph (KG) based agent designed to address the complexity of knowledge-intensive medical queries. Upon receiving a query, KGAREVION generates relevant triplets by using the knowledge base of the LLM. These triplets are then verified against a grounded KG to filter out erroneous information and ensure that only accurate, relevant data contribute to the final answer. Unlike RAG-based models, this multi-step process ensures robustness in reasoning while adapting to different models of medical reasoning. Evaluations on four gold-standard medical QA datasets show that KGAREVION improves accuracy by over 5.2\%, outperforming 15 models in handling complex medical questions. To test its capabilities, we curated three new medical QA datasets with varying levels of semantic complexity, where KGAREVION achieved a 10.4\% improvement in accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Unread},
  annotation = {1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/FHQM9294/Su et al. - 2024 - Knowledge Graph Based Agent for Complex, Knowledge-Intensive QA in Medicine.pdf}
}

@article{sun2020faithful,
  title = {Faithful Embeddings for Knowledge Base Queries},
  author = {Sun, Haitian and Arnold, Andrew and Bedrax Weiss, Tania and Pereira, Fernando and Cohen, William W},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {22505--22516},
  keywords = {,No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/V5RTFJ2G/Pre__Faithful Embeddings for Knowledge Base Queries.pdf}
}

@misc{sunConsistencyGuidedKnowledge2024,
  title = {Consistency {{Guided Knowledge Retrieval}} and {{Denoising}} in {{LLMs}} for {{Zero-shot Document-level Relation Triplet Extraction}}},
  author = {Sun, Qi and Huang, Kun and Yang, Xiaocui and Tong, Rong and Zhang, Kun and Poria, Soujanya},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13598},
  eprint = {2401.13598},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2401.13598},
  urldate = {2024-09-29},
  abstract = {Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in information systems that aims to simultaneously extract entities with semantic relations from a document. Existing methods heavily rely on a substantial amount of fully labeled data. However, collecting and annotating data for newly emerging relations is time-consuming and labor-intensive. Recent advanced Large Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text generation capabilities, inspiring us to explore an alternative approach for obtaining auto-labeled documents with new relations. In this paper, we propose a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework, which generates labeled data by retrieval and denoising knowledge from LLMs, called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide ChatGPT to generate labeled long-text data step by step. To improve the quality of synthetic data, we propose a denoising strategy based on the consistency of cross-document knowledge. Leveraging our denoised synthetic data, we proceed to fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets. We perform experiments for both zero-shot document-level relation and triplet extraction on two public datasets. The experimental results illustrate that our GenRDK framework outperforms strong baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {17 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/KULQWNJ7/Pre_2024_Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction.pdf}
}

@article{sureshRAGbasedSummarizationElectron2024J.Inst.,
  title = {Towards a {{RAG-based}} Summarization for the {{Electron Ion Collider}}},
  author = {Suresh, Karthik and Kackar, Neeltje and Schleck, Luke and Fanelli, Cristiano},
  year = {2024},
  month = jul,
  journal = {Journal of Instrumentation},
  volume = {19},
  number = {07},
  pages = {C07006},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/19/07/C07006},
  urldate = {2024-09-29},
  abstract = {Abstract                            The complexity and sheer volume of information --- encompassing documents, papers, data, and other resources --- from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)-based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses. Furthermore, we describe the concept of prompt template based instruction-tuning which provides flexibility and accuracy in summarization. Importantly, the implementation relies on LangChain~[1], which serves as the foundation of our entire workflow. This integration ensures efficiency and scalability, facilitating smooth deployment and accessibility for various user groups within the Electron Ion Collider (EIC) community. This innovative AI-driven framework not only simplifies the understanding of vast datasets but also encourages collaborative participation, thereby empowering researchers. As a demonstration, a web application has been developed to explain each stage of the RAG Agent development in detail. The application can be accessed at               https://rags4eic-ai4eic.streamlit.app               .[A tagged version of the source code can be found in               https://github.com/ai4eic/EIC-RAG-Project/releases/tag/AI4EIC2023\_PROCEEDING               .]},
  langid = {english},
  keywords = {Jab/JI,Unread},
  annotation = {0 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work describes the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to assess the effectiveness of responses, and describes the concept of prompt template based instruction-tuning which provides flexibility and accuracy in summarization.},
  file = {/Users/janwardenga/Zotero/storage/JB9CWSCJ/JI_2024_Towards a RAG-based summarization for the Electron Ion Collider.pdf}
}

@article{taipalusVectorDatabaseManagement2024CognitiveSystemsResearch,
  title = {Vector Database Management Systems: {{Fundamental}} Concepts, Use-Cases, and Current Challenges},
  shorttitle = {Vector Database Management Systems},
  author = {Taipalus, Toni},
  year = {2024},
  month = jun,
  journal = {Cognitive Systems Research},
  volume = {85},
  pages = {101216},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2024.101216},
  urldate = {2024-09-29},
  abstract = {Vector database management systems have emerged as an important component in modern data management, driven by the growing importance for the need to computationally describe rich data such as texts, images and video in various domains such as recommender systems, similarity search, and chatbots. These data descriptions are captured as numerical vectors that are computationally inexpensive to store and compare. However, the unique characteristics of vectorized data, including high dimensionality and sparsity, demand specialized solutions for efficient storage, retrieval, and processing. This narrative literature review provides an accessible introduction to the fundamental concepts, use-cases, and current challenges associated with vector database management systems, offering an overview for researchers and practitioners seeking to facilitate effective vector data management.},
  langid = {english},
  keywords = {Jab/CSR,Unread},
  annotation = {17 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/MEQV77K7/CSR_2024_Vector database management systems.pdf}
}

@article{theatlascollaborationSearchInvisibleHiggsboson2022J.HighEnerg.Phys.,
  title = {Search for Invisible {{Higgs-boson}} Decays in Events with Vector-Boson Fusion Signatures Using 139 Fb-1 of Proton-Proton Data Recorded by the {{ATLAS}} Experiment},
  author = {{The ATLAS collaboration} and Aad, G. and Abbott, B. and Abbott, D. C. and Abed Abud, A. and Abeling, K. and Abhayasinghe, D. K. and Abidi, S. H. and Aboulhorma, A. and Abramowicz, H. and Abreu, H. and Abulaiti, Y. and Abusleme Hoffman, A. C. and Acharya, B. S. and Achkar, B. and Adam, L. and Adam Bourdarios, C. and Adamczyk, L. and Adamek, L. and Addepalli, S. V. and Adelman, J. and Adiguzel, A. and Adorni, S. and Adye, T. and Affolder, A. A. and Afik, Y. and Agaras, M. N. and Agarwala, J. and Aggarwal, A. and Agheorghiesei, C. and {Aguilar-Saavedra}, J. A. and Ahmad, A. and Ahmadov, F. and Ahmed, W. S. and Ai, X. and Aielli, G. and Aizenberg, I. and Akbiyik, M. and {\AA}kesson, T. P. A. and Akimov, A. V. and Al Khoury, K. and Alberghi, G. L. and Albert, J. and Albicocco, P. and Alconada Verzini, M. J. and Alderweireldt, S. and Aleksa, M. and Aleksandrov, I. N. and Alexa, C. and Alexopoulos, T. and Alfonsi, A. and Alfonsi, F. and Alhroob, M. and Ali, B. and Ali, S. and Aliev, M. and Alimonti, G. and Allaire, C. and Allbrooke, B. M. M. and Allport, P. P. and Aloisio, A. and Alonso, F. and Alpigiani, C. and Alunno Camelia, E. and Alvarez Estevez, M. and Alviggi, M. G. and Amaral Coutinho, Y. and Ambler, A. and Ambroz, L. and Amelung, C. and Amidei, D. and Amor Dos Santos, S. P. and Amoroso, S. and Amos, K. R. and Amrouche, C. S. and Ananiev, V. and Anastopoulos, C. and Andari, N. and Andeen, T. and Anders, J. K. and Andrean, S. Y. and Andreazza, A. and Angelidakis, S. and Angerami, A. and Anisenkov, A. V. and Annovi, A. and Antel, C. and Anthony, M. T. and Antipov, E. and Antonelli, M. and Antrim, D. J. A. and Anulli, F. and Aoki, M. and Aparisi Pozo, J. A. and Aparo, M. A. and Aperio Bella, L. and Appelt, C. and Aranzabal, N. and Araujo Ferraz, V. and Arcangeletti, C. and Arce, A. T. H. and Arena, E. and Arguin, J-F. and Argyropoulos, S. and Arling, J.-H. and Armbruster, A. J. and Arnaez, O. and Arnold, H. and Arrubarrena Tame, Z. P. and Artoni, G. and Asada, H. and Asai, K. and Asai, S. and Asbah, N. A. and Asimakopoulou, E. M. and Assahsah, J. and Assamagan, K. and Astalos, R. and Atkin, R. J. and Atkinson, M. and Atlay, N. B. and Atmani, H. and Atmasiddha, P. A. and Augsten, K. and Auricchio, S. and Austrup, V. A. and Avner, G. and Avolio, G. and Ayoub, M. K. and Azuelos, G. and Babal, D. and Bachacou, H. and Bachas, K. and Bachiu, A. and Backman, F. and Badea, A. and Bagnaia, P. and Bahmani, M. and Bailey, A. J. and Bailey, V. R. and Baines, J. T. and Bakalis, C. and Baker, O. K. and Bakker, P. J. and Bakos, E. and Bakshi Gupta, D. and Balaji, S. and Balasubramanian, R. and Baldin, E. M. and Balek, P. and Ballabene, E. and Balli, F. and Baltes, L. M. and Balunas, W. K. and Balz, J. and Banas, E. and Bandieramonte, M. and Bandyopadhyay, A. and Bansal, S. and Barak, L. and Barberio, E. L. and Barberis, D. and Barbero, M. and Barbour, G. and Barends, K. N. and Barillari, T. and Barisits, M-S. and Barkeloo, J. and Barklow, T. and Barnett, R. M. and Baron, P. and Baroncelli, A. and Barone, G. and Barr, A. J. and Barranco Navarro, L. and Barreiro, F. and Barreiro Guimar{\~a}es Da Costa, J. and Barron, U. and Barsov, S. and Bartels, F. and Bartoldus, R. and Bartolini, G. and Barton, A. E. and Bartos, P. and Basalaev, A. and Basan, A. and Baselga, M. and Bashta, I. and Bassalat, A. and Basso, M. J. and Basson, C. R. and Bates, R. L. and Batlamous, S. and Batley, J. R. and Batool, B. and Battaglia, M. and Bauce, M. and Bauer, F. and Bauer, P. and Bayirli, A. and Beacham, J. B. and Beau, T. and Beauchemin, P. H. and Becherer, F. and Bechtle, P. and Beck, H. P. and Becker, K. and Becot, C. and Beddall, A. J. and Bednyakov, V. A. and Bee, C. P. and Beemster, L. J. and Beermann, T. A. and Begalli, M. and Begel, M. and Behera, A. and Behr, J. K. and Beirao Da Cruz E Silva, C. and Beirer, J. F. and Beisiegel, F. and Belfkir, M. and Bella, G. and Bellagamba, L. and Bellerive, A. and Bellos, P. and Beloborodov, K. and Belotskiy, K. and Belyaev, N. L. and Benchekroun, D. and Benhammou, Y. and Benjamin, D. P. and Benoit, M. and Bensinger, J. R. and Bentvelsen, S. and Beresford, L. and Beretta, M. and Berge, D. and Bergeaas Kuutmann, E. and Berger, N. and Bergmann, B. and Beringer, J. and Berlendis, S. and Bernardi, G. and Bernius, C. and Bernlochner, F. U. and Berry, T. and Berta, P. and Bertram, I. A. and Bessidskaia Bylund, O. and Bethke, S. and Betti, A. and Bevan, A. J. and Bhatta, S. and Bhattacharya, D. S. and Bhattarai, P. and Bhopatkar, V. S. and Bi, R. and Bi, R. and Bianchi, R. M. and Biebel, O. and Bielski, R. and Biesuz, N. V. and Biglietti, M. and Billoud, T. R. V. and Bindi, M. and Bingul, A. and Bini, C. and Biondi, S. and Biondini, A. and {Birch-sykes}, C. J. and Bird, G. A. and Birman, M. and Bisanz, T. and Biswal, J. P. and Biswas, D. and Bitadze, A. and Bj{\o}rke, K. and Bloch, I. and Blocker, C. and Blue, A. and Blumenschein, U. and Blumenthal, J. and Bobbink, G. J. and Bobrovnikov, V. S. and Boehler, M. and Bogavac, D. and Bogdanchikov, A. G. and Bohm, C. and Boisvert, V. and Bokan, P. and Bold, T. and Bomben, M. and Bona, M. and Boonekamp, M. and Booth, C. D. and Borb{\'e}ly, A. G. and {Borecka-Bielska}, H. M. and Borgna, L. S. and Borissov, G. and Bortoletto, D. and Boscherini, D. and Bosman, M. and Bossio Sola, J. D. and Bouaouda, K. and Boudreau, J. and {Bouhova-Thacker}, E. V. and Boumediene, D. and Bouquet, R. and Boveia, A. and Boyd, J. and Boye, D. and Boyko, I. R. and Bracinik, J. and Brahimi, N. and Brandt, G. and Brandt, O. and Braren, F. and Brau, B. and Brau, J. E. and Breaden Madden, W. D. and Brendlinger, K. and Brener, R. and Brenner, L. and Brenner, R. and Bressler, S. and Brickwedde, B. and Britton, D. and Britzger, D. and Brock, I. and Brooijmans, G. and Brooks, W. K. and Brost, E. and Bruckman De Renstrom, P. A. and Br{\"u}ers, B. and Bruncko, D. and Bruni, A. and Bruni, G. and Bruschi, M. and Bruscino, N. and Bryngemark, L. and Buanes, T. and Buat, Q. and Buchholz, P. and Buckley, A. G. and Budagov, I. A. and Bugge, M. K. and Bulekov, O. and Bullard, B. A. and Burdin, S. and Burgard, C. D. and Burger, A. M. and Burghgrave, B. and Burr, J. T. P. and Burton, C. D. and Burzynski, J. C. and Busch, E. L. and B{\"u}scher, V. and Bussey, P. J. and Butler, J. M. and Buttar, C. M. and Butterworth, J. M. and Buttinger, W. and Buxo Vazquez, C. J. and Buzykaev, A. R. and Cabras, G. and Cabrera Urb{\'a}n, S. and Caforio, D. and Cai, H. and Cai, Y. and Cairo, V. M. M. and Cakir, O. and Calace, N. and Calafiura, P. and Calderini, G. and Calfayan, P. and Callea, G. and Caloba, L. P. and Calvet, D. and Calvet, S. and Calvet, T. P. and Calvetti, M. and Camacho Toro, R. and Camarda, S. and Camarero Munoz, D. and Camarri, P. and Camerlingo, M. T. and Cameron, D. and Camincher, C. and Campanelli, M. and Camplani, A. and Canale, V. and Canesse, A. and Cano Bret, M. and Cantero, J. and Cao, Y. and Capocasa, F. and Capua, M. and Carbone, A. and Cardarelli, R. and Cardenas, J. C. J. and Cardillo, F. and Carducci, G. and Carli, T. and Carlino, G. and Carlson, B. T. and Carlson, E. M. and Carminati, L. and Carnesale, M. and Caron, S. and Carquin, E. and Carr{\'a}, S. and Carratta, G. and Carter, J. W. S. and Carter, T. M. and Casadei, D. and Casado, M. P. and Casha, A. F. and Castiglia, E. G. and Castillo, F. L. and Castillo Garcia, L. and Castillo Gimenez, V. and Castro, N. F. and Catinaccio, A. and Catmore, J. R. and Cavaliere, V. and Cavalli, N. and Cavasinni, V. and Celebi, E. and Celli, F. and Centonze, M. S. and Cerny, K. and Cerqueira, A. S. and Cerri, A. and Cerrito, L. and Cerutti, F. and Cervelli, A. and Cetin, S. A. and Chadi, Z. and Chakraborty, D. and Chala, M. and Chan, J. and Chan, W. S. and Chan, W. Y. and Chapman, J. D. and Chargeishvili, B. and Charlton, D. G. and Charman, T. P. and Chatterjee, M. and Chekanov, S. and Chekulaev, S. V. and Chelkov, G. A. and Chen, A. and Chen, B. and Chen, B. and Chen, C. and Chen, H. and Chen, H. and Chen, J. and Chen, J. and Chen, S. and Chen, S. J. and Chen, X. and Chen, X. and Chen, Y. and Cheng, C. L. and Cheng, H. C. and Cheplakov, A. and Cheremushkina, E. and Cherepanova, E. and Cherkaoui El Moursli, R. and Cheu, E. and Cheung, K. and Chevalier, L. and Chiarella, V. and Chiarelli, G. and Chiodini, G. and Chisholm, A. S. and Chitan, A. and Chiu, Y. H. and Chizhov, M. V. and Choi, K. and Chomont, A. R. and Chou, Y. and Chow, E. Y. S. and Chowdhury, T. and Christopher, L. D. and Chu, M. C. and Chu, X. and Chudoba, J. and Chwastowski, J. J. and Cieri, D. and Ciesla, K. M. and Cindro, V. and Ciocio, A. and Cirotto, F. and Citron, Z. H. and Citterio, M. and Ciubotaru, D. A. and Ciungu, B. M. and Clark, A. and Clark, P. J. and Clavijo Columbie, J. M. and Clawson, S. E. and Clement, C. and Clissa, L. and Coadou, Y. and Cobal, M. and Coccaro, A. and Coelho Barrue, R. F. and Coelho Lopes De Sa, R. and Coelli, S. and Cohen, H. and Coimbra, A. E. C. and Cole, B. and Collot, J. and Conde Mui{\~n}o, P. and Connell, S. H. and Connelly, I. A. and Conroy, E. I. and Conventi, F. and Cooke, H. G. and {Cooper-Sarkar}, A. M. and Cormier, F. and Corpe, L. D. and Corradi, M. and Corrigan, E. E. and Corriveau, F. and Costa, M. J. and Costanza, F. and Costanzo, D. and Cote, B. M. and Cowan, G. and Cowley, J. W. and Cranmer, K. and {Cr{\'e}p{\'e}-Renaudin}, S. and Crescioli, F. and Cristinziani, M. and Cristoforetti, M. and Croft, V. and Crosetti, G. and Cueto, A. and Cuhadar Donszelmann, T. and Cui, H. and Cui, Z. and Cukierman, A. R. and Cunningham, W. R. and Curcio, F. and Czodrowski, P. and Czurylo, M. M. and Da Cunha Sargedas De Sousa, M. J. and Da Fonseca Pinto, J. V. and Da Via, C. and Dabrowski, W. and Dado, T. and Dahbi, S. and Dai, T. and Dallapiccola, C. and Dam, M. and D'amen, G. and D'Amico, V. and Damp, J. and Dandoy, J. R. and Daneri, M. F. and Danninger, M. and Dao, V. and Darbo, G. and Darmora, S. and Dattagupta, A. and D'Auria, S. and David, C. and Davidek, T. and Davis, D. R. and {Davis-Purcell}, B. and Dawson, I. and De, K. and De Asmundis, R. and De Beurs, M. and De Castro, S. and De Groot, N. and De Jong, P. and De La Torre, H. and De Maria, A. and De Salvo, A. and De Sanctis, U. and De Santis, M. and De Santo, A. and De Vivie De Regie, J. B. and Dedovich, D. V. and Degens, J. and Deiana, A. M. and Del Peso, J. and Del Rio, F. and Deliot, F. and Delitzsch, C. M. and Della Pietra, M. and Della Volpe, D. and Dell'Acqua, A. and Dell'Asta, L. and Delmastro, M. and Delsart, P. A. and Demers, S. and Demichev, M. and Denisov, S. P. and D'Eramo, L. and Derendarz, D. and Derue, F. and Dervan, P. and Desch, K. and Dette, K. and Deutsch, C. and Deviveiros, P. O. and Di Bello, F. A. and Di Ciaccio, A. and Di Ciaccio, L. and Di Domenico, A. and Di Donato, C. and Di Girolamo, A. and Di Gregorio, G. and Di Luca, A. and Di Micco, B. and Di Nardo, R. and Diaconu, C. and Dias, F. A. and Dias Do Vale, T. and Diaz, M. A. and Diaz Capriles, F. G. and Didenko, M. and Diehl, E. B. and D{\'i}ez Cornell, S. and Diez Pardos, C. and Dimitriadi, C. and Dimitrievska, A. and Ding, W. and Dingfelder, J. and Dinu, I-M. and Dittmeier, S. J. and Dittus, F. and Djama, F. and Djobava, T. and Djuvsland, J. I. and Dodsworth, D. and Doglioni, C. and Dolejsi, J. and Dolezal, Z. and Donadelli, M. and Dong, B. and Donini, J. and D'onofrio, A. and D'Onofrio, M. and Dopke, J. and Doria, A. and Dova, M. T. and Doyle, A. T. and Drechsler, E. and Dreyer, E. and Drobac, A. S. and Du, D. and Du Pree, T. A. and Dubinin, F. and Dubovsky, M. and Duchovni, E. and Duckeck, G. and Ducu, O. A. and Duda, D. and Dudarev, A. and D'uffizi, M. and Duflot, L. and D{\"u}hrssen, M. and D{\"u}lsen, C. and Dumitriu, A. E. and Dunford, M. and Dungs, S. and Dunne, K. and Duperrin, A. and Duran Yildiz, H. and D{\"u}ren, M. and Durglishvili, A. and Dutta, B. and Dwyer, B. L. and Dyckes, G. I. and Dyndal, M. and Dysch, S. and Dziedzic, B. S. and Eckerova, B. and Eggleston, M. G. and Egidio Purcino De Souza, E. and Ehrke, L. F. and Eigen, G. and Einsweiler, K. and Ekelof, T. and El Ghazali, Y. and El Jarrari, H. and El Moussaouy, A. and Ellajosyula, V. and Ellert, M. and Ellinghaus, F. and Elliot, A. A. and Ellis, N. and Elmsheuser, J. and Elsing, M. and Emeliyanov, D. and Emerman, A. and Enari, Y. and Ene, I. and Erdmann, J. and Ereditato, A. and Erland, P. A. and Errenst, M. and Escalier, M. and Escobar, C. and Etzion, E. and Evans, G. and Evans, H. and Evans, M. O. and Ezhilov, A. and Ezzarqtouni, S. and Fabbri, F. and Fabbri, L. and Facini, G. and Fadeyev, V. and Fakhrutdinov, R. M. and Falciano, S. and Falke, P. J. and Falke, S. and Faltova, J. and Fan, Y. and Fang, Y. and Fanourakis, G. and Fanti, M. and Faraj, M. and Farbin, A. and Farilla, A. and Farooque, T. and Farrington, S. M. and Fassi, F. and Fassouliotis, D. and Faucci Giannelli, M. and Fawcett, W. J. and Fayard, L. and Fedin, O. L. and Fedotov, G. and Feickert, M. and Feligioni, L. and Fell, A. and Fellers, D. E. and Feng, C. and Feng, M. and Fenton, M. J. and Fenyuk, A. B. and Ferguson, S. W. and Fernandez Pretel, J. A. and Ferrando, J. and Ferrari, A. and Ferrari, P. and Ferrari, R. and Ferrere, D. and Ferretti, C. and Fiedler, F. and Filip{\v c}i{\v c}, A. and Filmer, E. K. and Filthaut, F. and Fiolhais, M. C. N. and Fiorini, L. and Fischer, F. and Fisher, W. C. and Fitschen, T. and Fleck, I. and Fleischmann, P. and Flick, T. and Flores, L. and Flores, M. and Flores Castillo, L. R. and Follega, F. M. and Fomin, N. and Foo, J. H. and Forland, B. C. and Formica, A. and Forti, A. C. and Fortin, E. and Fortman, A. W. and Foti, M. G. and Fountas, L. and Fournier, D. and Fox, H. and Francavilla, P. and Francescato, S. and Franchini, M. and Franchino, S. and Francis, D. and Franco, L. and Franconi, L. and Franklin, M. and Frattari, G. and Freegard, A. C. and Freeman, P. M. and Freund, W. S. and Freundlich, E. M. and Froidevaux, D. and Frost, J. A. and Fu, Y. and Fujimoto, M. and Fullana Torregrosa, E. and Fuster, J. and Gabrielli, A. and Gabrielli, A. and Gadow, P. and Gagliardi, G. and Gagnon, L. G. and Gallardo, G. E. and Gallas, E. J. and Gallop, B. J. and Gamboa Goni, R. and Gan, K. K. and Ganguly, S. and Gao, J. and Gao, Y. and Garay Walls, F. M. and Garcia, B. and Garc{\'i}a, C. and Garc{\'i}a Navarro, J. E. and Garc{\'i}a Pascual, J. A. and {Garcia-Sciveres}, M. and Gardner, R. W. and Garg, D. and Garg, R. B. and Gargiulo, S. and Garner, C. A. and Garonne, V. and Gasiorowski, S. J. and Gaspar, P. and Gaudio, G. and Gauzzi, P. and Gavrilenko, I. L. and Gavrilyuk, A. and Gay, C. and Gaycken, G. and Gazis, E. N. and Geanta, A. A. and Gee, C. M. and Geisen, J. and Geisen, M. and Gemme, C. and Genest, M. H. and Gentile, S. and George, S. and George, W. F. and Geralis, T. and Gerlach, L. O. and {Gessinger-Befurt}, P. and Ghasemi Bostanabad, M. and Ghosal, A. and Ghosh, A. and Ghosh, A. and Giacobbe, B. and Giagu, S. and Giangiacomi, N. and Giannetti, P. and Giannini, A. and Gibson, S. M. and Gignac, M. and Gil, D. T. and Gilbert, B. J. and Gillberg, D. and Gilles, G. and Gillwald, N. E. K. and Ginabat, L. and Gingrich, D. M. and Giordani, M. P. and Giraud, P. F. and Giugliarelli, G. and Giugni, D. and Giuli, F. and Gkialas, I. and Gkountoumis, P. and Gladilin, L. K. and Glasman, C. and Gledhill, G. R. and Glisic, M. and Gnesi, I. and Go, Y. and {Goblirsch-Kolb}, M. and Godin, D. and Goldfarb, S. and Golling, T. and Gololo, M. G. D. and Golubkov, D. and Gombas, J. P. and Gomes, A. and Gomez Delegido, A. J. and Goncalves Gama, R. and Gon{\c c}alo, R. and Gonella, G. and Gonella, L. and Gongadze, A. and Gonnella, F. and Gonski, J. L. and Gonz{\'a}lez De La Hoz, S. and Gonzalez Fernandez, S. and Gonzalez Lopez, R. and Gonzalez Renteria, C. and Gonzalez Suarez, R. and {Gonzalez-Sevilla}, S. and Gonzalvo Rodriguez, G. R. and Gonz{\'a}lez Andana, R. Y. and Goossens, L. and Gorasia, N. A. and Gorbounov, P. A. and Gordon, H. A. and Gorini, B. and Gorini, E. and Gori{\v s}ek, A. and Goshaw, A. T. and Gostkin, M. I. and Gottardo, C. A. and Gouighri, M. and Goumarre, V. and Goussiou, A. G. and Govender, N. and Goy, C. and {Grabowska-Bold}, I. and Graham, K. and Gramstad, E. and Grancagnolo, S. and Grandi, M. and Gratchev, V. and Gravila, P. M. and Gravili, F. G. and Gray, H. M. and Grefe, C. and Gregor, I. M. and Grenier, P. and Grevtsov, K. and Grieco, C. and Grillo, A. A. and Grimm, K. and Grinstein, S. and Grivaz, J.-F. and Groh, S. and Gross, E. and {Grosse-Knetter}, J. and Grud, C. and Grummer, A. and Grundy, J. C. and Guan, L. and Guan, W. and Gubbels, C. and Guerrero Rojas, J. G. R. and Guescini, F. and Guest, D. and Gugel, R. and Guida, A. and Guillemin, T. and Guindon, S. and Guo, F. and Guo, J. and Guo, L. and Guo, Y. and Gupta, R. and Gurbuz, S. and Gustavino, G. and Guth, M. and Gutierrez, P. and Gutierrez Zagazeta, L. F. and Gutschow, C. and Guyot, C. and Gwenlan, C. and Gwilliam, C. B. and Haaland, E. S. and Haas, A. and Habedank, M. and Haber, C. and Hadavand, H. K. and Hadef, A. and Hadzic, S. and Haleem, M. and Haley, J. and Hall, J. J. and Hallewell, G. D. and Halser, L. and Hamano, K. and Hamdaoui, H. and Hamer, M. and Hamity, G. N. and Han, J. and Han, K. and Han, L. and Han, L. and Han, S. and Han, Y. F. and Hanagaki, K. and Hance, M. and Hangal, D. A. and Hank, M. D. and Hankache, R. and Hansen, E. and Hansen, J. B. and Hansen, J. D. and Hansen, P. H. and Hara, K. and Harada, D. and Harenberg, T. and Harkusha, S. and Harris, Y. T. and Harrison, P. F. and Hartman, N. M. and Hartmann, N. M. and Hasegawa, Y. and Hasib, A. and Haug, S. and Hauser, R. and Havranek, M. and Hawkes, C. M. and Hawkings, R. J. and Hayashida, S. and Hayden, D. and Hayes, C. and Hayes, R. L. and Hays, C. P. and Hays, J. M. and Hayward, H. S. and He, F. and He, Y. and He, Y. and Heath, M. P. and Hedberg, V. and Heggelund, A. L. and Hehir, N. D. and Heidegger, C. and Heidegger, K. K. and Heidorn, W. D. and Heilman, J. and Heim, S. and Heim, T. and Heinemann, B. and Heinlein, J. G. and Heinrich, J. J. and Heinrich, L. and Hejbal, J. and Helary, L. and Held, A. and Helling, C. M. and Hellman, S. and Helsens, C. and Henderson, R. C. W. and Henkelmann, L. and Henriques Correia, A. M. and Herde, H. and Hern{\'a}ndez Jim{\'e}nez, Y. and Herr, H. and Herrmann, M. G. and Herrmann, T. and Herten, G. and Hertenberger, R. and Hervas, L. and Hessey, N. P. and Hibi, H. and {Hig{\'o}n-Rodriguez}, E. and Hillier, S. J. and Hinchliffe, I. and Hinterkeuser, F. and Hirose, M. and Hirose, S. and Hirschbuehl, D. and Hiti, B. and Hladik, O. and Hobbs, J. and Hobincu, R. and Hod, N. and Hodgkinson, M. C. and Hodkinson, B. H. and Hoecker, A. and Hofer, J. and Hohn, D. and Holm, T. and Holzbock, M. and Hommels, L. B. A. H. and Honan, B. P. and Hong, J. and Hong, T. M. and Hong, Y. and Honig, J. C. and H{\"o}nle, A. and Hooberman, B. H. and Hopkins, W. H. and Horii, Y. and Horyn, L. A. and Hou, S. and Howarth, J. and Hoya, J. and Hrabovsky, M. and Hrynevich, A. and Hryn'ova, T. and Hsu, P. J. and Hsu, S.-C. and Hu, Q. and Hu, S. and Hu, Y. F. and Huang, D. P. and Huang, X. and Huang, Y. and Huang, Y. and Hubacek, Z. and Huebner, M. and Huegging, F. and Huffman, T. B. and Huhtinen, M. and Huiberts, S. K. and Hulsken, R. and Huseynov, N. and Huston, J. and Huth, J. and Hyneman, R. and Hyrych, S. and Iacobucci, G. and Iakovidis, G. and Ibragimov, I. and {Iconomidou-Fayard}, L. and Iengo, P. and Iguchi, R. and Iizawa, T. and Ikegami, Y. and Ilg, A. and Ilic, N. and Imam, H. and Ingebretsen Carlson, T. and Introzzi, G. and Iodice, M. and Ippolito, V. and Ishino, M. and Islam, W. and Issever, C. and Istin, S. and Ito, H. and Iturbe Ponce, J. M. and Iuppa, R. and Ivina, A. and Izen, J. M. and Izzo, V. and Jacka, P. and Jackson, P. and Jacobs, R. M. and Jaeger, B. P. and Jagfeld, C. S. and J{\"a}kel, G. and Jakobs, K. and Jakoubek, T. and Jamieson, J. and Janas, K. W. and Jarlskog, G. and Jaspan, A. E. and Jav{\r u}rek, T. and Javurkova, M. and Jeanneau, F. and Jeanty, L. and Jejelava, J. and Jenni, P. and J{\'e}z{\'e}quel, S. and Jia, J. and Jia, X. and Jia, Z. and Jiang, Y. and Jiggins, S. and Jimenez Pena, J. and Jin, S. and Jinaru, A. and Jinnouchi, O. and Jivan, H. and Johansson, P. and Johns, K. A. and Johnson, C. A. and Jones, D. M. and Jones, E. and Jones, R. W. L. and Jones, T. J. and Jovicevic, J. and Ju, X. and Junggeburth, J. J. and Juste Rozas, A. and Kabana, S. and Kaczmarska, A. and Kado, M. and Kagan, H. and Kagan, M. and Kahn, A. and Kahn, A. and Kahra, C. and Kaji, T. and Kajomovitz, E. and Kakati, N. and Kalderon, C. W. and Kamenshchikov, A. and Kang, N. J. and Kano, Y. and Kar, D. and Karava, K. and Kareem, M. J. and Karentzos, E. and Karkanias, I. and Karpov, S. N. and Karpova, Z. M. and Kartvelishvili, V. and Karyukhin, A. N. and Kasimi, E. and Kato, C. and Katzy, J. and Kaur, S. and Kawade, K. and Kawagoe, K. and Kawaguchi, T. and Kawamoto, T. and Kawamura, G. and Kay, E. F. and Kaya, F. I. and Kazakos, S. and Kazanin, V. F. and Ke, Y. and Keaveney, J. M. and Keeler, R. and Kehris, G. V. and Keller, J. S. and Kelly, A. S. and Kelsey, D. and Kempster, J. J. and Kendrick, J. and Kennedy, K. E. and Kepka, O. and Kersten, S. and Ker{\v s}evan, B. P. and Ketabchi Haghighat, S. and Khandoga, M. and Khanov, A. and Kharlamov, A. G. and Kharlamova, T. and Khoda, E. E. and Khoo, T. J. and Khoriauli, G. and Khramov, E. and Khubua, J. and Kiehn, M. and Kilgallon, A. and Kim, E. and Kim, Y. K. and Kimura, N. and Kirchhoff, A. and Kirchmeier, D. and Kirfel, C. and Kirk, J. and Kiryunin, A. E. and Kishimoto, T. and Kisliuk, D. P. and Kitsaki, C. and Kivernyk, O. and Klassen, M. and Klein, C. and Klein, L. and Klein, M. H. and Klein, M. and Klein, U. and Klimek, P. and Klimentov, A. and Klimpel, F. and Klingl, T. and Klioutchnikova, T. and Klitzner, F. F. and Kluit, P. and Kluth, S. and Kneringer, E. and Knight, T. M. and Knue, A. and Kobayashi, D. and Kobayashi, R. and Kocian, M. and Kodama, T. and Kodys, P. and Koeck, D. M. and Koenig, P. T. and Koffas, T. and K{\"o}hler, N. M. and Kolb, M. and Koletsou, I. and Komarek, T. and K{\"o}neke, K. and Kong, A. X. Y. and Kono, T. and Konstantinidis, N. and Konya, B. and Kopeliansky, R. and Koperny, S. and Korcyl, K. and Kordas, K. and Koren, G. and Korn, A. and Korn, S. and Korolkov, I. and Korotkova, N. and Kortman, B. and Kortner, O. and Kortner, S. and Kostecka, W. H. and Kostyukhin, V. V. and Kotsokechagia, A. and Kotwal, A. and Koulouris, A. and {Kourkoumeli-Charalampidi}, A. and Kourkoumelis, C. and Kourlitis, E. and Kovanda, O. and Kowalewski, R. and Kozanecki, W. and Kozhin, A. S. and Kramarenko, V. A. and Kramberger, G. and Kramer, P. and Krasny, M. W. and Krasznahorkay, A. and Kremer, J. A. and Kretzschmar, J. and Kreul, K. and Krieger, P. and Krieter, F. and Krishnamurthy, S. and Krishnan, A. and Krivos, M. and Krizka, K. and Kroeninger, K. and Kroha, H. and Kroll, J. and Kroll, J. and Krowpman, K. S. and Kruchonak, U. and Kr{\"u}ger, H. and Krumnack, N. and Kruse, M. C. and Krzysiak, J. A. and Kubota, A. and Kuchinskaia, O. and Kuday, S. and Kuechler, D. and Kuechler, J. T. and Kuehn, S. and Kuhl, T. and Kukhtin, V. and Kulchitsky, Y. and Kuleshov, S. and Kumar, M. and Kumari, N. and Kuna, M. and Kupco, A. and Kupfer, T. and Kuprash, O. and Kurashige, H. and Kurchaninov, L. L. and Kurochkin, Y. A. and Kurova, A. and Kuwertz, E. S. and Kuze, M. and Kvam, A. K. and Kvita, J. and Kwan, T. and Kwok, K. W. and Lacasta, C. and Lacava, F. and Lacker, H. and Lacour, D. and Lad, N. N. and Ladygin, E. and Laforge, B. and Lagouri, T. and Lai, S. and Lakomiec, I. K. and Lalloue, N. and Lambert, J. E. and Lammers, S. and Lampl, W. and Lampoudis, C. and Lan{\c c}on, E. and Landgraf, U. and Landon, M. P. J. and Lang, V. S. and Lange, J. C. and Langenberg, R. J. and Lankford, A. J. and Lanni, F. and Lantzsch, K. and Lanza, A. and Lapertosa, A. and Laporte, J. F. and Lari, T. and Lasagni Manghi, F. and Lassnig, M. and Latonova, V. and Lau, T. S. and Laudrain, A. and Laurier, A. and Lavorgna, M. and Lawlor, S. D. and Lawrence, Z. and Lazzaroni, M. and Le, B. and Leban, B. and Lebedev, A. and LeBlanc, M. and LeCompte, T. and {Ledroit-Guillon}, F. and Lee, A. C. A. and Lee, G. R. and Lee, L. and Lee, S. C. and Leeuw, L. L. and Lefebvre, B. and Lefebvre, H. P. and Lefebvre, M. and Leggett, C. and Lehmann, K. and Lehmann Miotto, G. and Leight, W. A. and Leisos, A. and Leite, M. A. L. and Leitgeb, C. E. and Leitner, R. and Leney, K. J. C. and Lenz, T. and Leone, S. and Leonidopoulos, C. and Leopold, A. and Leroy, C. and Les, R. and Lester, C. G. and Levchenko, M. and Lev{\^e}que, J. and Levin, D. and Levinson, L. J. and Lewis, D. J. and Li, B. and Li, B. and Li, C. and Li, C-Q. and Li, H. and Li, H. and Li, H. and Li, J. and Li, K. and Li, L. and Li, M. and Li, Q. Y. and Li, S. and Li, T. and Li, X. and Li, Z. and Li, Z. and Li, Z. and Li, Z. and Liang, Z. and Liberatore, M. and Liberti, B. and Lie, K. and Lieber Marin, J. and Lin, K. and Linck, R. A. and Lindley, R. E. and Lindon, J. H. and Linss, A. and Lipeles, E. and Lipniacka, A. and Liss, T. M. and Lister, A. and Little, J. D. and Liu, B. and Liu, B. X. and Liu, D. and Liu, J. B. and Liu, J. K. K. and Liu, K. and Liu, M. and Liu, M. Y. and Liu, P. and Liu, Q. and Liu, X. and Liu, Y. and Liu, Y. and Liu, Y. L. and Liu, Y. W. and Livan, M. and Llorente Merino, J. and Lloyd, S. L. and Lobodzinska, E. M. and Loch, P. and Loffredo, S. and Lohse, T. and Lohwasser, K. and Lokajicek, M. and Long, J. D. and Longarini, I. and Longo, L. and Longo, R. and Lopez Paz, I. and Lopez Solis, A. and Lorenz, J. and Lorenzo Martinez, N. and Lory, A. M. and L{\"o}sle, A. and Lou, X. and Lou, X. and Lounis, A. and Love, J. and Love, P. A. and Lozano Bahilo, J. J. and Lu, G. and Lu, M. and Lu, S. and Lu, Y. J. and Lubatti, H. J. and Luci, C. and Lucio Alves, F. L. and Lucotte, A. and Luehring, F. and Luise, I. and Lundberg, O. and {Lund-Jensen}, B. and Luongo, N. A. and Lutz, M. S. and Lynn, D. and Lyons, H. and Lysak, R. and Lytken, E. and Lyu, F. and Lyubushkin, V. and Lyubushkina, T. and Ma, H. and Ma, L. L. and Ma, Y. and Mac Donell, D. M. and Maccarrone, G. and MacDonald, J. C. and Madar, R. and Mader, W. F. and Maeda, J. and Maeno, T. and Maerker, M. and Magerl, V. and Magro, J. and Mahon, D. J. and Maidantchik, C. and Maio, A. and Maj, K. and Majersky, O. and Majewski, S. and Makovec, N. and Maksimovic, V. and Malaescu, B. and Malecki, {\relax Pa}. and Maleev, V. P. and Malek, F. and Malito, D. and Mallik, U. and Malone, C. and Maltezos, S. and Malyukov, S. and Mamuzic, J. and Mancini, G. and Mandalia, J. P. and Mandi{\'c}, I. and Manhaes De Andrade Filho, L. and Maniatis, I. M. and Manisha, M. and Manjarres Ramos, J. and Mankad, D. C. and Mankinen, K. H. and Mann, A. and Manousos, A. and Mansoulie, B. and Manzoni, S. and Marantis, A. and Marchiori, G. and Marcisovsky, M. and Marcoccia, L. and Marcon, C. and Marinescu, M. and Marjanovic, M. and Marshall, Z. and {Marti-Garcia}, S. and Martin, T. A. and Martin, V. J. and Martin Dit Latour, B. and Martinelli, L. and Martinez, M. and Martinez Agullo, P. and Martinez Outschoorn, V. I. and Martinez Suarez, P. and {Martin-Haugh}, S. and Martoiu, V. S. and Martyniuk, A. C. and Marzin, A. and Maschek, S. R. and Masetti, L. and Mashimo, T. and Masik, J. and Maslennikov, A. L. and Massa, L. and Massarotti, P. and Mastrandrea, P. and Mastroberardino, A. and Masubuchi, T. and Mathisen, T. and Matic, A. and Matsuzawa, N. and Maurer, J. and Ma{\v c}ek, B. and Maximov, D. A. and Mazini, R. and Maznas, I. and Mazza, M. and Mazza, S. M. and Mc Ginn, C. and Mc Gowan, J. P. and Mc Kee, S. P. and McCarthy, T. G. and McCormack, W. P. and McDonald, E. F. and McDougall, A. E. and Mcfayden, J. A. and Mchedlidze, G. and McKay, M. A. and Mckenzie, R. P. and Mclaughlin, D. J. and McLean, K. D. and McMahon, S. J. and McNamara, P. C. and McPherson, R. A. and Mdhluli, J. E. and Meehan, S. and Megy, T. and Mehlhase, S. and Mehta, A. and Meirose, B. and Melini, D. and Mellado Garcia, B. R. and Melo, A. H. and Meloni, F. and Melzer, A. and Mendes Gouveia, E. D. and Mendes Jacques Da Costa, A. M. and Meng, H. Y. and Meng, L. and Menke, S. and Mentink, M. and Meoni, E. and Merlassino, C. and Merola, L. and Meroni, C. and Merz, G. and Meshkov, O. and Meshreki, J. K. R. and Metcalfe, J. and Mete, A. S. and Meyer, C. and Meyer, J-P. and Michetti, M. and Middleton, R. P. and Mijovi{\'c}, L. and Mikenberg, G. and Mikestikova, M. and Miku{\v z}, M. and Mildner, H. and Milic, A. and Milke, C. D. and Miller, D. W. and Miller, L. S. and Milov, A. and Milstead, D. A. and Min, T. and Minaenko, A. A. and Minashvili, I. A. and Mince, L. and Mincer, A. I. and Mindur, B. and Mineev, M. and Minegishi, Y. and Mino, Y. and Mir, L. M. and Miralles Lopez, M. and Mironova, M. and Mitani, T. and Mitra, A. and Mitsou, V. A. and Miu, O. and Miyagawa, P. S. and Miyazaki, Y. and Mizukami, A. and Mj{\"o}rnmark, J. U. and Mkrtchyan, T. and Mlynarikova, M. and Moa, T. and Mobius, S. and Mochizuki, K. and Moder, P. and Mogg, P. and Mohammed, A. F. and Mohapatra, S. and Mokgatitswane, G. and Mondal, B. and Mondal, S. and M{\"o}nig, K. and Monnier, E. and Monsonis Romero, L. and Montejo Berlingen, J. and Montella, M. and Monticelli, F. and Morange, N. and Moreira De Carvalho, A. L. and Moreno Ll{\'a}cer, M. and Moreno Martinez, C. and Morettini, P. and Morgenstern, S. and Mori, D. and Morii, M. and Morinaga, M. and Morisbak, V. and Morley, A. K. and Morvaj, L. and Moschovakos, P. and Moser, B. and Mosidze, M. and Moskalets, T. and Moskvitina, P. and Moss, J. and Moyse, E. J. W. and Muanza, S. and Mueller, J. and Mueller, R. and Muenstermann, D. and Mullier, G. A. and Mullin, J. J. and Mungo, D. P. and Munoz Martinez, J. L. and Munoz Sanchez, F. J. and Murin, M. and Murray, W. J. and Murrone, A. and Muse, J. M. and Mu{\v s}kinja, M. and Mwewa, C. and Myagkov, A. G. and Myers, A. J. and Myers, A. A. and Myers, G. and Myska, M. and Nachman, B. P. and Nackenhorst, O. and Nag Nag, A. and Nagai, K. and Nagano, K. and Nagle, J. L. and Nagy, E. and Nairz, A. M. and Nakahama, Y. and Nakamura, K. and Nanjo, H. and Narayan, R. and Narayanan, E. A. and Naryshkin, I. and Naseri, M. and Nass, C. and Navarro, G. and {Navarro-Gonzalez}, J. and Nayak, R. and Nechaeva, P. Y. and Nechansky, F. and Neep, T. J. and Negri, A. and Negrini, M. and Nellist, C. and Nelson, C. and Nelson, K. and Nemecek, S. and Nessi, M. and Neubauer, M. S. and Neuhaus, F. and Neundorf, J. and Newhouse, R. and Newman, P. R. and Ng, C. W. and Ng, Y. S. and Ng, Y. W. Y. and Ngair, B. and Nguyen, H. D. N. and Nickerson, R. B. and Nicolaidou, R. and Nielsen, D. S. and Nielsen, J. and Niemeyer, M. and Nikiforou, N. and Nikolaenko, V. and {Nikolic-Audit}, I. and Nikolopoulos, K. and Nilsson, P. and Nindhito, H. R. and Nisati, A. and Nishu, N. and Nisius, R. and Noacco Rosende, S. J. and Nobe, T. and Noel, D. L. and Noguchi, Y. and Nomidis, I. and Nomura, M. A. and Norfolk, M. B. and Norisam, R. R. B. and Novak, J. and Novak, T. and Novgorodova, O. and Novotny, L. and Novotny, R. and Nozka, L. and Ntekas, K. and Nurse, E. and Oakham, F. G. and Ocariz, J. and Ochi, A. and Ochoa, I. and {Ochoa-Ricoux}, J. P. and Oda, S. and Oerdek, S. and Ogrodnik, A. and Oh, A. and Ohm, C. C. and Oide, H. and Oishi, R. and Ojeda, M. L. and Okazaki, Y. and O'Keefe, M. W. and Okumura, Y. and Olariu, A. and Oleiro Seabra, L. F. and Olivares Pino, S. A. and Oliveira Damazio, D. and Oliveira Goncalves, D. and Oliver, J. L. and Olsson, M. J. R. and Olszewski, A. and Olszowska, J. and {\"O}ncel, {\"O}. O. and O'Neil, D. C. and O'neill, A. P. and Onofre, A. and Onyisi, P. U. E. and Oreamuno Madriz, R. G. and Oreglia, M. J. and Orellana, G. E. and Orestano, D. and Orlando, N. and Orr, R. S. and O'Shea, V. and Ospanov, R. and Otero Y Garzon, G. and Otono, H. and Ott, P. S. and Ottino, G. J. and Ouchrif, M. and Ouellette, J. and {Ould-Saada}, F. and Owen, M. and Owen, R. E. and Oyulmaz, K. Y. and Ozcan, V. E. and Ozturk, N. and Ozturk, S. and Pacalt, J. and Pacey, H. A. and Pachal, K. and Pacheco Pages, A. and Padilla Aranda, C. and Pagan Griso, S. and Palacino, G. and Palazzo, S. and Palestini, S. and Palka, M. and Pan, J. and Panchal, D. K. and Pandini, C. E. and Panduro Vazquez, J. G. and Pani, P. and Panizzo, G. and Paolozzi, L. and Papadatos, C. and Parajuli, S. and Paramonov, A. and Paraskevopoulos, C. and Paredes Hernandez, D. and Parida, B. and Park, T. H. and Parker, A. J. and Parker, M. A. and Parodi, F. and Parrish, E. W. and Parrish, V. A. and Parsons, J. A. and Parzefall, U. and Pascual Dias, B. and Pascual Dominguez, L. and Pascuzzi, V. R. and Pasquali, F. and Pasqualucci, E. and Passaggio, S. and Pastore, F. and Pasuwan, P. and Pater, J. R. and Pathak, A. and Patton, J. and Pauly, T. and Pearkes, J. and Pedersen, M. and Pedro, R. and Peleganchuk, S. V. and Penc, O. and Peng, C. and Peng, H. and Penzin, M. and Peralva, B. S. and Pereira Peixoto, A. P. and Pereira Sanchez, L. and Perepelitsa, D. V. and Perez Codina, E. and Perganti, M. and Perini, L. and Pernegger, H. and Perrella, S. and Perrevoort, A. and Perrin, O. and Peters, K. and Peters, R. F. Y. and Petersen, B. A. and Petersen, T. C. and Petit, E. and Petousis, V. and Petridou, C. and Petrukhin, A. and Pettee, M. and Pettersson, N. E. and Petukhova, K. and Peyaud, A. and Pezoa, R. and Pezzotti, L. and Pezzullo, G. and Pham, T. and Phillips, P. W. and Phipps, M. W. and Piacquadio, G. and Pianori, E. and Piazza, F. and Piegaia, R. and Pietreanu, D. and Pilkington, A. D. and Pinamonti, M. and Pinfold, J. L. and Pitman Donaldson, C. and Pizzi, D. A. and Pizzimento, L. and Pizzini, A. and Pleier, M.-A. and Plesanovs, V. and Pleskot, V. and Plotnikova, E. and Poddar, G. and Poettgen, R. and Poggi, R. and Poggioli, L. and Pogrebnyak, I. and Pohl, D. and Pokharel, I. and Polacek, S. and Polesello, G. and Poley, A. and Polifka, R. and Polini, A. and Pollard, C. S. and Pollock, Z. B. and Polychronakos, V. and Ponomarenko, D. and Pontecorvo, L. and Popa, S. and Popeneciu, G. A. and Portillo Quintero, D. M. and Pospisil, S. and Postolache, P. and Potamianos, K. and Potrap, I. N. and Potter, C. J. and Potti, H. and Poulsen, T. and Poveda, J. and Pownall, G. and Pozo Astigarraga, M. E. and Prades Ibanez, A. and Pralavorio, P. and Prapa, M. M. and Price, D. and Primavera, M. and Principe Martin, M. A. and Proffitt, M. L. and Proklova, N. and Prokofiev, K. and Prokoshin, F. and Proto, G. and Protopopescu, S. and Proudfoot, J. and Przybycien, M. and Pudzha, D. and Puzo, P. and Pyatiizbyantseva, D. and Qian, J. and Qin, Y. and Qiu, T. and Quadt, A. and {Queitsch-Maitland}, M. and Rabanal Bolanos, G. and Rafanoharana, D. and Ragusa, F. and Raine, J. A. and Rajagopalan, S. and Ran, K. and Raskina, V. and Rassloff, D. F. and Rave, S. and Ravina, B. and Ravinovich, I. and Raymond, M. and Read, A. L. and Readioff, N. P. and Rebuzzi, D. M. and Redlinger, G. and Reeves, K. and Reikher, D. and Reiss, A. and Rej, A. and Rembser, C. and Renardi, A. and Renda, M. and Rendel, M. B. and Rennie, A. G. and Resconi, S. and Ressegotti, M. and Resseguie, E. D. and Rettie, S. and Reynolds, B. and Reynolds, E. and Rezaei Estabragh, M. and Rezanova, O. L. and Reznicek, P. and Ricci, E. and Richter, R. and Richter, S. and {Richter-Was}, E. and Ridel, M. and Rieck, P. and Riedler, P. and Rifki, O. and Rijssenbeek, M. and Rimoldi, A. and Rimoldi, M. and Rinaldi, L. and Rinn, T. T. and Rinnagel, M. P. and Ripellino, G. and Riu, I. and Rivadeneira, P. and Rivera Vergara, J. C. and Rizatdinova, F. and Rizvi, E. and Rizzi, C. and Roberts, B. A. and Roberts, B. R. and Robertson, S. H. and Robin, M. and Robinson, D. and Robles Gajardo, C. M. and Robles Manzano, M. and Robson, A. and Rocchi, A. and Roda, C. and Rodriguez Bosca, S. and Rodriguez Garcia, Y. and Rodriguez Rodriguez, A. and Rodr{\'i}guez Vera, A. M. and Roe, S. and Roemer, J. T. and Roepe, A. R. and Roggel, J. and R{\o}hne, O. and Rojas, R. A. and Roland, B. and Roland, C. P. A. and Roloff, J. and Romaniouk, A. and Romano, M. and Romero Hernandez, A. C. and Rompotis, N. and Ronzani, M. and Roos, L. and Rosati, S. and Rosser, B. J. and Rossi, E. and Rossi, E. and Rossi, L. P. and Rossini, L. and Rosten, R. and Rotaru, M. and Rottler, B. and Rousseau, D. and Rousso, D. and Rovelli, G. and Roy, A. and Rozanov, A. and Rozen, Y. and Ruan, X. and Ruby, A. J. and Ruggeri, T. A. and R{\"u}hr, F. and {Ruiz-Martinez}, A. and Rummler, A. and Rurikova, Z. and Rusakovich, N. A. and Russell, H. L. and Rustige, L. and Rutherfoord, J. P. and R{\"u}ttinger, E. M. and Rybacki, K. and Rybar, M. and Rye, E. B. and Ryzhov, A. and Sabater Iglesias, J. A. and Sabatini, P. and Sabetta, L. and Sadrozinski, H. F-W. and Sadykov, R. and Safai Tehrani, F. and Safarzadeh Samani, B. and Safdari, M. and Saha, S. and Sahinsoy, M. and Sahu, A. and Saimpert, M. and Saito, M. and Saito, T. and Salamani, D. and Salamanna, G. and Salnikov, A. and Salt, J. and Salvador Salas, A. and Salvatore, D. and Salvatore, F. and Salzburger, A. and Sammel, D. and Sampsonidis, D. and Sampsonidou, D. and S{\'a}nchez, J. and Sanchez Pineda, A. and Sanchez Sebastian, V. and Sandaker, H. and Sander, C. O. and Sanderswood, I. G. and Sandesara, J. A. and Sandhoff, M. and Sandoval, C. and Sankey, D. P. C. and Sansoni, A. and Santoni, C. and Santos, H. and Santpur, S. N. and Santra, A. and Saoucha, K. A. and Sapronov, A. and Saraiva, J. G. and Sardain, J. and Sasaki, O. and Sato, K. and Sauer, C. and Sauerburger, F. and Sauvan, E. and Savard, P. and Sawada, R. and Sawyer, C. and Sawyer, L. and Sayago Galvan, I. and Sbarra, C. and Sbrizzi, A. and Scanlon, T. and Schaarschmidt, J. and Schacht, P. and Schaefer, D. and Sch{\"a}fer, U. and Schaffer, A. C. and Schaile, D. and Schamberger, R. D. and Schanet, E. and Scharf, C. and Scharmberg, N. and Schegelsky, V. A. and Scheirich, D. and Schenck, F. and Schernau, M. and Scheulen, C. and Schiavi, C. and Schillaci, Z. M. and Schioppa, E. J. and Schioppa, M. and Schlag, B. and Schleicher, K. E. and Schlenker, S. and Schmieden, K. and Schmitt, C. and Schmitt, S. and Schoeffel, L. and Schoening, A. and Scholer, P. G. and Schopf, E. and Schott, M. and Schovancova, J. and Schramm, S. and Schroeder, F. and {Schultz-Coulon}, H-C. and Schumacher, M. and Schumm, B. A. and Schune, {\relax Ph}. and Schwartzman, A. and Schwarz, T. A. and Schwemling, {\relax Ph}. and Schwienhorst, R. and Sciandra, A. and Sciolla, G. and Scuri, F. and Scutti, F. and Sebastiani, C. D. and Sedlaczek, K. and Seema, P. and Seidel, S. C. and Seiden, A. and Seidlitz, B. D. and Seiss, T. and Seitz, C. and Seixas, J. M. and Sekhniaidze, G. and Sekula, S. J. and Selem, L. and {Semprini-Cesari}, N. and Sen, S. and Senthilkumar, V. and Serin, L. and Serkin, L. and Sessa, M. and Severini, H. and Sevova, S. and Sforza, F. and Sfyrla, A. and Shabalina, E. and Shaheen, R. and Shahinian, J. D. and Shaikh, N. W. and Shaked Renous, D. and Shan, L. Y. and Shapiro, M. and Sharma, A. and Sharma, A. S. and Sharma, S. and Shatalov, P. B. and Shaw, K. and Shaw, S. M. and Sherwood, P. and Shi, L. and Shimmin, C. O. and Shimogama, Y. and Shinner, J. D. and Shipsey, I. P. J. and Shirabe, S. and Shiyakova, M. and Shlomi, J. and Shochet, M. J. and Shojaii, J. and Shope, D. R. and Shrestha, S. and Shrif, E. M. and Shroff, M. J. and Sicho, P. and Sickles, A. M. and Sideras Haddad, E. and Sidiropoulou, O. and Sidoti, A. and Siegert, F. and Sijacki, {\relax Dj}. and Sili, F. and Silva, J. M. and Silva Oliveira, M. V. and Silverstein, S. B. and Simion, S. and Simoniello, R. and Simpson, E. L. and Simpson, N. D. and Simsek, S. and Sindhu, S. and Sinervo, P. and Sinetckii, V. and Singh, S. and Singh, S. and Sinha, S. and Sinha, S. and Sioli, M. and Siral, I. and Sivoklokov, S. {\relax Yu}. and Sj{\"o}lin, J. and Skaf, A. and Skorda, E. and Skubic, P. and Slawinska, M. and Smakhtin, V. and Smart, B. H. and Smiesko, J. and Smirnov, S. {\relax Yu}. and Smirnov, Y. and Smirnova, L. N. and Smirnova, O. and Smith, E. A. and Smith, H. A. and Smith, R. and Smizanska, M. and Smolek, K. and Smykiewicz, A. and Snesarev, A. A. and Snoek, H. L. and Snyder, S. and Sobie, R. and Soffer, A. and Solans Sanchez, C. A. and Soldatov, E. {\relax Yu}. and Soldevila, U. and Solodkov, A. A. and Solomon, S. and Soloshenko, A. and Solovieva, K. and Solovyanov, O. V. and Solovyev, V. and Sommer, P. and Son, H. and Sonay, A. and Song, W. Y. and Sopczak, A. and Sopio, A. L. and Sopkova, F. and Sothilingam, V. and Sottocornola, S. and Soualah, R. and Soukharev, A. M. and Soumaimi, Z. and South, D. and Spagnolo, S. and Spalla, M. and Spangenberg, M. and Span{\`o}, F. and Sperlich, D. and Spigo, G. and Spina, M. and Spinali, S. and Spiteri, D. P. and Spousta, M. and Staats, E. J. and Stabile, A. and Stamen, R. and Stamenkovic, M. and Stampekis, A. and Standke, M. and Stanecka, E. and Stanislaus, B. and Stanitzki, M. M. and Stankaityte, M. and Stapf, B. and Starchenko, E. A. and Stark, G. H. and Stark, J. and Starko, D. M. and Staroba, P. and Starovoitov, P. and St{\"a}rz, S. and Staszewski, R. and Stavropoulos, G. and Steentoft, J. and Steinberg, P. and Steinhebel, A. L. and Stelzer, B. and Stelzer, H. J. and {Stelzer-Chilton}, O. and Stenzel, H. and Stevenson, T. J. and Stewart, G. A. and Stockton, M. C. and Stoicea, G. and Stolarski, M. and Stonjek, S. and Straessner, A. and Strandberg, J. and Strandberg, S. and Strauss, M. and Strebler, T. and Strizenec, P. and Str{\"o}hmer, R. and Strom, D. M. and Strom, L. R. and Stroynowski, R. and Strubig, A. and Stucci, S. A. and Stugu, B. and Stupak, J. and Styles, N. A. and Su, D. and Su, S. and Su, W. and Su, X. and Sugizaki, K. and Sulin, V. V. and Sullivan, M. J. and Sultan, D. M. S. and Sultanaliyeva, L. and Sultansoy, S. and Sumida, T. and Sun, S. and Sun, S. and Sunneborn Gudnadottir, O. and Sutton, M. R. and Svatos, M. and Swiatlowski, M. and Swirski, T. and Sykora, I. and Sykora, M. and Sykora, T. and Ta, D. and Tackmann, K. and Taffard, A. and Tafirout, R. and Taibah, R. H. M. and Takashima, R. and Takeda, K. and Takeva, E. P. and Takubo, Y. and Talby, M. and Talyshev, A. A. and Tam, K. C. and Tamir, N. M. and Tanaka, A. and Tanaka, J. and Tanaka, R. and Tang, J. and Tao, Z. and Tapia Araya, S. and Tapprogge, S. and Tarek Abouelfadl Mohamed, A. and Tarem, S. and Tariq, K. and Tarna, G. and Tartarelli, G. F. and Tas, P. and Tasevsky, M. and Tassi, E. and Tateno, G. and Tayalati, Y. and Taylor, G. N. and Taylor, W. and Teagle, H. and Tee, A. S. and Teixeira De Lima, R. and {Teixeira-Dias}, P. and Teoh, J. J. and Terashi, K. and Terron, J. and Terzo, S. and Testa, M. and Teuscher, R. J. and Themistokleous, N. and {Theveneaux-Pelzer}, T. and Thielmann, O. and Thomas, D. W. and Thomas, J. P. and Thompson, E. A. and Thompson, P. D. and Thomson, E. and Thorpe, E. J. and Tian, Y. and Tikhomirov, V. and Tikhonov, {\relax Yu}. A. and Timoshenko, S. and Ting, E. X. L. and Tipton, P. and Tisserant, S. and Tlou, S. H. and Tnourji, A. and Todome, K. and {Todorova-Nova}, S. and Todt, S. and Togawa, M. and Tojo, J. and Tok{\'a}r, S. and Tokushuku, K. and Tombs, R. and Tomoto, M. and Tompkins, L. and Tornambe, P. and Torrence, E. and Torres, H. and Torr{\'o} Pastor, E. and Toscani, M. and Tosciri, C. and Tovey, D. R. and Traeet, A. and Trandafir, I. S. and Treado, C. J. and Trefzger, T. and Tricoli, A. and Trigger, I. M. and {Trincaz-Duvoid}, S. and Trischuk, D. A. and Trischuk, W. and Trocm{\'e}, B. and Trofymov, A. and Troncon, C. and Trovato, F. and Truong, L. and Trzebinski, M. and Trzupek, A. and Tsai, F. and Tsai, M. and Tsiamis, A. and Tsiareshka, P. V. and Tsirigotis, A. and Tsiskaridze, V. and Tskhadadze, E. G. and Tsopoulou, M. and Tsujikawa, Y. and Tsukerman, I. I. and Tsulaia, V. and Tsuno, S. and Tsur, O. and Tsybychev, D. and Tu, Y. and Tudorache, A. and Tudorache, V. and Tuna, A. N. and Turchikhin, S. and Turk Cakir, I. and Turra, R. and Tuts, P. M. and Tzamarias, S. and Tzanis, P. and Tzovara, E. and Uchida, K. and Ukegawa, F. and Ulloa Poblete, P. A. and Unal, G. and Unal, M. and Undrus, A. and Unel, G. and Uno, K. and Urban, J. and Urquijo, P. and Usai, G. and Ushioda, R. and Usman, M. and Uysal, Z. and Vacek, V. and Vachon, B. and Vadla, K. O. H. and Vafeiadis, T. and Valderanis, C. and Valdes Santurio, E. and Valente, M. and Valentinetti, S. and Valero, A. and Vallier, A. and Valls Ferrer, J. A. and Van Daalen, T. R. and Van Gemmeren, P. and Van Stroud, S. and Van Vulpen, I. and Vanadia, M. and Vandelli, W. and Vandenbroucke, M. and Vandewall, E. R. and Vannicola, D. and Vannoli, L. and Vari, R. and Varnes, E. W. and Varni, C. and Varol, T. and Varouchas, D. and Varvell, K. E. and Vasile, M. E. and Vaslin, L. and Vasquez, G. A. and Vazeille, F. and Vazquez Furelos, D. and Vazquez Schroeder, T. and Veatch, J. and Vecchio, V. and Veen, M. J. and Veliscek, I. and Veloce, L. M. and Veloso, F. and Veneziano, S. and Ventura, A. and Verbytskyi, A. and Verducci, M. and Vergis, C. and Verissimo De Araujo, M. and Verkerke, W. and Vermeulen, J. C. and Vernieri, C. and Verschuuren, P. J. and Vessella, M. and Vesterbacka, M. L. and Vetterli, M. C. and Vgenopoulos, A. and Viaux Maira, N. and Vickey, T. and Vickey Boeriu, O. E. and Viehhauser, G. H. A. and Vigani, L. and Villa, M. and Villaplana Perez, M. and Villhauer, E. M. and Vilucchi, E. and Vincter, M. G. and Virdee, G. S. and Vishwakarma, A. and Vittori, C. and Vivarelli, I. and Vladimirov, V. and Voevodina, E. and Vogel, M. and Vokac, P. and Von Ahnen, J. and Von Toerne, E. and Vormwald, B. and Vorobel, V. and Vorobev, K. and Vos, M. and Vossebeld, J. H. and Vozak, M. and Vozdecky, L. and Vranjes, N. and Vranjes Milosavljevic, M. and Vrba, V. and Vreeswijk, M. and Vu, N. K. and Vuillermet, R. and Vujinovic, O. V. and Vukotic, I. and Wada, S. and Wagner, C. and Wagner, W. and Wahdan, S. and Wahlberg, H. and Wakasa, R. and Wakida, M. and Walbrecht, V. M. and Walder, J. and Walker, R. and Walkowiak, W. and Wang, A. M. and Wang, A. Z. and Wang, C. and Wang, C. and Wang, H. and Wang, J. and Wang, P. and Wang, R.-J. and Wang, R. and Wang, R. and Wang, S. M. and Wang, S. and Wang, T. and Wang, W. T. and Wang, W. X. and Wang, X. and Wang, X. and Wang, X. and Wang, Y. and Wang, Z. and Wang, Z. and Wang, Z. and Warburton, A. and Ward, R. J. and Warrack, N. and Watson, A. T. and Watson, M. F. and Watts, G. and Waugh, B. M. and Webb, A. F. and Weber, C. and Weber, M. S. and Weber, S. A. and Weber, S. M. and Wei, C. and Wei, Y. and Weidberg, A. R. and Weingarten, J. and Weirich, M. and Weiser, C. and Wenaus, T. and Wendland, B. and Wengler, T. and Wenke, N. S. and Wermes, N. and Wessels, M. and Whalen, K. and Wharton, A. M. and White, A. S. and White, A. and White, M. J. and Whiteson, D. and Wickremasinghe, L. and Wiedenmann, W. and Wiel, C. and Wielers, M. and Wieseotte, N. and Wiglesworth, C. and {Wiik-Fuchs}, L. A. M. and Wilbern, D. J. and Wilkens, H. G. and Williams, D. M. and Williams, H. H. and Williams, S. and Willocq, S. and Windischhofer, P. J. and Winklmeier, F. and Winter, B. T. and Wittgen, M. and Wobisch, M. and Wolf, A. and W{\"o}lker, R. and Wollrath, J. and Wolter, M. W. and Wolters, H. and Wong, V. W. S. and Wongel, A. F. and Worm, S. D. and Wosiek, B. K. and Wo{\'z}niak, K. W. and Wraight, K. and Wu, J. and Wu, S. L. and Wu, X. and Wu, Y. and Wu, Z. and Wuerzinger, J. and Wyatt, T. R. and Wynne, B. M. and Xella, S. and Xia, L. and Xia, M. and Xiang, J. and Xiao, X. and Xie, M. and Xie, X. and Xiotidis, I. and Xu, D. and Xu, H. and Xu, H. and Xu, L. and Xu, R. and Xu, T. and Xu, W. and Xu, Y. and Xu, Z. and Xu, Z. and Yabsley, B. and Yacoob, S. and Yamaguchi, N. and Yamaguchi, Y. and Yamauchi, H. and Yamazaki, T. and Yamazaki, Y. and Yan, J. and Yan, S. and Yan, Z. and Yang, H. J. and Yang, H. T. and Yang, S. and Yang, T. and Yang, X. and Yang, X. and Yang, Y. and Yang, Z. and Yao, W-M. and Yap, Y. C. and Ye, H. and Ye, J. and Ye, S. and Ye, X. and Yeletskikh, I. and Yexley, M. R. and Yin, P. and Yorita, K. and Young, C. J. S. and Young, C. and Yuan, M. and Yuan, R. and Yue, X. and Zaazoua, M. and Zabinski, B. and Zacharis, G. and Zaid, E. and Zaitsev, A. M. and Zakareishvili, T. and Zakharchuk, N. and Zambito, S. and Zanzi, D. and Zaplatilek, O. and Zei{\ss}ner, S. V. and Zeitnitz, C. and Zeng, J. C. and Zenger, D. T. and Zenin, O. and {\v Z}eni{\v s}, T. and Zenz, S. and Zerradi, S. and Zerwas, D. and Zhang, B. and Zhang, D. F. and Zhang, G. and Zhang, J. and Zhang, K. and Zhang, L. and Zhang, M. and Zhang, R. and Zhang, S. and Zhang, X. and Zhang, X. and Zhang, Z. and Zhao, H. and Zhao, P. and Zhao, T. and Zhao, Y. and Zhao, Z. and Zhemchugov, A. and Zheng, Z. and Zhong, D. and Zhou, B. and Zhou, C. and Zhou, H. and Zhou, N. and Zhou, Y. and Zhu, C. G. and Zhu, C. and Zhu, H. L. and Zhu, H. and Zhu, J. and Zhu, Y. and Zhuang, X. and Zhukov, K. and Zhulanov, V. and Zieminska, D. and Zimine, N. I. and Zimmermann, S. and Zinsser, J. and Ziolkowski, M. and {\v Z}ivkovi{\'c}, L. and Zoccoli, A. and Zoch, K. and Zorbas, T. G. and Zormpa, O. and Zou, W. and Zwalinski, L.},
  year = {2022},
  month = aug,
  journal = {Journal of High Energy Physics},
  volume = {2022},
  number = {8},
  pages = {104},
  issn = {1029-8479},
  doi = {10.1007/JHEP08(2022)104},
  urldate = {2024-09-29},
  abstract = {A               bstract                                         A direct search for Higgs bosons produced via vector-boson fusion and subsequently decaying into invisible particles is reported. The analysis uses 139 fb                                -                 1                              of               pp               collision data at a centre-of-mass energy of                                                   \$\$ {\textbackslash}sqrt\{s\} \$\$                                                               s                                                                                       = 13 TeV recorded by the ATLAS detector at the LHC. The observed numbers of events are found to be in agreement with the background expectation from Standard Model processes. For a scalar Higgs boson with a mass of 125 GeV and a Standard Model production cross section, an observed upper limit of 0               .               145 is placed on the branching fraction of its decay into invisible particles at 95\% confidence level, with an expected limit of 0               .               103. These results are interpreted in the context of models where the Higgs boson acts as a portal to dark matter, and limits are set on the scattering cross section of weakly interacting massive particles and nucleons. Invisible decays of additional scalar bosons with masses from 50 GeV to 2 TeV are also studied, and the derived upper limits on the cross section times branching fraction decrease with increasing mass from 1               .               0 pb for a scalar boson mass of 50 GeV to 0               .               1 pb at a mass of 2 TeV.},
  langid = {english},
  keywords = {Jab/JHEP,Unread},
  annotation = {81 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/SQHPDKTE/JHEP_2022_Search for invisible Higgs-boson decays in events with vector-boson fusion signatures using 139 fb−1 of proton-proton data recorded by the ATLAS experiment.pdf}
}

@misc{thulkeEfficientRetrievalAugmented2021,
  title = {Efficient {{Retrieval Augmented Generation}} from {{Unstructured Knowledge}} for {{Task-Oriented Dialog}}},
  author = {Thulke, David and Daheim, Nico and Dugast, Christian and Ney, Hermann},
  year = {2021},
  month = feb,
  number = {arXiv:2102.04643},
  eprint = {2102.04643},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2102.04643},
  urldate = {2024-09-29},
  abstract = {This paper summarizes our work on the first track of the ninth Dialog System Technology Challenge (DSTC 9), ``Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access''. The goal of the task is to generate responses to user turns in a task-oriented dialog that require knowledge from unstructured documents. The task is divided into three subtasks: detection, selection and generation. In order to be compute efficient, we formulate the selection problem in terms of hierarchical classification steps. We achieve our best results with this model. Alternatively, we employ siamese sequence embedding models, referred to as Dense Knowledge Retrieval, to retrieve relevant documents. This method further reduces the computation time by a factor of more than 100x at the cost of degradation in R@1 of 5-6\% compared to the first model. Then for either approach, we use Retrieval Augmented Generation to generate responses based on multiple selected snippets and we show how the method can be used to fine-tune trained embeddings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {43 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/A8T57IS3/Pre_2021_Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog.pdf}
}

@article{tiddiKnowledgeGraphsTools2022ArtificialIntelligence,
  title = {Knowledge Graphs as Tools for Explainable Machine Learning: {{A}} Survey},
  shorttitle = {Knowledge Graphs as Tools for Explainable Machine Learning},
  author = {Tiddi, Ilaria and Schlobach, Stefan},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {302},
  pages = {103627},
  issn = {00043702},
  doi = {10.1016/j.artint.2021.103627},
  urldate = {2024-09-29},
  langid = {english},
  keywords = {Jab/AI,Unread},
  annotation = {170 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/9RSZUJ2B/AI_2022_Knowledge graphs as tools for explainable machine learning.pdf}
}

@article{traagLouvainLeidenGuaranteeing2019SciRep,
  title = {From {{Louvain}} to {{Leiden}}: Guaranteeing Well-Connected Communities},
  shorttitle = {From {{Louvain}} to {{Leiden}}},
  author = {Traag, Vincent and Waltman, Ludo and van Eck, Nees Jan},
  year = {2019},
  month = mar,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  eprint = {1810.08473},
  primaryclass = {cs},
  pages = {5233},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-41695-z},
  urldate = {2024-12-26},
  abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society,Unread},
  annotation = {3020 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The Leiden algorithm is found to be faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees on communities that are guaranteed to be connected.},
  file = {/Users/janwardenga/Zotero/storage/AZL74LLM/Traag et al. - 2019 - From Louvain to Leiden guaranteeing well-connected communities.pdf}
}

@article{turkiAutomatingUseShape,
  title = {Automating the Use of {{Shape Expressions}} for the Validation of Semantic Knowledge in {{Wikidata}}},
  author = {Turki, Houcemeddine and Taieb, Mohamed Ali Hadj and Chebil, Khalil and Ben, Mohamed and Rasberry, Lane and Mietchen, Daniel},
  abstract = {In this position paper, we discuss the semantic alignment-based approach for automating the ShEx-based validation of Wikidata items as proposed by Wikimedia Deutschland in July 2023, and we propose an alternative method that automates the shape-based validation of Wikidata entities and statements based on the conversion of ShEx EntitySchemas into SPARQL queries that identify relevant entities. We explain the advantages and drawbacks of both methods to provide the community with a useful overview of the matter.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/5CNZS4UG/Turki et al. - Automating the use of Shape Expressions for the validation of semantic knowledge in Wikidata.pdf}
}

@article{universityofliechtensteinliechtensteinSpecialIssueEditorial2020JAIS,
  title = {Special {{Issue Editorial}} --{{Accumulation}} and {{Evolution}} of {{Design Knowledge}} in {{Design Science Research}}: {{A Journey Through Time}} and {{Space}}},
  shorttitle = {Special {{Issue Editorial}} --{{Accumulation}} and {{Evolution}} of {{Design Knowledge}} in {{Design Science Research}}},
  author = {{University of Liechtenstein, Liechtenstein} and Vom Brocke, Jan and Winter, Robert and {University of St. Gallen, Switzerland} and Hevner, Alan and {University of South Florida, USA} and Maedche, Alexander and {Karlsruhe Institute of Technology, Germany}},
  year = {2020},
  month = may,
  journal = {Journal of the Association for Information Systems},
  volume = {21},
  number = {3},
  pages = {520--544},
  issn = {15369323},
  doi = {10.17705/1jais.00611},
  urldate = {2024-11-25},
  abstract = {Sir Isaac Newton (1676) famously said, ``If I have seen further, it is by standing on the shoulders of giants.'' Research is a collaborative, evolutionary endeavor---and it is no different with design science research (DSR), which builds upon existing design knowledge and creates new design knowledge to pass on to future projects. However, despite the vast, growing body of DSR contributions, scant evidence of the accumulation and evolution of design knowledge has been articulated in an organized DSR body of knowledge. Most contributions rather stand on their own feet than on the shoulders of giants, and this continues to limit how far we can see, curtailing the extent of the broader impacts that can be made through DSR. In this editorial, we aim at providing guidance on how to position design knowledge contributions in wider problem and solution spaces. We propose (1) a model conceptualizing design knowledge as a resilient relationship between problem and solution spaces, (2) a model that demonstrates how individual DSR projects consume and produce design knowledge, (3) a map to position a design knowledge contribution in problem and solution spaces, and (4) principles on how to use this map in a DSR project. We show how fellow researchers, readers, editors, and reviewers, as well as the IS community as a whole, can make use of these proposals, and also illustrate future research opportunities.},
  langid = {english},
  keywords = {Unread},
  annotation = {132 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CTTQ58MA/University of Liechtenstein, Liechtenstein et al. - 2020 - Special Issue Editorial –Accumulation and Evolution of Design Knowledge in Design Science Research.pdf}
}

@article{vanasscheDeclarativeRDFGraph2023JournalofWebSemantics,
  title = {Declarative {{RDF}} Graph Generation from Heterogeneous (Semi-)Structured Data: {{A}} Systematic Literature Review},
  shorttitle = {Declarative {{RDF}} Graph Generation from Heterogeneous (Semi-)Structured Data},
  author = {Van Assche, Dylan and Delva, Thomas and Haesendonck, Gerald and Heyvaert, Pieter and De Meester, Ben and Dimou, Anastasia},
  year = {2023},
  month = jan,
  journal = {Journal of Web Semantics},
  volume = {75},
  pages = {100753},
  issn = {15708268},
  doi = {10.1016/j.websem.2022.100753},
  urldate = {2024-09-26},
  abstract = {More and more data in various formats are integrated into knowledge graphs. However, there is no overview of existing approaches for generating knowledge graphs from heterogeneous (semi)structured data, making it difficult to select the right one for a certain use case. To support better decision making, we study the existing approaches for generating knowledge graphs from heterogeneous (semi-)structured data relying on mapping languages. In this paper, we investigated existing mapping languages for schema and data transformations, and corresponding materialization and virtualization systems that generate knowledge graphs. We gather and unify 52 articles regarding knowledge graph generation from heterogeneous (semi-)structured data. We assess 15 characteristics on mapping languages for schema transformations, 5 characteristics for data transformations, and 14 characteristics for systems. Our survey paper provides an overview of the mapping languages and systems proposed the past two decades. Our work paves the way towards a better adoption of knowledge graph generation, as the right mapping language and system can be selected for each use case.},
  langid = {english},
  keywords = {Jab/JWS,Unread},
  annotation = {37 citations (Semantic Scholar/DOI) [2025-02-15]\\
75 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper investigates existing mapping languages for schema and data transformations, and corresponding materialization and virtualization systems that generate knowledge graphs, and provides an overview of the mapping languages and systems proposed the past two decades.},
  file = {/Users/janwardenga/Zotero/storage/IG3HL3YW/JWS_2023_Declarative RDF graph generation from heterogeneous (semi-)structured data.pdf}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-02-22},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Read},
  annotation = {116275 citations (Semantic Scholar/arXiv) [2025-02-22]},
  file = {/Users/janwardenga/Zotero/storage/2WG7ZGM4/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/janwardenga/Zotero/storage/Y8XVRCYR/1706.html}
}

@inproceedings{veiraUnsupervisedEmbeddingEnhancements2019Proc.Twenty-EighthInt.Jt.Conf.Artif.Intell.,
  title = {Unsupervised {{Embedding Enhancements}} of {{Knowledge Graphs}} Using {{Textual Associations}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Veira, Neil and Keng, Brian and Padmanabhan, Kanchana and Veneris, Andreas},
  year = {2019},
  month = aug,
  pages = {5218--5225},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macao, China},
  doi = {10.24963/ijcai.2019/725},
  urldate = {2024-09-29},
  abstract = {Knowledge graph embeddings are instrumental for representing and learning from multi-relational data, with recent embedding models showing high effectiveness for inferring new facts from existing databases. However, such precisely structured data is usually limited in quantity and in scope. Therefore, to fully optimize the embeddings it is important to also consider more widely available sources of information such as text. This paper describes an unsupervised approach to incorporate textual information by augmenting entity embeddings with embeddings of associated words. The approach does not modify the optimization objective for the knowledge graph embedding, which allows it to be integrated with existing embedding models. Two distinct forms of textual data are considered, with different embedding enhancements proposed for each case. In the first case, each entity has an associated text document that describes it. In the second case, a text document is not available, and instead entities occur as words or phrases in an unstructured corpus of text fragments. Experiments show that both methods can offer improvement on the link prediction task when applied to many different knowledge graph embedding models.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  keywords = {Jab/TIJCAI,Unread},
  annotation = {23 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The approach does not modify the optimization objective for the knowledge graph embedding, which allows it to be integrated with existing embedding models, and two distinct forms of textual data are considered, with different embedding enhancements proposed for each case.},
  file = {/Users/janwardenga/Zotero/storage/WNDKMZKH/TIJCAI_2019_Unsupervised Embedding Enhancements of Knowledge Graphs using Textual Associations.pdf}
}

@misc{veldhuizenTriejoinSimpleWorstCase2014,
  title = {Triejoin: {{A Simple}}, {{Worst-Case Optimal Join Algorithm}}},
  shorttitle = {Triejoin},
  author = {Veldhuizen, Todd},
  year = {2014},
  publisher = {OpenProceedings.org},
  doi = {10.5441/002/ICDT.2014.13},
  urldate = {2025-01-23},
  abstract = {Recent years have seen exciting developments in join algorithms. In 2008, Atserias, Grohe and Marx (henceforth AGM) proved a tight bound on the maximum result size of a full conjunctive query, given constraints on the input relation sizes. In 2012, Ngo, Porat, Re  and Rudra (henceforth NPRR) devised a join algorithm with worst-case running time proportional to the AGM bound [8]. Our commercial database system LogicBlox employs a novel join algorithm, leapfrog triejoin, which compared conspicuously well to the NPRR algorithm in preliminary benchmarks. This spurred us to analyze the complexity of leapfrog triejoin. In this paper we establish that leapfrog triejoin is also worst-case optimal, up to a log factor, in the sense of NPRR. We improve on the results of NPRR by proving that leapfrog triejoin achieves worst-case optimality for ner-grained classes of database instances, such as those de ned by constraints on projection cardinalities. We show that NPRR is not worstcase optimal for such classes, giving a counterexample where leapfrog triejoin runs in O(n log n) time and NPRR runs in  (n1.375) time. On a practical note, leapfrog triejoin can be implemented using conventional data structures such as B-trees, and extends naturally to 91 queries. We believe our algorithm o ers a useful addition to the existing toolbox of join algorithms, being easy to absorb, simple to implement, and having a concise optimality proof.},
  langid = {english},
  keywords = {Database Technology,Database Theory,Unread},
  file = {/Users/janwardenga/Zotero/storage/8QHYU87M/Veldhuizen - 2014 - Triejoin A Simple, Worst-Case Optimal Join Algorithm.pdf}
}

@inproceedings{venkatakrishnanSemanticInterlinkingImmigration2024CompanionProc.ACMWebConf.2024,
  title = {Semantic Interlinking of {{Immigration Data}} Using {{LLMs}} for {{Knowledge Graph Construction}}},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2024},
  author = {Venkatakrishnan, Radhakrishnan and Tanyildizi, Emrah and Canbaz, M. Abdullah},
  year = {2024},
  month = may,
  pages = {605--608},
  publisher = {ACM},
  address = {Singapore Singapore},
  doi = {10.1145/3589335.3651557},
  urldate = {2024-09-29},
  abstract = {The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.},
  isbn = {9798400701726},
  langid = {english},
  keywords = {Jab/WWC,Unread},
  annotation = {3 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/2RJ949LU/WWC_2024_Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction.pdf}
}

@misc{wangDataFormulatorAIpowered2023,
  title = {Data {{Formulator}}: {{AI-powered Concept-driven Visualization Authoring}}},
  shorttitle = {Data {{Formulator}}},
  author = {Wang, Chenglong and Thompson, John and Lee, Bongshin},
  year = {2023},
  month = oct,
  number = {arXiv:2309.10094},
  eprint = {2309.10094},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.10094},
  urldate = {2024-09-29},
  abstract = {With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Unread},
  annotation = {18 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/T64A78LN/Pre_2023_Data Formulator.pdf}
}

@article{wangDistributedPregelbasedProvenanceaware2020WorldWideWeb,
  title = {Distributed {{Pregel-based}} Provenance-Aware Regular Path Query Processing on {{RDF}} Knowledge Graphs},
  author = {Wang, Xin and Wang, Simiao and Xin, Yueqi and Yang, Yajun and Li, Jianxin and Wang, Xiaofei},
  year = {2020},
  month = may,
  journal = {World Wide Web},
  volume = {23},
  number = {3},
  pages = {1465--1496},
  issn = {1386-145X, 1573-1413},
  doi = {10.1007/s11280-019-00739-0},
  urldate = {2024-09-26},
  abstract = {With the proliferation of knowledge graphs, massive RDF graphs have been published on the Web. As an essential type of queries for RDF graphs, Regular Path Queries (RPQs) have been attracting increasing research efforts. However, the existing query processing approaches mainly focus on RPQs under the standard semantics, which cannot provide the provenance of the answer sets. We propose a distributed Pregel-based approach DP2RPQ to evaluating provenance-aware RPQs over big RDF graphs. Our method employs Glushkov automata to keep track of matching processes of RPQs in parallel. Meanwhile, three optimization strategies are devised according to the cost model, including vertex-computation optimization, message-communication reduction, and counting-paths alleviation, which can reduce the intermediate results of the basic DP2RPQ algorithm dramatically and overcome the counting-paths problem to some extent. The proposed algorithms are verified by extensive experiments on both synthetic and real-world datasets, which show that our approach can efficiently answer the provenance-aware RPQs over large RDF graphs. Furthermore, the RPQ semantics of DP2RPQ is richer than that of RDFPath, and the performance of DP2RPQ is still far better than that of RDFPath.},
  langid = {english},
  keywords = {Jab/WWW,Unread},
  annotation = {27 citations (Semantic Scholar/DOI) [2025-02-15]\\
0 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work proposes a distributed Pregel-based approach DP2RPQ to evaluating provenance-aware RPQs over big RDF graphs, and shows that this approach can efficiently answer the provenance-aware RPQs over large RDF graphs.},
  file = {/Users/janwardenga/Zotero/storage/XHT2MK9D/WWW_2020_Distributed Pregel-based provenance-aware regular path query processing on RDF knowledge graphs.pdf}
}

@misc{wangFeB4RAGEvaluatingFederated2024,
  title = {{{FeB4RAG}}: {{Evaluating Federated Search}} in the {{Context}} of {{Retrieval Augmented Generation}}},
  shorttitle = {{{FeB4RAG}}},
  author = {Wang, Shuai and Khramtsova, Ekaterina and Zhuang, Shengyao and Zuccon, Guido},
  year = {2024},
  month = feb,
  number = {arXiv:2402.11891},
  eprint = {2402.11891},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.11891},
  urldate = {2024-09-29},
  abstract = {Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG 1, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used BEIR benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements . Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines. CCS Concepts: {$\bullet$} Information systems {$\rightarrow$} Test collections; Language models; Relevance assessment; Question answering; Distributed retrieval.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/NNPQMIYW/Pre_2024_FeB4RAG.pdf}
}

@misc{wangLearningPlanRetrievalAugmented2024,
  title = {Learning to {{Plan}} for {{Retrieval-Augmented Large Language Models}} from {{Knowledge Graphs}}},
  author = {Wang, Junjie and Chen, Mingyang and Hu, Binbin and Yang, Dan and Liu, Ziqi and Shen, Yue and Wei, Peng and Zhang, Zhiqiang and Gu, Jinjie and Zhou, Jun and Pan, Jeff Z. and Zhang, Wen and Chen, Huajun},
  year = {2024},
  month = jun,
  number = {arXiv:2406.14282},
  eprint = {2406.14282},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.14282},
  urldate = {2024-09-30},
  abstract = {Improving the performance of large language models (LLMs) in complex question-answering (QA) scenarios has always been a research focal point. Recent studies have attempted to enhance LLMs' performance by combining stepwise planning with external retrieval. While effective for advanced models like GPT-3.5, smaller LLMs face challenges in decomposing complex questions, necessitating supervised fine-tuning. Previous work has relied on manual annotation and knowledge distillation from teacher LLMs, which are time-consuming and not accurate enough. In this paper, we introduce a novel framework for enhancing LLMs' planning capabilities by using planning data derived from knowledge graphs (KGs). LLMs fine-tuned with this data have improved planning capabilities, better equipping them to handle complex QA tasks that involve retrieval. Evaluations on multiple datasets, including our newly proposed benchmark, highlight the effectiveness of our framework and the benefits of KG-derived planning data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/V28YIP7J/Wang et al. - 2024 - Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs.pdf}
}

@misc{wangLLMsKnowWhat2024,
  title = {{{LLMs Know What They Need}}: {{Leveraging}} a {{Missing Information Guided Framework}} to {{Empower Retrieval-Augmented Generation}}},
  shorttitle = {{{LLMs Know What They Need}}},
  author = {Wang, Keheng and Duan, Feiyu and Li, Peiguang and Wang, Sirui and Cai, Xunliang},
  year = {2024},
  month = apr,
  number = {arXiv:2404.14043},
  eprint = {2404.14043},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.14043},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules. Code and data are released in https://github.com/AdelWang/MIGRES.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/R9UCYBAG/Pre_2024_LLMs Know What They Need.pdf}
}

@misc{wangSearchingBestPractices2024,
  title = {Searching for {{Best Practices}} in {{Retrieval-Augmented Generation}}},
  author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01219},
  eprint = {2407.01219},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.01219},
  urldate = {2024-09-29},
  abstract = {Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ``retrieval as generation'' strategy. Resources are available at https://github.com/FudanDNN-NLP/RAG.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {12 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/J7PKKULG/Pre_2024_Searching for Best Practices in Retrieval-Augmented Generation.pdf}
}

@misc{wangSurveyFactualityLarge2023,
  title = {Survey on {{Factuality}} in {{Large Language Models}}: {{Knowledge}}, {{Retrieval}} and {{Domain-Specificity}}},
  shorttitle = {Survey on {{Factuality}} in {{Large Language Models}}},
  author = {Wang, Cunxiang and Liu, Xiaoze and Yue, Yuanhao and Tang, Xiangru and Zhang, Tianhang and Jiayang, Cheng and Yao, Yunzhi and Gao, Wenyang and Hu, Xuming and Qi, Zehan and Wang, Yidong and Yang, Linyi and Wang, Jindong and Xie, Xing and Zhang, Zheng and Zhang, Yue},
  year = {2023},
  month = dec,
  number = {arXiv:2310.07521},
  eprint = {2310.07521},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2310.07521},
  urldate = {2024-09-29},
  abstract = {This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential enhancements. Our survey offers a structured guide for researchers aiming to fortify the factual reliability of LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Read},
  annotation = {140 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/XDFR288A/Pre_2023_Survey on Factuality in Large Language Models.pdf}
}

@article{wangSurveyLargeLanguage2024Front.Comput.Sci.,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2024},
  month = dec,
  journal = {Frontiers of Computer Science},
  volume = {18},
  number = {6},
  eprint = {2308.11432},
  primaryclass = {cs},
  pages = {186345},
  issn = {2095-2228, 2095-2236},
  doi = {10.1007/s11704-024-40231-1},
  urldate = {2025-03-03},
  abstract = {Abstract Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLMbased autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present an overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Read},
  annotation = {865 citations (Semantic Scholar/DOI) [2025-03-03]\\
TLDR: This paper discusses the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work, and presents a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering.},
  file = {/Users/janwardenga/Zotero/storage/9BUMY3E4/Wang et al. - 2024 - A Survey on Large Language Model based Autonomous Agents.pdf}
}

@misc{weberLargeLanguageModels2024,
  title = {Large {{Language Models}} as {{Software Components}}: {{A Taxonomy}} for {{LLM-Integrated Applications}}},
  shorttitle = {Large {{Language Models}} as {{Software Components}}},
  author = {Weber, Irene},
  year = {2024},
  month = jun,
  number = {arXiv:2406.10300},
  eprint = {2406.10300},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.10300},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLMintegrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {A.1,D.2.11,Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/HYBUD6J7/Pre_2024_Large Language Models as Software Components.pdf}
}

@article{weissHexastoreSextupleIndexing2008Proc.VLDBEndow.,
  title = {Hexastore: Sextuple Indexing for Semantic Web Data Management},
  shorttitle = {Hexastore},
  author = {Weiss, Cathrin and Karras, Panagiotis and Bernstein, Abraham},
  year = {2008},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {1},
  number = {1},
  pages = {1008--1019},
  issn = {2150-8097},
  doi = {10.14778/1453856.1453965},
  urldate = {2024-09-29},
  abstract = {Despite the intense interest towards realizing the Semantic Web vision, most existing RDF data management schemes are constrained in terms of efficiency and scalability. Still, the growing popularity of the RDF format arguably calls for an effort to offset these drawbacks. Viewed from a relationaldatabase perspective, these constraints are derived from the very nature of the RDF data model, which is based on a triple format. Recent research has attempted to address these constraints using a vertical-partitioning approach, in which separate two-column tables are constructed for each property. However, as we show, this approach suffers from similar scalability drawbacks on queries that are not bound by RDF property value. In this paper, we propose an RDF storage scheme that uses the triple nature of RDF as an asset. This scheme enhances the vertical partitioning idea and takes it to its logical conclusion. RDF data is indexed in six possible ways, one for each possible ordering of the three RDF elements. Each instance of an RDF element is associated with two vectors; each such vector gathers elements of one of the other types, along with lists of the third-type resources attached to each vector element. Hence, a sextupleindexing scheme emerges. This format allows for quick and scalable general-purpose query processing; it confers significant advantages (up to five orders of magnitude) compared to previous approaches for RDF data management, at the price of a worst-case five-fold increase in index space. We experimentally document the advantages of our approach on real-world and synthetic data sets with practical queries.},
  langid = {english},
  keywords = {,Read},
  annotation = {715 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper proposes an RDF storage scheme that uses the triple nature of RDF as an asset, which confers significant advantages compared to previous approaches for RDF data management, at the price of a worst-case five-fold increase in index space.},
  file = {/Users/janwardenga/Zotero/storage/A4BF5ZW3/PVE_2008_Hexastore.pdf}
}

@inproceedings{wilcockConversationalAIKnowledge2022202217thACMIEEEInt.Conf.Hum.-RobotInteract.HRI,
  title = {Conversational {{AI}} and {{Knowledge Graphs}} for {{Social Robot Interaction}}},
  booktitle = {2022 17th {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}} ({{HRI}})},
  author = {Wilcock, Graham and Jokinen, Kristiina},
  year = {2022},
  month = mar,
  pages = {1090--1094},
  publisher = {IEEE},
  address = {Sapporo, Japan},
  doi = {10.1109/HRI53351.2022.9889583},
  urldate = {2024-09-29},
  abstract = {The paper describes an approach that combines work from three fields with previously separate research communities: social robotics, conversational AI, and graph databases. The aim is to develop a generic framework in which a variety of social robots can provide high-quality information to users by accessing semantically-rich knowledge graphs about multiple different domains. An example implementation uses a Furhat robot with Rasa open source conversational AI and knowledge graphs in Neo4j graph databases.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66540-731-1},
  langid = {english},
  keywords = {Jab/HRI,Read},
  annotation = {14 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: The aim is to develop a generic framework in which a variety of social robots can provide high-quality information to users by accessing semantically-rich knowledge graphs about multiple different domains.},
  file = {/Users/janwardenga/Zotero/storage/CALDTRQT/HRI_2022_Conversational AI and Knowledge Graphs for Social Robot Interaction.pdf}
}

@misc{wuAdversarialDatabasesImprove2024,
  title = {Adversarial {{Databases Improve Success}} in {{Retrieval-based Large Language Models}}},
  author = {Wu, Sean and Koo, Michael and Kao, Li Yo and Black, Andy and Blum, Lesley and Scalzo, Fabien and Kurtz, Ira},
  year = {2024},
  month = jul,
  number = {arXiv:2407.14609},
  eprint = {2407.14609},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.14609},
  urldate = {2024-09-29},
  abstract = {Open-source LLMs have shown great potential as fine-tuned chatbots, and demonstrate robust abilities in reasoning and surpass many existing benchmarks. Retrieval-Augmented Generation (RAG) is a technique for improving the performance of LLMs on tasks that the models weren't explicitly trained on, by leveraging external knowledge databases. Numerous studies have demonstrated the effectiveness of RAG to more successfully accomplish downstream tasks when using vector datasets that consist of relevant background information. It has been implicitly assumed by those in the field that if adversarial background information is utilized in this context, that the success of using a RAG-based approach would be nonexistent or even negatively impact the results. To address this assumption, we tested several open-source LLMs on the ability of RAG to improve their success in answering multiple-choice questions (MCQ) in the medical subspecialty field of Nephrology. Unlike previous studies, we examined the effect of RAG in utilizing both relevant and adversarial background databases. We set up several open-source LLMs, including Llama 3, Phi-3, Mixtral 8x7b, Zephyr{$\beta$}, and Gemma 7B Instruct, in a zero-shot RAG pipeline. The source of relevant information was the nephSAP information syllabus from which the MCQ were obtained, and the UpToDate corpus of clinical information in Nephrology. As adversarial sources of information, text from the Bible and a Random Words generated database were used for comparison. Our data show that most of the open-source LLMs improve their multiple-choice test-taking success as expected when incorporating relevant information vector databases. Surprisingly however, adversarial Bible text significantly improved the success of many LLMs and even random word text improved test taking ability of some of the models. In summary, our results demonstrate for the first time the countertintuitive ability of adversarial information datasets to improve the RAG-based LLM success. The LLM's pre-trained priors are likely involved rather than the RAG mechanism. Whether utilizing adversarial information databases can improve LLM performance in other arenas is a question worthy of pursuing in future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {0 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/K4RZEI9L/Pre_2024_Adversarial Databases Improve Success in Retrieval-based Large Language Models.pdf}
}

@misc{wuAvaTaROptimizingLLM2024,
  title = {{{AvaTaR}}: {{Optimizing LLM Agents}} for {{Tool-Assisted Knowledge Retrieval}}},
  shorttitle = {{{AvaTaR}}},
  author = {Wu, Shirley and Zhao, Shiyu and Huang, Qian and Huang, Kexin and Yasunaga, Michihiro and Cao, Kaidi and Ioannidis, Vassilis N. and Subbian, Karthik and Leskovec, Jure and Zou, James},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11200},
  eprint = {2406.11200},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.11200},
  urldate = {2024-09-29},
  abstract = {Large language model (LLM) agents have demonstrated impressive capability in utilizing external tools and knowledge to boost accuracy and reduce hallucinations. However, developing the prompting techniques that make LLM agents able to effectively use external tools and knowledge is a heuristic and laborious task. Here, we introduce AVATAR, a novel and automatic framework that optimizes an LLM agent to effectively use the provided tools and improve its performance on a given task/domain. During optimization, we design a comparator module to iteratively provide insightful and holistic prompts to the LLM agent via reasoning between positive and negative examples sampled from training data. We demonstrate AVATAR on four complex multimodal retrieval datasets featuring textual, visual, and relational information. We find AVATAR consistently outperforms state-of-the-art approaches across all four challenging tasks and exhibits strong generalization ability when applied to novel cases, achieving an average relative improvement of 14\% on the Hit@1 metric. Code and dataset are available at https://github.com/zou-group/avatar.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/DDXJU6KK/Pre_2024_AvaTaR.pdf}
}

@article{wuKnowledgeGraphIntegration2024,
  title = {Knowledge {{Graph Integration}} and {{Self-Verification}} for {{Comprehensive Retrieval-Augmented Generation}}},
  author = {Wu, Chenyuan and Shen, Tingjia and Yan, Ruiran and Wang, Hao and Liu, Zheng and Wang, Zhen and Lian, Defu and Chen, Enhong},
  year = {2024},
  abstract = {Retrieval-Augmented Generation (RAG) has gained significant attention from both academic researchers and the industry as a promising solution to address the knowledge limitations of large language models (LLMs). However, LLMs often exhibit hallucination phenomena when employing RAG. To effectively address hallucination phenomena in a wide range of question types, we employ various choices and strategies. Specifically, we utilize LLaMA3's emergent self-verification capability to determine whether the given reference can adequately answer a particular question, thereby avoiding hallucination phenomena. Subsequently, by utilizing knowledge graphs to augment our knowledge base, we enhance contextual understanding and reduce hallucinations on RAG. LLM's advanced capabilities further enable us to effectively integrate and interpret the contents of knowledge graphs, ensuring more coherent and accurate responses. Finally, the effective handling of these diverse question types allows us to provide precise and informative answers, tailored to the specific requirements of each query. In general, our work comprehensively utilizes the advanced capabilities of LLM to enhance the robustness and credibility of our information retrieval system. This multi-faceted approach, coupled with a meticulous evaluation of references, ensures the delivery of high-quality responses, irrespective of the complexity of the questions.},
  langid = {english},
  keywords = {No DOI found,Unread},
  file = {/Users/janwardenga/Zotero/storage/NREHXVGL/Wu et al. - 2024 - Knowledge Graph Integration and Self-Verification for Comprehensive Retrieval-Augmented Generation.pdf}
}

@misc{wuRetrievalAugmentedGeneration2024,
  title = {Retrieval {{Augmented Generation}} for {{Dynamic Graph Modeling}}},
  author = {Wu, Yuxia and Fang, Yuan and Liao, Lizi},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14523},
  eprint = {2408.14523},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.14523},
  urldate = {2024-09-29},
  abstract = {Dynamic graph modeling is crucial for analyzing evolving patterns in various applications. Existing approaches often integrate graph neural networks with temporal modules or redefine dynamic graph modeling as a generative sequence task. However, these methods typically rely on isolated historical contexts of the target nodes from a narrow perspective, neglecting occurrences of similar patterns or relevant cases associated with other nodes. In this work, we introduce the Retrieval-Augmented Generation for Dynamic Graph Modeling (RAG4DyG) framework, which leverages guidance from contextually and temporally analogous examples to broaden the perspective of each node. This approach presents two critical challenges: (1) How to identify and retrieve highquality demonstrations that are contextually and temporally analogous to dynamic graph samples? (2) How can these demonstrations be effectively integrated to improve dynamic graph modeling? To address these challenges, we propose RAG4DyG, which enriches the understanding of historical contexts by retrieving and learning from contextually and temporally pertinent demonstrations. Specifically, we employ a time- and context-aware contrastive learning module to identify and retrieve relevant cases for each query sequence. Moreover, we design a graph fusion strategy to integrate the retrieved cases, thereby augmenting the inherent historical contexts for improved prediction. Extensive experiments on real-world datasets across different domains demonstrate the effectiveness of RAG4DyG for dynamic graph modeling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Read},
  annotation = {1 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/JRSYJF7U/Pre_2024_Retrieval Augmented Generation for Dynamic Graph Modeling.pdf}
}

@misc{wuRetrievalAugmentedGenerationNatural2024,
  title = {Retrieval-{{Augmented Generation}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Natural Language Processing}}},
  author = {Wu, Shangyu and Xiong, Ying and Cui, Yufei and Wu, Haolun and Chen, Can and Yuan, Ye and Huang, Lianming and Liu, Xue and Kuo, Tei-Wei and Guan, Nan and Xue, Chun Jason},
  year = {2024},
  month = jul,
  number = {arXiv:2407.13193},
  eprint = {2407.13193},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2407.13193},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrievalaugmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG training, including RAG with/without datastore update. Then, we introduce the application of RAG in representative natural language processing tasks and industrial scenarios. Finally, this paper discusses the future directions and challenges of RAG for promoting its development.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Unread},
  annotation = {10 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/4DA8PTBV/Pre_2024_Retrieval-Augmented Generation for Natural Language Processing.pdf}
}

@misc{wuRetrieveRewriteAnswerKGtoTextEnhanced2023,
  title = {Retrieve-{{Rewrite-Answer}}: {{A KG-to-Text Enhanced LLMs Framework}} for {{Knowledge Graph Question Answering}}},
  shorttitle = {Retrieve-{{Rewrite-Answer}}},
  author = {Wu, Yike and Hu, Nan and Bi, Sheng and Qi, Guilin and Ren, Jie and Xie, Anhuan and Song, Wei},
  year = {2023},
  month = sep,
  number = {arXiv:2309.11206},
  eprint = {2309.11206},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2309.11206},
  urldate = {2024-09-29},
  abstract = {Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {46 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/TM5ETZW5/Pre_2023_Retrieve-Rewrite-Answer.pdf}
}

@misc{wuSTaRKBenchmarkingLLM2024,
  title = {{{STaRK}}: {{Benchmarking LLM Retrieval}} on {{Textual}} and {{Relational Knowledge Bases}}},
  shorttitle = {{{STaRK}}},
  author = {Wu, Shirley and Zhao, Shiyu and Yasunaga, Michihiro and Huang, Kexin and Cao, Kaidi and Huang, Qian and Ioannidis, Vassilis N. and Subbian, Karthik and Zou, James and Leskovec, Jure},
  year = {2024},
  month = may,
  number = {arXiv:2404.13207},
  eprint = {2404.13207},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.13207},
  urldate = {2024-09-29},
  abstract = {Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, previous works have mostly studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains/datasets: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their groundtruth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, indicating the demand for building more capable retrieval systems. The benchmark data and code are available on https://github.com/snap-stanford/stark.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {11 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/9ZGHVN5L/Pre_2024_STaRK.pdf}
}

@misc{wysockiLLMbasedKnowledgeSynthesis2024,
  title = {An {{LLM-based Knowledge Synthesis}} and {{Scientific Reasoning Framework}} for {{Biomedical Discovery}}},
  author = {Wysocki, Oskar and Wysocka, Magdalena and Carvalho, Danilo and Bogatu, Alex Teodor and Gusicuma, Danilo Miranda and Delmas, Maxime and Unsworth, Harriet and Freitas, Andre},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18626},
  eprint = {2406.18626},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.18626},
  urldate = {2024-09-29},
  abstract = {We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Quantitative Biology - Quantitative Methods,Unread},
  annotation = {4 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/CH86JSR9/Pre_2024_An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery.pdf}
}

@misc{xiaoDistillVQLearningRetrieval2022,
  title = {Distill-{{VQ}}: {{Learning Retrieval Oriented Vector Quantization By Distilling Knowledge}} from {{Dense Embeddings}}},
  shorttitle = {Distill-{{VQ}}},
  author = {Xiao, Shitao and Liu, Zheng and Han, Weihao and Zhang, Jianjin and Lian, Defu and Gong, Yeyun and Chen, Qi and Yang, Fan and Sun, Hao and Shao, Yingxia and Deng, Denvy and Zhang, Qi and Xie, Xing},
  year = {2022},
  month = apr,
  number = {arXiv:2204.00185},
  eprint = {2204.00185},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2204.00185},
  urldate = {2024-09-29},
  abstract = {Vector quantization (VQ) based ANN indexes, such as Inverted File System (IVF) and Product Quantization (PQ), have been widely applied to embedding based document retrieval thanks to the competitive time and memory efficiency. Originally, VQ is learned to minimize the reconstruction loss, i.e., the distortions between the original dense embeddings and the reconstructed embeddings after quantization. Unfortunately, such an objective is inconsistent with the goal of selecting ground-truth documents for the input query, which may cause severe loss of retrieval quality. Recent works identify such a defect, and propose to minimize the retrieval loss through contrastive learning. However, these methods intensively rely on queries with ground-truth documents, whose performance is limited by the insufficiency of labeled data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {28 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/WWXB68Z2/Pre_2022_Distill-VQ.pdf}
}

@inproceedings{xieBriefSurveyVector202320239thInt.Conf.BigDataInf.Anal.BigDIA,
  title = {A {{Brief Survey}} of {{Vector Databases}}},
  booktitle = {2023 9th {{International Conference}} on {{Big Data}} and {{Information Analytics}} ({{BigDIA}})},
  author = {Xie, Xingrui and Liu, Han and Hou, Wenzhe and Huang, Hongbin},
  year = {2023},
  month = dec,
  pages = {364--371},
  publisher = {IEEE},
  address = {Haikou, China},
  doi = {10.1109/BigDIA60676.2023.10429609},
  urldate = {2024-09-29},
  abstract = {The explosive growth of massive high-dimensional data requires capabilities for data processing, storing, and analyzing. This brings significant challenges to traditional databases due to the poor ability to handle high-dimensional data and its original design for stand-alone machines. Fortunately, vector databases have provided a practical solution for the management and analysis of high-dimensional data. Especially, they retrieve results related to the query efficiently after encoding various forms of data (e.g., text, image, and video) into vectors. The purpose of this paper is to offer insight into vector databases by presenting a brief survey. Firstly, the workflow of vector databases including indexing and querying, is detailed along with a specific case. Subsequently, we elaborate on the related methods applied in vector databases, which are the core techniques to enhance search efficiency and reduce computational overhead, particularly similarity search algorithms and similarity metrics. Further, we introduce widely used vector database products (e.g., Pinecone, Chroma, and Milvus) and compare them from multiple factors that should be taken into consideration. We also discuss potential avenues for future research in this domain. To conclude, this survey provides a comprehensive understanding of vector databases for retrieval from vast high-dimensional datasets.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350330076},
  langid = {english},
  keywords = {Jab/BigDIA,Unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This survey provides a comprehensive understanding of vector databases for retrieval from vast high-dimensional datasets by presenting a brief survey of the related methods applied in vector databases.},
  file = {/Users/janwardenga/Zotero/storage/U29KSCLG/BigDIA_2023_A Brief Survey of Vector Databases.pdf}
}

@misc{xieWeKnowRAGAdaptiveApproach2024,
  title = {{{WeKnow-RAG}}: {{An Adaptive Approach}} for {{Retrieval-Augmented Generation Integrating Web Search}} and {{Knowledge Graphs}}},
  shorttitle = {{{WeKnow-RAG}}},
  author = {Xie, Weijian and Liang, Xuefeng and Liu, Yuhui and Ni, Kaihua and Cheng, Hong and Hu, Zetian},
  year = {2024},
  month = aug,
  number = {arXiv:2408.07611},
  eprint = {2408.07611},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2408.07611},
  urldate = {2024-09-30},
  abstract = {Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Unread},
  annotation = {2 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/YVCT97DS/Xie et al. - 2024 - WeKnow-RAG An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowl.pdf}
}

@misc{xiongBenchmarkingRetrievalAugmentedGeneration2024,
  title = {Benchmarking {{Retrieval-Augmented Generation}} for {{Medicine}}},
  author = {Xiong, Guangzhi and Jin, Qiao and Lu, Zhiyong and Zhang, Aidong},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13178},
  eprint = {2402.13178},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.13178},
  urldate = {2024-09-29},
  abstract = {While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrievalaugmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MEDRAG toolkit introduced in this work. Overall, MEDRAG improves the accuracy of six different LLMs by up to 18\% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the ``lostin-the-middle'' effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {96 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/9DYJSJ4Y/Pre_2024_Benchmarking Retrieval-Augmented Generation for Medicine.pdf}
}

@inproceedings{xuFinetunedLLMsKnow2023Proc.2023Conf.Empir.MethodsNat.Lang.Process.,
  title = {Fine-Tuned {{LLMs Know More}}, {{Hallucinate Less}} with {{Few-Shot Sequence-to-Sequence Semantic Parsing}} over {{Wikidata}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xu, Silei and Liu, Shicheng and Culhane, Theo and Pertseva, Elizaveta and Wu, Meng-Hsi and Semnani, Sina and Lam, Monica},
  year = {2023},
  pages = {5778--5791},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.353},
  urldate = {2025-02-27},
  abstract = {While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a highquality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation.},
  langid = {english},
  keywords = {Unread},
  file = {/Users/janwardenga/Zotero/storage/WS46R765/Xu et al. - 2023 - Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over.pdf}
}

@misc{xuLeveragingAdvantagesInteractive2021,
  title = {Leveraging {{Advantages}} of {{Interactive}} and {{Non-Interactive Models}} for {{Vector-Based Cross-Lingual Information Retrieval}}},
  author = {Xu, Linlong and Yang, Baosong and Lv, Xiaoyu and Bi, Tianchi and Liu, Dayiheng and Zhang, Haibo},
  year = {2021},
  month = nov,
  number = {arXiv:2111.01992},
  eprint = {2111.01992},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2111.01992},
  urldate = {2024-09-29},
  abstract = {Interactive and non-interactive model are the two de-facto standard frameworks in vector-based cross-lingual information retrieval (V-CLIR), which embed queries and documents in synchronous and asynchronous fashions, respectively. From the retrieval accuracy and computational efficiency perspectives, each model has its own superiority and shortcoming. In this paper, we propose a novel framework to leverage the advantages of these two paradigms. Concretely, we introduce semi-interactive mechanism, which builds our model upon non-interactive architecture but encodes each document together with its associated multilingual queries. Accordingly, cross-lingual features can be better learned like an interactive model. Besides, we further transfer knowledge from a well-trained interactive model to ours by reusing its word embeddings and adopting knowledge distillation. Our model is initialized from a multilingual pre-trained language model M-BERT, and evaluated on two open-resource CLIR datasets derived from Wikipedia and an in-house dataset collected from a real-world search engine. Extensive analyses reveal that our methods significantly boost the retrieval accuracy while maintaining the computational efficiency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {5 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/A8GDJ96B/Pre_2021_Leveraging Advantages of Interactive and Non-Interactive Models for Vector-Based Cross-Lingual Information Retrieval.pdf}
}

@article{xuNanjingYunjinIntelligent2024HeritSci,
  title = {Nanjing {{Yunjin}} Intelligent Question-Answering System Based on Knowledge Graphs and Retrieval Augmented Generation Technology},
  author = {Xu, Liang and Lu, Lu and Liu, Minglu and Song, Chengxuan and Wu, Lizhen},
  year = {2024},
  month = apr,
  journal = {Heritage Science},
  volume = {12},
  number = {1},
  pages = {118},
  issn = {2050-7445},
  doi = {10.1186/s40494-024-01231-3},
  urldate = {2024-09-30},
  abstract = {Nanjing Yunjin, a traditional Chinese silk weaving craft, is celebrated globally for its unique local characteristics and exquisite workmanship, forming an integral part of the world's intangible cultural heritage. However, with the advancement of information technology, the experiential knowledge of the Nanjing Yunjin production process is predominantly stored in text format. As a highly specialized and vertical domain, this information is not readily convert into usable data. Previous studies on a knowledge graph-based Nanjing Yunjin Question-Answering System have partially addressed this issue. However, knowledge graphs need to be constantly updated and rely on predefined entities and relationship types. Faced with ambiguous or complex natural language problems, knowledge graph information retrieval faces some challenges. Therefore, this study proposes a Nanjing Yunjin Question-Answering System that integrates Knowledge Graphs and Retrieval Augmented Generation techniques. In this system, the ROBERTA model is first utilized to vectorize Nanjing Yunjin textual information, delving deep into textual semantics to unveil its profound cultural connotations. Additionally, the FAISS vector database is employed for efficient storage and retrieval of Nanjing Yunjin information, achieving a deep semantic match between questions and answers. Ultimately, related retrieval results are fed into the Large Language Model for enhanced generation, aiming for more accurate text generation outcomes and improving the interpretability and logic of the Question-Answering System. This research merges technologies like text embedding, vectorized retrieval, and natural language generation, aiming to overcome the limitations of knowledge graphs-based Question-Answering System in terms of graph updating, dependency on predefined types, and semantic understanding. System implementation and testing have shown that the Nanjing Yunjin Intelligent Question-Answering System, constructed on the basis of Knowledge Graphs and Retrieval Augmented Generation, possesses a broader knowledge base that considers context, resolving issues of polysemy, vague language, and sentence ambiguity, and efficiently and accurately generates answers to natural language queries. This significantly facilitates the retrieval and utilization of Yunjin knowledge, providing a paradigm for constructing Question-Answering System for other intangible cultural heritages, and holds substantial theoretical and practical significance for the deep exploration and discovery of the knowledge structure of human intangible heritage, promoting cultural inheritance and protection.},
  langid = {english},
  keywords = {,Unread},
  annotation = {4 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This research merges technologies like text embedding, vectorized retrieval, and natural language generation, aiming to overcome the limitations of knowledge graphs-based Question-Answering System in terms of graph updating, dependency on predefined types, and semantic understanding.},
  file = {/Users/janwardenga/Zotero/storage/26IJ2FWP/Xu et al. - 2024 - Nanjing Yunjin intelligent question-answering system based on knowledge graphs and retrieval augment.pdf}
}

@inproceedings{xuRetrievalAugmentedGenerationKnowledge2024Proc.47thInt.ACMSIGIRConf.Res.Dev.Inf.Retr.,
  title = {Retrieval-{{Augmented Generation}} with {{Knowledge Graphs}} for {{Customer Service Question Answering}}},
  booktitle = {Proceedings of the 47th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Xu, Zhentao and Cruz, Mark Jerome and Guevara, Matthew and Wang, Tie and Deshpande, Manasi and Wang, Xiaofeng and Li, Zheng},
  year = {2024},
  month = jul,
  eprint = {2404.17723},
  primaryclass = {cs},
  pages = {2905--2909},
  doi = {10.1145/3626772.3661370},
  urldate = {2024-09-30},
  abstract = {In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrievalaugmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and interissue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6\% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,I.2,Unread},
  annotation = {26 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This work introduces a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG) and improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation.},
  file = {/Users/janwardenga/Zotero/storage/HQZDVY6N/Xu et al. - 2024 - Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering.pdf}
}

@incollection{yangLLMBasedSPARQLGeneration2023KnowledgeGraphandSemanticComputing:KnowledgeGraphEmpowersArtificialGeneralIntelligence,
  title = {{{LLM-Based SPARQL Generation}} with {{Selected Schema}} from {{Large Scale Knowledge Base}}},
  booktitle = {Knowledge {{Graph}} and {{Semantic Computing}}: {{Knowledge Graph Empowers Artificial General Intelligence}}},
  author = {Yang, Shuangtao and Teng, Mao and Dong, Xiaozheng and Bo, Fu},
  editor = {Wang, Haofen and Han, Xianpei and Liu, Ming and Cheng, Gong and Liu, Yongbin and Zhang, Ningyu},
  year = {2023},
  volume = {1923},
  pages = {304--316},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-7224-1_24},
  urldate = {2025-02-23},
  isbn = {978-981-9972-23-4 978-981-9972-24-1},
  langid = {english},
  keywords = {Unread}
}

@article{yanKNOWNETGuidedHealth2024IEEETrans.Visual.Comput.Graphics,
  title = {{{KNOWNET}}: {{Guided Health Information Seeking}} from {{LLMs}} via {{Knowledge Graph Integration}}},
  shorttitle = {{{KNOWNET}}},
  author = {Yan, Youfu and Hou, Yu and Xiao, Yongkang and Zhang, Rui and Wang, Qianwen},
  year = {2024},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  eprint = {2407.13598},
  primaryclass = {cs},
  pages = {1--11},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2024.3456364},
  urldate = {2024-09-29},
  abstract = {The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KNOWNET a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KNOWNET extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KNOWNET provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KNOWNET conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Jab/TVCG,Unread},
  annotation = {73 citations (Semantic Scholar/arXiv) [2025-02-15]\\
TLDR: This paper introduces KNOWNET a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration and conceptualizes the understanding of a subject as the gradual construction of graph visualization.},
  file = {/Users/janwardenga/Zotero/storage/CL296TEQ/TVCG_2024_KNOWNET.pdf}
}

@inproceedings{yeWordEmbeddingsDocument2016Proc.38thInt.Conf.Softw.Eng.,
  title = {From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Software Engineering}}},
  author = {Ye, Xin and Shen, Hui and Ma, Xiao and Bunescu, Razvan and Liu, Chang},
  year = {2016},
  month = may,
  pages = {404--415},
  publisher = {ACM},
  address = {Austin Texas},
  doi = {10.1145/2884781.2884862},
  urldate = {2024-09-29},
  abstract = {The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.},
  isbn = {978-1-4503-3900-1},
  langid = {english},
  keywords = {Jab/IICSE,Unread},
  file = {/Users/janwardenga/Zotero/storage/Z7HW7ELY/IICSE_2016_From word embeddings to document similarities for improved information retrieval in software engineering.pdf}
}

@article{yilmazMIDDLEEASTTECHNICAL,
  title = {{{MIDDLE EAST TECHNICAL UNIVERSITY}}},
  author = {Yilmaz, Ey{\"u}p Halit},
  langid = {english},
  keywords = {No DOI found,Not citable but relevant,Read},
  file = {/Users/janwardenga/Zotero/storage/DULZJ2AH/Pre__MIDDLE EAST TECHNICAL UNIVERSITY.pdf}
}

@misc{yinNeuralMachineTranslating2019,
  title = {Neural {{Machine Translating}} from {{Natural Language}} to {{SPARQL}}},
  author = {Yin, Xiaoyu and Gromann, Dagmar and Rudolph, Sebastian},
  year = {2019},
  month = jun,
  number = {arXiv:1906.09302},
  eprint = {1906.09302},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.09302},
  urldate = {2025-02-23},
  abstract = {SPARQL is a highly powerful query language for an ever-growing number of Linked Data resources and Knowledge Graphs. Using it requires a certain familiarity with the entities in the domain to be queried as well as expertise in the language's syntax and semantics, none of which average human web users can be assumed to possess. To overcome this limitation, automatically translating natural language questions to SPARQL queries has been a vibrant field of research. However, to this date, the vast success of deep learning methods has not yet been fully propagated to this research problem. This paper contributes to filling this gap by evaluating the utilization of eight different Neural Machine Translation (NMT) models for the task of translating from natural language to the structured query language SPARQL. While highlighting the importance of high-quantity and high-quality datasets, the results show a dominance of a CNN-based architecture with a BLEU score of up to 98 and accuracy of up to 94\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Unread},
  file = {/Users/janwardenga/Zotero/storage/2IGXAAEE/Yin et al. - 2019 - Neural Machine Translating from Natural Language to SPARQL.pdf;/Users/janwardenga/Zotero/storage/JJLTL7GZ/1906.html}
}

@misc{yuEvaluationRetrievalAugmentedGeneration2024,
  title = {Evaluation of {{Retrieval-Augmented Generation}}: {{A Survey}}},
  shorttitle = {Evaluation of {{Retrieval-Augmented Generation}}},
  author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  year = {2024},
  month = jul,
  number = {arXiv:2405.07437},
  eprint = {2405.07437},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2405.07437},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Jab/Pre,Unread},
  annotation = {41 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/XJMWGQQJ/Pre_2024_Evaluation of Retrieval-Augmented Generation.pdf}
}

@misc{yuKnowledgeCentricBenchmarkingFramework2024,
  title = {A {{Knowledge-Centric Benchmarking Framework}} and {{Empirical Study}} for {{Retrieval-Augmented Generation}}},
  author = {Yu, Shuo and Cheng, Mingyue and Yang, Jiqian and Ouyang, Jie},
  year = {2024},
  month = sep,
  number = {arXiv:2409.13694},
  eprint = {2409.13694},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.13694},
  urldate = {2024-09-29},
  abstract = {Retrieval-Augmented Generation (RAG) enhances generative models by integrating retrieval mechanisms, which allow these models to access and utilize external knowledge sources. Despite its advantages, RAG encounters significant challenges, particularly in effectively handling real-world queries and mitigating hallucinations. The KDD Cup 2024 CRAG competition brings these issues to the forefront by incorporating both web pages and a mock API as knowledge sources, adding the complexity of parsing HTML before large language models (LLMs) can process the information. In this paper, we propose a novel RAG benchmark designed to address these challenges. Our work provides a comprehensive set of experimental results, offering valuable insights for the study of RAG. We thoroughly examine the entire RAG process, including knowledge source selection, retrieval, organization, and reasoning. Key findings from our study include the impact of automated knowledge source selection using agents and the influence of noise chunks on RAG reasoning. Additionally, we conduct detailed experiments to analyze the effects of various hyperparameters on RAG performance. To support further research, we have made our results, the associated code, and a parsed version of the CRAG dataset publicly available1, contributing to the advancement of RAG methodologies and establishing a solid foundation for future work in this domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/DUAA389P/Pre_2024_A Knowledge-Centric Benchmarking Framework and Empirical Study for Retrieval-Augmented Generation.pdf}
}

@inproceedings{yuRetrievalaugmentedGenerationHeterogeneous2022Proc.2022Conf.NorthAm.ChapterAssoc.Comput.Linguist.Hum.Lang.Technol.Stud.Res.Workshop,
  title = {Retrieval-Augmented {{Generation}} across {{Heterogeneous Knowledge}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Student Research Workshop}}},
  author = {Yu, Wenhao},
  year = {2022},
  pages = {52--58},
  publisher = {Association for Computational Linguistics},
  address = {Hybrid: Seattle, Washington + Online},
  doi = {10.18653/v1/2022.naacl-srw.7},
  urldate = {2024-09-29},
  abstract = {Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved stateof-the-art performance on many NLP downstream tasks. Compared with conventional pretrained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.},
  langid = {english},
  keywords = {Jab/PCNACACLHLTSRW,Unread},
  annotation = {9 citations (Semantic Scholar/DOI) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/6FUJ2KMY/PCNACACLHLTSRW_2022_Retrieval-augmented Generation across Heterogeneous Knowledge.pdf}
}

@incollection{zaheraGeneratingSPARQLNatural2024StudiesontheSemanticWeb,
  title = {Generating {{SPARQL}} from {{Natural Language Using Chain-of-Thoughts Prompting}}},
  booktitle = {Studies on the {{Semantic Web}}},
  author = {Zahera, Hamada M. and Ali, Manzoor and Sherif, Mohamed Ahmed and Moussallem, Diego and Ngonga Ngomo, Axel-Cyrille},
  editor = {Salatino, Angelo and Alam, Mehwish and Ongenae, Femke and Vahdati, Sahar and Gentile, Anna-Lisa and Pellegrini, Tassilo and Jiang, Shufan},
  year = {2024},
  month = sep,
  publisher = {IOS Press},
  doi = {10.3233/SSW240028},
  urldate = {2025-02-20},
  abstract = {Purpose: SPARQL is a highly expressive query language for knowledge graphs; yet, formulating precise SPARQL queries can be challenging for non-expert users. A potential solution is translating natural questions into SPARQL queries, known as SPARQL generation. This paper addresses the challenges of translating natural language questions into SPARQL queries for different knowledge graphs. Methodology: We propose COT-SPARQL, our approach to generate SPARQL queries from input questions. Our approach employs Chain-of-thoughts prompting that guides large language models through intermediate reasoning steps and facilitates generating precise SPARQL queries. Furthermore, our approach incorporates entities and relations from the input question, and one-shot example in the prompt to provide additional context during the query generation process. Findings: We conducted several experiments on benchmark datasets and showed that our approach outperforms the state-of-the-art methods by a large margin. Our approach achieves a significant improvement in F1 score of 4.4\% and 3.0\% for the QALD-10 and QALD-9 datasets, respectively. Value: Our COT-SPARQL approach contributes to the semantic web community by simplifying access to knowledge graphs for non-expert users. In particular, COTSPARQL enables non-expert end-users to query knowledge graphs in natural languages, where COT-SPARQL converts user natural languages queries into SPARQL queries, which can be executed via the knowledge graph's SPARQL endpoint.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  isbn = {978-1-64368-537-3},
  keywords = {Unread}
}

@article{zenzKeywordsSemanticQueries2009JournalofWebSemantics,
  title = {From Keywords to Semantic Queries---{{Incremental}} Query Construction on the Semantic Web},
  author = {Zenz, Gideon and Zhou, Xuan and Minack, Enrico and Siberski, Wolf and Nejdl, Wolfgang},
  year = {2009},
  month = sep,
  journal = {Journal of Web Semantics},
  volume = {7},
  number = {3},
  pages = {166--176},
  issn = {15708268},
  doi = {10.1016/j.websem.2009.07.005},
  urldate = {2025-02-02},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {Unread},
  annotation = {169 citations (Semantic Scholar/DOI) [2025-02-02]\\
TLDR: The overall design of QUICK is described, the core algorithms to enable efficient query construction are presented, and the effectiveness of the system is demonstrated through an experimental study.}
}

@inproceedings{zhang2024triple,
  title = {Triple-Aware Reasoning: A Retrieval-Augmented Generation Approach for Enhancing Question-Answering Tasks with Knowledge Graphs and Large Language Models},
  booktitle = {The 37th Canadian Conference on Artificial Intelligence},
  author = {Zhang, Hongzhi and Shafiq, M Omair},
  year = {2024},
  abstract = {With the rapid developments of large language models, the application of large language models has spread to almost every aspect of human life. However, large language models also face some challenges. Large language models may be limited by the data used during pre-training and cannot obtain the latest knowledge. In addition, due to the huge amount of training data during the pre-training, it is sometimes difficult for large language models to pay more attention to knowledge with more semantic information for specific domains. To address these issues, while utilizing the state-of-the-art large language models, we present a retrieval-augmented generation-based method called Triple-Aware Reasoning. This method involves searching the knowledge graph for all alternative answer triples and subjecting the triples to multiple filtering to ensure the relevance of external knowledge and selecting relevant answers for each question-answering pair. By integrating knowledge graphs into the reasoning process, our method enables large language models to access and utilize additional external knowledge. Experimental results on the well-known CommonsenseQA and OpenBookQA datasets demonstrate the effectiveness of our approach. The large language model combined with the retrievalaugmented generation method demonstrates higher accuracy than the original model in multiple-choice question-answering tasks. The experimental results show the improvement by our method on the inference performance of large language models. Keywords: Retrieval-augmented generation, Large language model, Knowledge graph, Reasoning task, Question-answering task},
  keywords = {,Jab/Pre,No DOI found,Read,Unread},
  file = {/Users/janwardenga/Zotero/storage/W2AYJEL8/Pre_2024_Triple-aware reasoning a retrieval-augmented generation approach for enhancing question-answering tasks with knowledge graphs and large language models.pdf}
}

@misc{zhangExtractDefineCanonicalize2024,
  title = {Extract, {{Define}}, {{Canonicalize}}: {{An LLM-based Framework}} for {{Knowledge Graph Construction}}},
  shorttitle = {Extract, {{Define}}, {{Canonicalize}}},
  author = {Zhang, Bowen and Soh, Harold},
  year = {2024},
  month = apr,
  number = {arXiv:2404.03868},
  eprint = {2404.03868},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.03868},
  urldate = {2024-09-29},
  abstract = {In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schema easily exceed the LLMs' context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {75 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/R9EI8EGY/Pre_2024_Extract, Define, Canonicalize.pdf}
}

@article{zhangFuzzyRDFKnowledge2023IEEETrans.FuzzySyst.,
  title = {Fuzzy {{RDF Knowledge Graph Embeddings Through Vector Space Model}}},
  author = {Zhang, Xiaowen and Ma, Zongmin},
  year = {2023},
  month = mar,
  journal = {IEEE Transactions on Fuzzy Systems},
  volume = {31},
  number = {3},
  pages = {835--844},
  issn = {1063-6706, 1941-0034},
  doi = {10.1109/TFUZZ.2022.3190633},
  urldate = {2024-09-26},
  abstract = {Resource description framework (RDF) is a World Wide Web consortium recommendation and has been widely accepted for semantic data models (e.g., knowledge graphs). As such, a large amount of RDF data is readily available. Information in the real world is often uncertain, and uncertain RDF data models have been proposed and received increasing attention. Knowledge graph embedding is an effective way to perform vector projection of structured knowledge, after which we can infer a large-scale knowledge graph. Nowadays, knowledge graph embedding models are widely proposed, but most of them can only handle crisp information. In this article, we concentrate on modeling fuzzy RDF graphs in the vector space. We propose a fuzzy RDF knowledge graph embedding model (FRKGE), which can project the fuzzy entities and fuzzy relations with membership degree to the vector space. Our proposed model preserves the structure of the fuzzy RDF knowledge graph and the explicit expression of membership degree in the vector space, which provides a natural expression of fuzzy RDF tuples. Our experimental results demonstrate that the FRKGE model is competent for the task of representation learning for fuzzy RDF knowledge graphs.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {,Read},
  annotation = {8 citations (Semantic Scholar/DOI) [2025-02-15]\\
0 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This article proposes a fuzzy RDF knowledge graph embedding model (FRKGE), which can project the fuzzy entities and fuzzy relations with membership degree to the vector space, which provides a natural expression of fuzzy R DF tuples.},
  file = {/Users/janwardenga/Zotero/storage/44WTPM7Y/TFS_2023_Fuzzy RDF Knowledge Graph Embeddings Through Vector Space Model.pdf}
}

@article{zhao2024chat2data,
  title = {{{Chat2Data}}: {{An}} Interactive Data Analysis System with {{RAG}}, Vector Databases and Llms},
  author = {Zhao, Xinyang and Zhou, Xuanhe and Li, Guoliang},
  year = {2024},
  journal = {Proc. VLDB Endow},
  keywords = {No DOI found,Read},
  file = {/Users/janwardenga/Zotero/storage/EQRGZ7YN/Pre__Chat2Data An Interactive Data Analysis System with RAG, Vector Databases and LLMs.pdf}
}

@misc{zhaoRetrievalAugmentedGeneration2024,
  title = {Retrieval {{Augmented Generation}} ({{RAG}}) and {{Beyond}}: {{A Comprehensive Survey}} on {{How}} to {{Make}} Your {{LLMs}} Use {{External Data More Wisely}}},
  shorttitle = {Retrieval {{Augmented Generation}} ({{RAG}}) and {{Beyond}}},
  author = {Zhao, Siyun and Yang, Yuqing and Wang, Zilong and He, Zhiyuan and Qiu, Luna K. and Qiu, Lili},
  year = {2024},
  month = sep,
  number = {arXiv:2409.14924},
  eprint = {2409.14924},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2409.14924},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. External data not only bolsters the models' domain-specific expertise and temporal relevance but also diminishes incidences of hallucination, thereby enhancing both the controllability and interpretability of outputs. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and the task's primary focus: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {11 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/LQU4TIRQ/Pre_2024_Retrieval Augmented Generation (RAG) and Beyond.pdf}
}

@misc{zhaoRetrievalAugmentedGenerationAIGenerated2024,
  title = {Retrieval-{{Augmented Generation}} for {{AI-Generated Content}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{AI-Generated Content}}},
  author = {Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
  year = {2024},
  month = jun,
  number = {arXiv:2402.19473},
  eprint = {2402.19473},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.19473},
  urldate = {2024-09-29},
  abstract = {Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG techniques into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {6 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/IHTIZZ7U/Pre_2024_Retrieval-Augmented Generation for AI-Generated Content.pdf}
}

@misc{zhengKSLLMKnowledgeSelection2024,
  title = {{{KS-LLM}}: {{Knowledge Selection}} of {{Large Language Models}} with {{Evidence Document}} for {{Question Answering}}},
  shorttitle = {{{KS-LLM}}},
  author = {Zheng, Xinxin and Che, Feihu and Wu, Jinyang and Zhang, Shuai and Nie, Shuai and Liu, Kang and Tao, Jianhua},
  year = {2024},
  month = apr,
  number = {arXiv:2404.15660},
  eprint = {2404.15660},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.15660},
  urldate = {2024-09-29},
  abstract = {Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {3 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/PMHKL8MU/Pre_2024_KS-LLM.pdf}
}

@article{zhengSemanticSPARQLSimilarity2016Proc.VLDBEndow.,
  title = {Semantic {{SPARQL}} Similarity Search over {{RDF}} Knowledge Graphs},
  author = {Zheng, Weiguo and Zou, Lei and Peng, Wei and Yan, Xifeng and Song, Shaoxu and Zhao, Dongyan},
  year = {2016},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {11},
  pages = {840--851},
  issn = {2150-8097},
  doi = {10.14778/2983200.2983201},
  urldate = {2024-09-26},
  abstract = {RDF knowledge graphs have attracted increasing attentions these years. However, due to the schema-free nature of RDF data, it is very difficult for users to have full knowledge of the underlying schema. Furthermore, the same kind of information can be represented in diverse graph fragments. Hence, it is a huge challenge to formulate complex SPARQL expressions by taking the union of all possible structures.},
  langid = {english},
  keywords = {,Jab/PVE,Read},
  annotation = {100 citations (Semantic Scholar/DOI) [2025-02-20]\\
12 citations (Semantic Scholar/DOI) [2025-02-15]\\
6 citations (Semantic Scholar/DOI) [2025-02-15]\\
TLDR: This paper proposes an effective framework to access the RDF repository even if users have no full knowledge of the underlying schema, and is the first to propose a novel similarity measure, semantic graph edit distance, to improve the efficiency performance.},
  file = {/Users/janwardenga/Zotero/storage/UQ8Q6ME7/PVE_2016_Semantic SPARQL similarity search over RDF knowledge graphs.pdf}
}

@misc{zhouLLMEnhancedDataManagement2024,
  title = {{{LLM-Enhanced Data Management}}},
  author = {Zhou, Xuanhe and Zhao, Xinyang and Li, Guoliang},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02643},
  eprint = {2402.02643},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.02643},
  urldate = {2024-09-29},
  abstract = {Machine learning (ML) techniques for optimizing data management problems have been extensively studied and widely deployed in recent five years. However traditional ML methods have limitations on generalizability (adapting to different scenarios) and inference ability (understanding the context). Fortunately, large language models (LLMs) have shown high generalizability and human-competitive abilities in understanding context, which are promising for data management tasks (e.g., database diagnosis and data analytics). However, existing LLMs have several limitations: hallucination, high cost, and low accuracy for complicated tasks. To address these challenges, we design LLMDB, an LLM-enhanced data management paradigm which has good generalizability and high inference ability while avoiding hallucination, reducing LLM cost, and achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high cost of LLMs by vector databases which provide semantic search and caching abilities. LLMDB improves the task accuracy by LLM agent which provides multiple-round inference and pipeline executions. We showcase three real-world data management scenarios that LLMDB can well support, including query rewrite, database diagnosis and data analytics. We also summarize the open research challenges of LLMDB.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {31 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/LQCCKJ7U/Pre_2024_LLM-Enhanced Data Management.pdf}
}

@misc{zhouMultidomainDialogueState2020,
  title = {Multi-Domain {{Dialogue State Tracking}} as {{Dynamic Knowledge Graph Enhanced Question Answering}}},
  author = {Zhou, Li and Small, Kevin},
  year = {2020},
  month = jun,
  number = {arXiv:1911.06192},
  eprint = {1911.06192},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1911.06192},
  urldate = {2024-09-29},
  abstract = {Multi-domain dialogue state tracking (DST) is a critical component for conversational AI systems. The domain ontology (i.e., specification of domains, slots, and values) of a conversational AI system is generally incomplete, making the capability for DST models to generalize to new slots, values, and domains during inference imperative. In this paper, we propose to model multi-domain DST as a question answering problem, referred to as Dialogue State Tracking via Question Answering (DSTQA). Within DSTQA, each turn generates a question asking for the value of a (domain, slot) pair, thus making it naturally extensible to unseen domains, slots, and values. Additionally, we use a dynamically-evolving knowledge graph to explicitly learn relationships between (domain, slot) pairs. Our model has a 5.80\% and 12.21\% relative improvement over the current state-of-the-art model on MultiWOZ 2.0 and MultiWOZ 2.1 datasets, respectively. Additionally, our model consistently outperforms the state-of-the-art model in domain adaptation settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {7 citations (Semantic Scholar/arXiv) [2025-02-15]\\
84 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/SXYBXW77/Pre_2020_Multi-domain Dialogue State Tracking as Dynamic Knowledge Graph Enhanced Question Answering.pdf}
}

@misc{zhuLLMsKnowledgeGraph2024,
  title = {{{LLMs}} for {{Knowledge Graph Construction}} and {{Reasoning}}: {{Recent Capabilities}} and {{Future Opportunities}}},
  shorttitle = {{{LLMs}} for {{Knowledge Graph Construction}} and {{Reasoning}}},
  author = {Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  year = {2024},
  month = aug,
  number = {arXiv:2305.13168},
  eprint = {2305.13168},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2305.13168},
  urldate = {2024-09-29},
  abstract = {This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Unread},
  annotation = {38 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/LQ9NY6YV/Pre_2024_LLMs for Knowledge Graph Construction and Reasoning.pdf}
}

@misc{zoupanosEfficientComparisonSentence2022,
  title = {Efficient Comparison of Sentence Embeddings},
  author = {Zoupanos, Spyros and Kolovos, Stratis and Kanavos, Athanasios and Papadimitriou, Orestis and Maragoudakis, Manolis},
  year = {2022},
  month = apr,
  number = {arXiv:2204.00820},
  eprint = {2204.00820},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2204.00820},
  urldate = {2024-09-29},
  abstract = {The domain of natural language processing (NLP), which has greatly evolved over the last years, has highly benefited from the recent developments in word and sentence embeddings. Such embeddings enable the transformation of complex NLP tasks, like semantic similarity or Question and Answering (Q\&A), into much simpler to perform vector comparisons. However, such a problem transformation raises new challenges like the efficient comparison of embeddings and their manipulation. In this work, we will discuss about various word and sentence embeddings algorithms, we will select a sentence embedding algorithm, BERT, as our algorithm of choice and we will evaluate the performance of two vector comparison approaches, FAISS and Elasticsearch, in the specific problem of sentence embeddings. According to the results, FAISS outperforms Elasticsearch when used in a centralized environment with only one node, especially when big datasets are included.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Unread},
  annotation = {10 citations (Semantic Scholar/arXiv) [2025-02-15]\\
10 citations (Semantic Scholar/arXiv) [2025-02-15]},
  file = {/Users/janwardenga/Zotero/storage/LXINZI5U/Pre_2022_Efficient comparison of sentence embeddings.pdf}
}
